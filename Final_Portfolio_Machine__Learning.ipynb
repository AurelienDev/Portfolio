{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f78c727-106f-4790-90a8-1620a531f1ba",
   "metadata": {
    "id": "4f78c727-106f-4790-90a8-1620a531f1ba"
   },
   "source": [
    "# **OBJECTIVE :**\n",
    "# **`Business Case & Hypothesis`**\n",
    "\n",
    "## >>> <u>Forecast the monthly transactions amount<u>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d31d8-94d3-4a10-9061-f6234218f6f2",
   "metadata": {
    "id": "c56d31d8-94d3-4a10-9061-f6234218f6f2"
   },
   "source": [
    "## Source : Financial Transactions Dataset (www.Kaggle.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c608894-ee97-4d29-a6ef-3bba813e5eca",
   "metadata": {
    "id": "7c608894-ee97-4d29-a6ef-3bba813e5eca"
   },
   "source": [
    "#### Overview : This comprehensive financial dataset combines transaction records, customer information, and card data from a banking institution, spanning across the 2010s decade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98806a-e3f7-4007-a440-42144d4be937",
   "metadata": {
    "id": "7d98806a-e3f7-4007-a440-42144d4be937"
   },
   "source": [
    "### Dataset Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46f4980-2e3a-4ca0-90a9-b1cfa77270ba",
   "metadata": {
    "id": "e46f4980-2e3a-4ca0-90a9-b1cfa77270ba"
   },
   "source": [
    "#### <b><u>1. Transaction Data<b><u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2f52a-0c1b-465a-964d-eddbe8bb251d",
   "metadata": {
    "id": "0fd2f52a-0c1b-465a-964d-eddbe8bb251d"
   },
   "source": [
    "##### * Detailed transaction records including amounts, timestamps, and merchant details\n",
    "##### * Covers transactions throughout the 2010s\n",
    "##### * Features transaction types, amounts, and merchant information\n",
    "##### * Perfect for analyzing spending patterns and building fraud detection models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241da9e-cd11-4430-96d7-28a651be7665",
   "metadata": {
    "id": "2241da9e-cd11-4430-96d7-28a651be7665"
   },
   "source": [
    "| Variable      | Description                                                                                                 |\n",
    "| ------------- | ----------------------------------------------------------------------------------------------------------- |\n",
    "| id            | ID number of the transaction                                                                                |\n",
    "| date          | Transction's date                                                                                           |\n",
    "| client_id     | Client id number                                                                                            |\n",
    "| card_id       | Card id number                                                                                              |\n",
    "| amount        | Amount of the transaction                                                                                   |\n",
    "| use_chip      | Transaction methodology                                                                                     |\n",
    "| merchant_id   | ID munber of the merchant                                                                                   |\n",
    "| merchant_city | City of the merchant                                                                                        |\n",
    "| merchant_state| State of the merchant                                                                                       |\n",
    "| zip           | Zip code of the merchant                                                                                    |\n",
    "| mcc           | ID number indicating the category goods or services concerned by the transation                             |\n",
    "| errors        | Transaction Error                                                                                           |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754d24b-c67b-4f64-a367-d30fb61b6ee1",
   "metadata": {
    "id": "3754d24b-c67b-4f64-a367-d30fb61b6ee1"
   },
   "source": [
    "#### <b><u>2. Merchant Category Codes<b><u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b83dc-484e-429f-9aab-5d09a408f3e9",
   "metadata": {
    "id": "5e5b83dc-484e-429f-9aab-5d09a408f3e9"
   },
   "source": [
    "##### * Standard classification codes for business types\n",
    "##### * Enables transaction categorization and spending analysis\n",
    "##### * Industry-standard MCC codes with descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7c4d8-b652-4c87-a7d5-fd3bd63aa1bd",
   "metadata": {
    "id": "8ba7c4d8-b652-4c87-a7d5-fd3bd63aa1bd"
   },
   "source": [
    "| Variable      | Description                                                                                                 |\n",
    "| ------------- | ----------------------------------------------------------------------------------------------------------- |\n",
    "| id            | ID number of the category                                                                                   |                         \n",
    "| description   | Categorie description                                                                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vSdP4WZtmVUg",
   "metadata": {
    "id": "vSdP4WZtmVUg"
   },
   "source": [
    "# <b><span style=\"color:blue;\">1. CONTEXT DESCRIPTION<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b7811-7e07-4200-8b5b-a5a598107b81",
   "metadata": {
    "id": "126b7811-7e07-4200-8b5b-a5a598107b81"
   },
   "source": [
    "## **The goal of this project is to forecast the monthly transaction volume based on historical financial data from 2015 to 2019.**\n",
    "\n",
    "###**After an initial exploratory analysis, I decided to use LightGBM, a gradient boosting algorithm particularly effective for tabular data and capable of efficiently handling categorical features.**\n",
    "\n",
    "\n",
    "On the below sessions, I used the combo LightGBM + Optuna.\n",
    "\n",
    "YouTube link where I learned about Optuna : https://www.youtube.com/watch?v=kR9pOvr9Urs&t=1046s\n",
    "\n",
    "Regarding the machine learning model, I considered two alternatives::\n",
    "\n",
    ">>> Prophet and LightGBM ; for handling large datasets, I opted for LightGBM.\n",
    "\n",
    ">>>> Indeed, LightGBM is a decision tree-based Gradient Boosting Framework. LightGBM targets the leaf that shows the greatest ability to minimize total error. Additionnaly, LightGBM trains quickly even in the absence of a GPU.\n",
    "\n",
    "Following extensive manual hyperparameter testing, I decided to use Optuna.\n",
    "\n",
    ">>> Optuna is a tool that evaluates various parameters to identify the most effective one.\n",
    "\n",
    ">>>> I then applied the optimal hyperparameters throughout the machine learning session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IoXXgz4Q7G7w",
   "metadata": {
    "id": "IoXXgz4Q7G7w"
   },
   "source": [
    "### **Comments :**\n",
    "The primary challenge was securing dependable forecasts while preventing data leaks and overfitting.\n",
    "\n",
    "To accomplish this, I performed numerous tests in pursuit of the optimal approach.\n",
    "\n",
    "The fifth experiment produces an R² of 0.65, an MAE of 0.62, and an RMSE of 0.85. The RMSE > MAE. This indicates that certain outliers are affecting the prediction model.\n",
    "\n",
    "To tackle the outlier problem, I implemented a hybrid model that merges LightGBM as the primary prediction model with ARIMA to handle the residual data.\n",
    "\n",
    "Regrettably, ARIMA consumes significant CPU resources, and since I am utilizing a free Google Colab license, I was unable to obtain results surpassing those from experiment 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6_sU88RlyEOM",
   "metadata": {
    "id": "6_sU88RlyEOM"
   },
   "source": [
    "# <b><span style=\"color:blue;\">2. SUCCESSIVE EXPERIMENTS<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YlioUPT3yf1F",
   "metadata": {
    "id": "YlioUPT3yf1F"
   },
   "source": [
    "### **1. Basic Monthly aggregated mode (not in the presentation)**\n",
    "Approach: Incorporating lag variables (temporal lags) into a monthly dataset.\n",
    "\n",
    "Outcome: R² ≈ 0.99\n",
    "\n",
    "Analysis: Data leakage → the model indirectly perceived future values ​​through misaligned lag variables.\n",
    "\n",
    "### **2. Complete transactions after 2015, including filtering and data smoothing (presentation paragraph 3).**\n",
    "Approach: Direct the model's attention to uniform years post-2015.\n",
    "\n",
    "Outcome: R² ≈ -0.010\n",
    "\n",
    "Analysis: The model is constrained by insufficient context and is not optimized (numerous hours for the machine learning process).\n",
    "\n",
    "### **3. Comprehensive transactions Post-2015 Filtering enhanced model (slide paragraph 4)**\n",
    "Approach: Enhancing the model (version 2) by utilizing cache for quicker computations.\n",
    "\n",
    "Outcome: R² ≈ 0.55\n",
    "\n",
    "Analysis: The model is producing significantly improved results; however, I believe this time series machine learning model could achieve a better R².\n",
    "\n",
    "### **4. Monthly aggregated model with temporal split no lag**\n",
    "Method: Monthly aggregation and temporal split with lag\n",
    "\n",
    "Outcome: R² ≈ 0.65 on post-2015 data\n",
    "\n",
    "Analysis:\n",
    "No data leakage (strict time split)\n",
    "Very consistent data after 2015 ⇒ predictable model\n",
    "R² reflects the stability of the series, not overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6236c9e4-2f80-4ecd-a106-daf1b0c99b85",
   "metadata": {
    "id": "6236c9e4-2f80-4ecd-a106-daf1b0c99b85"
   },
   "source": [
    "## <span style=\"color:blue;\">3. **Experiment 2** simple **cache methodology & data smoothing** - MACHINE LEARNING USING LIGHTGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hCyZTmbnEFuZ",
   "metadata": {
    "id": "hCyZTmbnEFuZ"
   },
   "source": [
    "### **Comments :**\n",
    "In this initial step, I will download and aggregate data to obtain a consolidated DataFrame with all transaction information.\n",
    "Then I will\n",
    "1. Import libraries and install Synpase for machine learning purposes\n",
    "2. Data Extraction and Load - I have extracted data by connecting to Kaggle\n",
    "3. Spark Session - I have created a Spark Session for big data usage\n",
    "4. Optuna Setup - I took 1% sample Dataframe and used Optuna to determine the best hyperparameters automatically\n",
    "5. Creation of the pipeline, LGBM algorithm, temporal split\n",
    "\n",
    "**Poor Result du to a lack of context : R² ≈ -0.010**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vj-ADx8Uorlo",
   "metadata": {
    "id": "Vj-ADx8Uorlo"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Initialization</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jacvecuno5-r",
   "metadata": {
    "collapsed": true,
    "id": "Jacvecuno5-r",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 1. Install JAVA - SPARK\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "!apt-get install openjdk-11-jdk -y\n",
    "!pip install pyspark==3.5.0\n",
    "!pip install findspark\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] += \":/usr/lib/jvm/java-11-openjdk-amd64/bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aZ9RDfGHmRrU",
   "metadata": {
    "id": "aZ9RDfGHmRrU"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 2. SPARK Session Initialization\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML_Portfolio\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:1.0.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BiHUIfdhUyXR",
   "metadata": {
    "id": "BiHUIfdhUyXR"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 3. Install Synapse Optuna\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "!pip install synapseml optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dMzYfP_rm9aK",
   "metadata": {
    "id": "dMzYfP_rm9aK"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Import Libraries\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from itertools import product\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, mean, unix_timestamp, to_date, regexp_replace, when, abs,\n",
    "    dayofweek, month, avg, count, lit\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from synapse.ml.lightgbm import LightGBMRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2de7d55-75b1-4826-bbd2-37d773daf7bc",
   "metadata": {
    "id": "d2de7d55-75b1-4826-bbd2-37d773daf7bc"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Extraction - Load</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1uSZIX9J8nUo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uSZIX9J8nUo",
    "outputId": "4a84ab0b-c295-49de-cce7-fb7e0c8dc8aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab cache for faster access to the 'transactions-fraud-datasets' dataset.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 5. Data Loading\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "path = kagglehub.dataset_download(\"computingvictor/transactions-fraud-datasets\")\n",
    "transaction_data = f\"{path}/transactions_data.csv\"\n",
    "mcc_codes_data = f\"{path}/mcc_codes.json\"\n",
    "\n",
    "td = pd.read_csv(transaction_data)\n",
    "with open(mcc_codes_data, \"r\") as f:\n",
    "    mcc_dict = json.load(f)\n",
    "mcc = pd.DataFrame.from_dict(mcc_dict, orient=\"index\", columns=[\"description\"]).reset_index().rename(columns={\"index\": \"mcc_code\"})\n",
    "\n",
    "mcc_spark = spark.createDataFrame(mcc)\n",
    "td_spark = spark.read.csv(transaction_data, header=True, inferSchema=True).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YLVonj1csjb4",
   "metadata": {
    "id": "YLVonj1csjb4"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Data Wrangling</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5CgA0bpbxm",
   "metadata": {
    "id": "9a5CgA0bpbxm"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 6. Data Preparation\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "LABEL_COL = \"amount\"\n",
    "CATEGORICAL_FEATURE_COL = \"description\"\n",
    "\n",
    "td_spark = td_spark.withColumn(LABEL_COL, regexp_replace(LABEL_COL, \"\\$\", \"\"))\n",
    "td_spark = td_spark.withColumn(LABEL_COL, col(LABEL_COL).cast(DoubleType()))\n",
    "td_spark = td_spark.withColumn(\"date\", to_date(\"date\"))\n",
    "\n",
    "df = td_spark.join(mcc_spark, td_spark.mcc == mcc_spark.mcc_code, \"left\")\n",
    "df = df.drop(\"id\", \"client_id\", \"card_id\", \"use_chip\", \"merchant_id\", \"merchant_city\",\n",
    "             \"merchant_state\", \"zip\", \"mcc\", \"errors\", \"mcc_code\")\n",
    "\n",
    "df = df.withColumn(\"date_timestamp\", unix_timestamp(col(\"date\")).cast(DoubleType()))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"date\")).cast(DoubleType()))\n",
    "df = df.withColumn(\"month\", month(col(\"date\")).cast(DoubleType()))\n",
    "df = df.withColumn(\"is_refund\", when(col(LABEL_COL) < 0, 1.0).otherwise(0.0).cast(DoubleType()))\n",
    "df = df.withColumn(LABEL_COL, when(col(LABEL_COL).isNull(), 0.0).otherwise(abs(col(LABEL_COL))))\n",
    "\n",
    "CLEANED_CATEGORICAL_COL = \"description_clean\"\n",
    "df = df.withColumn(CLEANED_CATEGORICAL_COL, regexp_replace(col(CATEGORICAL_FEATURE_COL), \"[ ,:\\\\[\\\\]\\\\{\\\\}]\", \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pOF6-dnUpbvT",
   "metadata": {
    "id": "pOF6-dnUpbvT"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 7. Sample (1%) will be used to search the best hyper parameters settings\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "df_sample = df.sample(fraction=0.01, seed=42).toPandas()\n",
    "df_sample.dropna(subset=[LABEL_COL], inplace=True)\n",
    "df_sample[\"date_timestamp\"] = pd.to_datetime(df_sample[\"date\"]).astype(int) / 10**9\n",
    "df_sample[\"is_refund\"] = (df_sample[LABEL_COL] < 0).astype(float)\n",
    "df_sample[LABEL_COL] = df_sample[LABEL_COL].abs()\n",
    "\n",
    "X = df_sample[[\"date_timestamp\", \"is_refund\"]]\n",
    "y = df_sample[LABEL_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B13TppdVpbtb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "B13TppdVpbtb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "15752775-1d61-4455-87e2-05c3a8b3d4b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 18:18:06,922] A new study created in memory with name: no-name-57a4e1da-1bf0-4ba7-8482-79364ff98811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimization starting on 1% sample...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-01 18:18:24,572] Trial 0 finished with value: 75.33336139121394 and parameters: {'num_leaves': 153, 'max_depth': 15, 'learning_rate': 0.022079166749651016, 'feature_fraction': 0.9887256591512779, 'bagging_fraction': 0.7694494483051384, 'num_iterations': 506}. Best is trial 0 with value: 75.33336139121394.\n",
      "[I 2025-11-01 18:18:33,982] Trial 1 finished with value: 75.33516233772073 and parameters: {'num_leaves': 48, 'max_depth': 13, 'learning_rate': 0.0165253572023927, 'feature_fraction': 0.9925144157806065, 'bagging_fraction': 0.8878605938404487, 'num_iterations': 497}. Best is trial 0 with value: 75.33336139121394.\n",
      "[I 2025-11-01 18:18:49,723] Trial 2 finished with value: 75.33334662373754 and parameters: {'num_leaves': 233, 'max_depth': 12, 'learning_rate': 0.05165046435797901, 'feature_fraction': 0.8094348887352868, 'bagging_fraction': 0.7693234761306247, 'num_iterations': 633}. Best is trial 2 with value: 75.33334662373754.\n",
      "[I 2025-11-01 18:18:53,162] Trial 3 finished with value: 75.3346461708803 and parameters: {'num_leaves': 40, 'max_depth': 6, 'learning_rate': 0.05238422279269173, 'feature_fraction': 0.9450935755759657, 'bagging_fraction': 0.7131263559080584, 'num_iterations': 416}. Best is trial 2 with value: 75.33334662373754.\n",
      "[I 2025-11-01 18:19:03,265] Trial 4 finished with value: 75.3337900121797 and parameters: {'num_leaves': 229, 'max_depth': 11, 'learning_rate': 0.01369706213812038, 'feature_fraction': 0.7761546545331941, 'bagging_fraction': 0.967152171196962, 'num_iterations': 504}. Best is trial 2 with value: 75.33334662373754.\n",
      "[I 2025-11-01 18:19:16,633] Trial 5 finished with value: 75.33334999509789 and parameters: {'num_leaves': 233, 'max_depth': 14, 'learning_rate': 0.015688937281912056, 'feature_fraction': 0.8433608839280923, 'bagging_fraction': 0.9221748118503391, 'num_iterations': 592}. Best is trial 2 with value: 75.33334662373754.\n",
      "[I 2025-11-01 18:19:26,762] Trial 6 finished with value: 75.33784095535036 and parameters: {'num_leaves': 36, 'max_depth': 7, 'learning_rate': 0.01419125446773346, 'feature_fraction': 0.8237011372037017, 'bagging_fraction': 0.8595866435659251, 'num_iterations': 743}. Best is trial 2 with value: 75.33334662373754.\n",
      "[I 2025-11-01 18:19:40,007] Trial 7 finished with value: 75.33334661368993 and parameters: {'num_leaves': 212, 'max_depth': 11, 'learning_rate': 0.05866715711169298, 'feature_fraction': 0.7690979468447738, 'bagging_fraction': 0.7860163232432587, 'num_iterations': 696}. Best is trial 7 with value: 75.33334661368993.\n",
      "[I 2025-11-01 18:19:45,163] Trial 8 finished with value: 75.33700383402879 and parameters: {'num_leaves': 50, 'max_depth': 6, 'learning_rate': 0.03650346849563564, 'feature_fraction': 0.7097139209864382, 'bagging_fraction': 0.8355880578816333, 'num_iterations': 405}. Best is trial 7 with value: 75.33334661368993.\n",
      "[I 2025-11-01 18:19:52,350] Trial 9 finished with value: 75.33378006976533 and parameters: {'num_leaves': 93, 'max_depth': 14, 'learning_rate': 0.02039849886475306, 'feature_fraction': 0.8763056813394847, 'bagging_fraction': 0.8522423482613573, 'num_iterations': 426}. Best is trial 7 with value: 75.33334661368993.\n",
      "[I 2025-11-01 18:20:10,359] Trial 10 finished with value: 75.3333466063559 and parameters: {'num_leaves': 172, 'max_depth': 9, 'learning_rate': 0.08577542356421035, 'feature_fraction': 0.7148258479028721, 'bagging_fraction': 0.7817799226486011, 'num_iterations': 997}. Best is trial 10 with value: 75.3333466063559.\n",
      "[I 2025-11-01 18:20:25,881] Trial 11 finished with value: 75.3333466063207 and parameters: {'num_leaves': 171, 'max_depth': 9, 'learning_rate': 0.09787165282999918, 'feature_fraction': 0.7008019787226754, 'bagging_fraction': 0.7855886049211697, 'num_iterations': 1000}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:20:41,155] Trial 12 finished with value: 75.33334660632977 and parameters: {'num_leaves': 163, 'max_depth': 9, 'learning_rate': 0.09684336017220545, 'feature_fraction': 0.7090378310462581, 'bagging_fraction': 0.7100293118071392, 'num_iterations': 975}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:20:58,657] Trial 13 finished with value: 75.3333466063208 and parameters: {'num_leaves': 118, 'max_depth': 9, 'learning_rate': 0.09927375571548404, 'feature_fraction': 0.700478123880619, 'bagging_fraction': 0.7043598410480008, 'num_iterations': 994}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:21:00,688] Trial 14 finished with value: 75.33422894272347 and parameters: {'num_leaves': 106, 'max_depth': 8, 'learning_rate': 0.0703149033675483, 'feature_fraction': 0.7508143010333583, 'bagging_fraction': 0.735203048758299, 'num_iterations': 205}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:21:19,277] Trial 15 finished with value: 75.3333472670353 and parameters: {'num_leaves': 117, 'max_depth': 10, 'learning_rate': 0.036253265636337594, 'feature_fraction': 0.9087711177011303, 'bagging_fraction': 0.8084869886099987, 'num_iterations': 894}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:21:26,486] Trial 16 finished with value: 75.33337629886226 and parameters: {'num_leaves': 132, 'max_depth': 5, 'learning_rate': 0.07935780737628026, 'feature_fraction': 0.7457165343087223, 'bagging_fraction': 0.7487332273851394, 'num_iterations': 818}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:21:40,686] Trial 17 finished with value: 75.33334678797786 and parameters: {'num_leaves': 189, 'max_depth': 8, 'learning_rate': 0.0636738035079704, 'feature_fraction': 0.7924314424608596, 'bagging_fraction': 0.7003824887552348, 'num_iterations': 889}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:22:00,330] Trial 18 finished with value: 75.33469747115964 and parameters: {'num_leaves': 84, 'max_depth': 10, 'learning_rate': 0.010314528131513895, 'feature_fraction': 0.7423405834767991, 'bagging_fraction': 0.8243280386651306, 'num_iterations': 870}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:22:10,225] Trial 19 finished with value: 75.33334661148078 and parameters: {'num_leaves': 200, 'max_depth': 8, 'learning_rate': 0.09962144869839645, 'feature_fraction': 0.8685840478289141, 'bagging_fraction': 0.7437599151561858, 'num_iterations': 747}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:22:31,181] Trial 20 finished with value: 75.33334662481435 and parameters: {'num_leaves': 134, 'max_depth': 11, 'learning_rate': 0.04487622769466529, 'feature_fraction': 0.7376172609779434, 'bagging_fraction': 0.8880071287118501, 'num_iterations': 930}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:22:51,912] Trial 21 finished with value: 75.33334660632445 and parameters: {'num_leaves': 163, 'max_depth': 9, 'learning_rate': 0.09520293974693501, 'feature_fraction': 0.7038801162669607, 'bagging_fraction': 0.7200669466932081, 'num_iterations': 995}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:23:05,312] Trial 22 finished with value: 75.33334661545665 and parameters: {'num_leaves': 181, 'max_depth': 9, 'learning_rate': 0.07499372140892199, 'feature_fraction': 0.7054118726205305, 'bagging_fraction': 0.7399241270113065, 'num_iterations': 828}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:23:18,552] Trial 23 finished with value: 75.33334660877394 and parameters: {'num_leaves': 147, 'max_depth': 7, 'learning_rate': 0.09941472808845467, 'feature_fraction': 0.7256135915820754, 'bagging_fraction': 0.7002212528147715, 'num_iterations': 960}. Best is trial 11 with value: 75.3333466063207.\n",
      "[I 2025-11-01 18:23:41,876] Trial 24 finished with value: 75.33334660643165 and parameters: {'num_leaves': 256, 'max_depth': 9, 'learning_rate': 0.0808024037834706, 'feature_fraction': 0.7775365150841325, 'bagging_fraction': 0.8019110420922068, 'num_iterations': 999}. Best is trial 11 with value: 75.3333466063207.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters :\n",
      "{'num_leaves': 171, 'max_depth': 9, 'learning_rate': 0.09787165282999918, 'feature_fraction': 0.7008019787226754, 'bagging_fraction': 0.7855886049211697, 'num_iterations': 1000}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 8. Bayesian Optimization to find the best hyper parameters settings - Using OPTUNA\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 256),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 15),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.7, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.7, 1.0),\n",
    "        \"num_iterations\": trial.suggest_int(\"num_iterations\", 200, 1000),\n",
    "        \"verbose\": -1,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(X, label=y)\n",
    "    model = lgb.train(params, train_data)\n",
    "    preds = model.predict(X)\n",
    "    rmse = ((preds - y) ** 2).mean() ** 0.5\n",
    "    return rmse\n",
    "\n",
    "print(\"\\n Optimization starting on 1% sample...\")\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=25)\n",
    "best_params = study.best_params\n",
    "print(f\"Best Hyperparameters :\\n{best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aK2dMfbys07E",
   "metadata": {
    "id": "aK2dMfbys07E"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Machine Learning</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CKIW_jRXpbrB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKIW_jRXpbrB",
    "outputId": "effb6bd9-a0f3-4fc4-8c97-1ed3ae1d5a52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Temporal Split : Train ≤ 2018-12-31 | Test > 2018-12-31\n",
      "Résultats 2019 → R² = -0.0106 | RMSE = 74.78\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 9. Rolling validation Spark with best hyperparameters found\n",
    "# -------------------------------------------------------------------------\n",
    "def target_encode(train_df, test_df, categorical_col, target_col, smoothing=100.0):\n",
    "    global_mean = train_df.select(avg(col(target_col))).collect()[0][0]\n",
    "    agg = train_df.groupBy(categorical_col).agg(\n",
    "        avg(col(target_col)).alias(\"mean_category\"),\n",
    "        count(col(target_col)).alias(\"count_category\")\n",
    "    )\n",
    "    agg = agg.withColumn(\n",
    "        \"mean_encoded\",\n",
    "        (col(\"mean_category\") * col(\"count_category\") + lit(global_mean) * lit(smoothing))\n",
    "        / (col(\"count_category\") + lit(smoothing))\n",
    "    ).select(categorical_col, \"mean_encoded\")\n",
    "\n",
    "    train_encoded = train_df.join(agg, on=categorical_col, how=\"left\").na.fill({\"mean_encoded\": global_mean})\n",
    "    test_encoded = test_df.join(agg, on=categorical_col, how=\"left\").na.fill({\"mean_encoded\": global_mean})\n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "\n",
    "def train_with_best_params(train_df, test_df, params):\n",
    "    final_features = [\"date_timestamp\", \"day_of_week\", \"month\", \"is_refund\", \"mean_encoded\"]\n",
    "    assembler = VectorAssembler(inputCols=final_features, outputCol=\"features_vector\")\n",
    "    pipeline = Pipeline(stages=[assembler])\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "    train_data = pipeline_model.transform(train_df)\n",
    "    test_data = pipeline_model.transform(test_df)\n",
    "\n",
    "    lgbm = LightGBMRegressor(\n",
    "        featuresCol=\"features_vector\",\n",
    "        labelCol=LABEL_COL,\n",
    "        objective=\"regression\",\n",
    "        numLeaves=params[\"num_leaves\"],\n",
    "        maxDepth=params[\"max_depth\"],\n",
    "        learningRate=params[\"learning_rate\"],\n",
    "        featureFraction=params[\"feature_fraction\"],\n",
    "        baggingFraction=params[\"bagging_fraction\"],\n",
    "        numIterations=params[\"num_iterations\"]\n",
    "    )\n",
    "\n",
    "    model = lgbm.fit(train_data)\n",
    "    preds = model.transform(test_data)\n",
    "\n",
    "    evaluator = RegressionEvaluator(labelCol=LABEL_COL, predictionCol=\"prediction\")\n",
    "    metrics = {\n",
    "        \"mse\": evaluator.evaluate(preds, {evaluator.metricName: \"mse\"}),\n",
    "        \"rmse\": evaluator.evaluate(preds, {evaluator.metricName: \"rmse\"}),\n",
    "        \"mae\": evaluator.evaluate(preds, {evaluator.metricName: \"mae\"}),\n",
    "        \"r2\": evaluator.evaluate(preds, {evaluator.metricName: \"r2\"}),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "#split_years = [2015, 2016, 2017, 2018]\n",
    "split_years = [2018]\n",
    "results = []\n",
    "\n",
    "for year in split_years:\n",
    "    split_date = f\"{year}-12-31\"\n",
    "    print(f\"\\n Temporal Split : Train ≤ {split_date} | Test > {split_date}\")\n",
    "\n",
    "    train_df = df.filter(col(\"date\") <= split_date)\n",
    "    test_df = df.filter(col(\"date\") > split_date)\n",
    "\n",
    "    if train_df.count() == 0 or test_df.count() == 0:\n",
    "        print(f\"Not enough data for the year : {year}.\")\n",
    "        continue\n",
    "\n",
    "    train_encoded, test_encoded = target_encode(train_df, test_df, CLEANED_CATEGORICAL_COL, LABEL_COL)\n",
    "    metrics = train_with_best_params(train_encoded, test_encoded, best_params)\n",
    "    metrics[\"train_end\"] = year\n",
    "    metrics[\"test_year\"] = year + 1\n",
    "\n",
    "    print(f\"Résultats {year+1} → R² = {metrics['r2']:.4f} | RMSE = {metrics['rmse']:.2f}\")\n",
    "    results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VnLIPoJRpbmr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VnLIPoJRpbmr",
    "outputId": "0bb7ce0d-ef01-41c2-d20c-5fb121006fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Export done 'ML_Portfolio'\n",
      "           mse       rmse        mae        r2  train_end  test_year\n",
      "0  5592.309422  74.781745  44.413696 -0.010614       2018       2019\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 10. Final Export\n",
    "# -------------------------------------------------------------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"ML_Portfolio.csv\", index=False)\n",
    "print(\"\\nExport done 'ML_Portfolio'\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xt8IeGwtvcNS",
   "metadata": {
    "id": "xt8IeGwtvcNS"
   },
   "source": [
    "**Below the performance indicators result for 2015 - 2018:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706xmDJiu4c6",
   "metadata": {
    "id": "706xmDJiu4c6"
   },
   "source": [
    "| mse           | rmse       |  mae       | r2         |  train year | test year |\n",
    "| ------------- | -----------|------------|------------|-------------|-----------|\n",
    "| 5706.459224   | 75.027384  | 43.473863  | -0.005715  | 2015        | 2016      |\n",
    "| 5808.157885   | 76.211271  | 47.710919  | -0.043708  | 2016        | 2017      |\n",
    "| 5706.459224   | 75.541109  | 46.136135  | -0.025676  | 2017        | 2018      |\n",
    "| 5592.319461   | 74.781812  | 44.413757  | -0.010614  | 2018        | 2019      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_OPlKa1XvFnH",
   "metadata": {
    "id": "_OPlKa1XvFnH"
   },
   "source": [
    "In addition to the earlier analysis conducted, **a trend shift occurred in 2015**. It clarifies the **'weak' R² outcome in 2015.**\n",
    "\n",
    "In **2016**, the **R² improved 100-fold** *compared to 2015*, suggesting that the LightGBM algorithm has learned. And the R² for 2017 is expected to rise.\n",
    "\n",
    "In **2017**, **the R² improved to twice the value of 2016**; nonetheless, it **remains quite low** and can even be negative.\n",
    "\n",
    "In **2018**, I observe a **similar situation as in 2017**, with a slight rise in **R², yet it remains very low**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qcSZpZT20VoN",
   "metadata": {
    "id": "qcSZpZT20VoN"
   },
   "source": [
    "### **Comments :**\n",
    "\n",
    "Let's verify if the performance indicator is due to the trend shift in 2015.\n",
    "So I will filter the Dataframe with data from 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hpHIbdddqQ7x",
   "metadata": {
    "id": "hpHIbdddqQ7x"
   },
   "source": [
    "## <span style=\"color:blue;\">4. **Experiment 3** Using cache methodology without data smoothing - MACHINE LEARNING USING LIGHTGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wOGNYqy9rCbf",
   "metadata": {
    "id": "wOGNYqy9rCbf"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Initialization</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43Qg5N9TrIYH",
   "metadata": {
    "collapsed": true,
    "id": "43Qg5N9TrIYH",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 1. Install JAVA - SPARK\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "!apt-get install openjdk-11-jdk -y\n",
    "!pip install pyspark==3.5.0\n",
    "!pip install findspark\n",
    "!pip install --force-reinstall numpy==1.26.4 pandas==2.2.2\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] += \":/usr/lib/jvm/java-11-openjdk-amd64/bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PSgosFx9Nsz0",
   "metadata": {
    "id": "PSgosFx9Nsz0"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 2. Spark Configuration\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML_Portfolio_After_2015\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:1.0.0\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.default.parallelism\", \"200\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NUC8aRIJ4tay",
   "metadata": {
    "collapsed": true,
    "id": "NUC8aRIJ4tay",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 3. Install Synapse Optuna\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "!pip install synapseml optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8pgukAGk12zi",
   "metadata": {
    "id": "8pgukAGk12zi"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Import Libraries\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from pyspark.sql.functions import (\n",
    "    col, mean, unix_timestamp, to_date, regexp_replace, when, abs,\n",
    "    dayofweek, month, sum, avg, count, lit, isnan, countDistinct, stddev\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from synapse.ml.lightgbm import LightGBMRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "icUGsDUbs_Z0",
   "metadata": {
    "id": "icUGsDUbs_Z0"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Extraction - Load</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B-De6eg012uv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-De6eg012uv",
    "outputId": "473e7a65-f4cf-4c62-9ee8-6c3f8e83d08f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab cache for faster access to the 'transactions-fraud-datasets' dataset.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 5. Data Loading\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "path = kagglehub.dataset_download(\"computingvictor/transactions-fraud-datasets\")\n",
    "transaction_data = f\"{path}/transactions_data.csv\"\n",
    "mcc_codes_data = f\"{path}/mcc_codes.json\"\n",
    "\n",
    "with open(mcc_codes_data, \"r\") as f:\n",
    "    mcc_dict = json.load(f)\n",
    "mcc = pd.DataFrame.from_dict(mcc_dict, orient=\"index\", columns=[\"description\"]).reset_index().rename(columns={\"index\": \"mcc_code\"})\n",
    "\n",
    "mcc_spark = spark.createDataFrame(mcc)\n",
    "td_spark = spark.read.csv(transaction_data, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ybq--JBGWL1A",
   "metadata": {
    "id": "Ybq--JBGWL1A"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Data Wrangling</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usQ_mwKf12sW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usQ_mwKf12sW",
    "outputId": "678ca675-5a8b-42dd-84c8-706a3f6dac6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded : 6,734,248 rows\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 6. Feature Engineering\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "LABEL_COL = \"amount\"\n",
    "CATEGORICAL_FEATURE_COL = \"description\"\n",
    "CLEANED_CATEGORICAL_COL = \"description_clean\"\n",
    "\n",
    "td_spark = td_spark.withColumn(LABEL_COL, regexp_replace(LABEL_COL, \"\\$\", \"\"))\n",
    "td_spark = td_spark.withColumn(LABEL_COL, col(LABEL_COL).cast(DoubleType()))\n",
    "td_spark = td_spark.withColumn(\"date\", to_date(\"date\"))\n",
    "\n",
    "df = td_spark.join(mcc_spark, td_spark.mcc == mcc_spark.mcc_code, \"left\")\n",
    "df = df.drop(\"id\", \"client_id\", \"card_id\", \"use_chip\", \"merchant_id\", \"merchant_city\",\n",
    "             \"merchant_state\", \"zip\", \"mcc\", \"errors\", \"mcc_code\")\n",
    "\n",
    "df = df.withColumn(\"date_timestamp\", unix_timestamp(col(\"date\")).cast(DoubleType()))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"date\")).cast(DoubleType()))\n",
    "df = df.withColumn(\"month\", month(col(\"date\")).cast(DoubleType()))\n",
    "df = df.withColumn(\"is_refund\", when(col(LABEL_COL) < 0, 1.0).otherwise(0.0).cast(DoubleType()))\n",
    "df = df.withColumn(LABEL_COL, when(col(LABEL_COL).isNull(), 0.0).otherwise(abs(col(LABEL_COL))))\n",
    "df = df.withColumn(CLEANED_CATEGORICAL_COL, regexp_replace(col(CATEGORICAL_FEATURE_COL), \"[ ,:\\\\[\\\\]\\\\{\\\\}]\", \"_\"))\n",
    "\n",
    "# Filter period of interest\n",
    "df = df.filter((col(\"date\") >= \"2015-01-01\") & (col(\"date\") <= \"2019-10-31\"))\n",
    "df = df.cache()\n",
    "print(f\"Dataset loaded : {df.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D0RLUfCC12p8",
   "metadata": {
    "id": "D0RLUfCC12p8"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 7. Target Encoding\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "def target_encode_optimized(train_df, test_df, cat_col, label_col):\n",
    "    # Calcul of average per category\n",
    "    mean_encoding = train_df.groupBy(cat_col).agg(avg(label_col).alias(\"mean_encoded\"))\n",
    "    mean_encoding = mean_encoding.cache()\n",
    "    mean_encoding.count()\n",
    "\n",
    "    # Global Average for empty or unknown values\n",
    "    global_mean = train_df.select(avg(label_col)).collect()[0][0]\n",
    "\n",
    "    # Joining train tables\n",
    "    train_encoded = train_df.join(mean_encoding, cat_col, \"left\") \\\n",
    "        .withColumn(\"mean_encoded\", when(col(\"mean_encoded\").isNull(), global_mean).otherwise(col(\"mean_encoded\")))\n",
    "\n",
    "    test_encoded = test_df.join(mean_encoding, cat_col, \"left\") \\\n",
    "        .withColumn(\"mean_encoded\", when(col(\"mean_encoded\").isNull(), global_mean).otherwise(col(\"mean_encoded\")))\n",
    "\n",
    "    # Return as Spark DataFrames\n",
    "    return train_encoded, test_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ib7H8XBt12nc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ib7H8XBt12nc",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "43e8ddbb-3f04-49cf-d5c1-fe623f0d1cc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 12:52:17,982] A new study created in memory with name: no-name-58c54689-9de5-497f-8446-dda4951e6ef4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna Optimization start on 1% data sample...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 12:52:21,139] Trial 0 finished with value: 51.0878380411952 and parameters: {'num_leaves': 115, 'max_depth': 15, 'learning_rate': 0.05395030966670229, 'feature_fraction': 0.8795975452591109, 'bagging_fraction': 0.7468055921327309, 'num_iterations': 324}. Best is trial 0 with value: 51.0878380411952.\n",
      "[I 2025-11-03 12:52:29,643] Trial 1 finished with value: 50.91268696034915 and parameters: {'num_leaves': 44, 'max_depth': 14, 'learning_rate': 0.039913058785616795, 'feature_fraction': 0.9124217733388136, 'bagging_fraction': 0.7061753482887407, 'num_iterations': 976}. Best is trial 1 with value: 50.91268696034915.\n",
      "[I 2025-11-03 12:52:35,277] Trial 2 finished with value: 56.89118695944336 and parameters: {'num_leaves': 219, 'max_depth': 7, 'learning_rate': 0.015199348301309814, 'feature_fraction': 0.7550213529560301, 'bagging_fraction': 0.7912726728878613, 'num_iterations': 620}. Best is trial 1 with value: 50.91268696034915.\n",
      "[I 2025-11-03 12:52:41,472] Trial 3 finished with value: 54.14778484988534 and parameters: {'num_leaves': 128, 'max_depth': 8, 'learning_rate': 0.04091220574443785, 'feature_fraction': 0.7418481581956126, 'bagging_fraction': 0.7876433945605654, 'num_iterations': 493}. Best is trial 1 with value: 50.91268696034915.\n",
      "[I 2025-11-03 12:52:43,832] Trial 4 finished with value: 54.816848756407616 and parameters: {'num_leaves': 134, 'max_depth': 13, 'learning_rate': 0.015837031559118753, 'feature_fraction': 0.8542703315240835, 'bagging_fraction': 0.8777243706586128, 'num_iterations': 237}. Best is trial 1 with value: 50.91268696034915.\n",
      "[I 2025-11-03 12:52:50,961] Trial 5 finished with value: 57.934248639029136 and parameters: {'num_leaves': 168, 'max_depth': 6, 'learning_rate': 0.011615865989246453, 'feature_fraction': 0.984665661176, 'bagging_fraction': 0.9896896099223678, 'num_iterations': 847}. Best is trial 1 with value: 50.91268696034915.\n",
      "[I 2025-11-03 12:52:55,285] Trial 6 finished with value: 55.208056200558005 and parameters: {'num_leaves': 99, 'max_depth': 6, 'learning_rate': 0.04833180632488466, 'feature_fraction': 0.8320457481218804, 'bagging_fraction': 0.7366114704534336, 'num_iterations': 596}. Best is trial 1 with value: 50.91268696034915.\n",
      "[I 2025-11-03 12:53:00,376] Trial 7 finished with value: 55.22576977491358 and parameters: {'num_leaves': 38, 'max_depth': 15, 'learning_rate': 0.01814596135349025, 'feature_fraction': 0.8987566853061946, 'bagging_fraction': 0.7935133228268233, 'num_iterations': 616}. Best is trial 1 with value: 50.91268696034915.\n",
      "[I 2025-11-03 12:53:11,014] Trial 8 finished with value: 47.56964723236516 and parameters: {'num_leaves': 154, 'max_depth': 7, 'learning_rate': 0.09323621351781479, 'feature_fraction': 0.9325398470083344, 'bagging_fraction': 0.9818496824692567, 'num_iterations': 916}. Best is trial 8 with value: 47.56964723236516.\n",
      "[I 2025-11-03 12:53:18,839] Trial 9 finished with value: 53.38937114942398 and parameters: {'num_leaves': 166, 'max_depth': 15, 'learning_rate': 0.012260057359187526, 'feature_fraction': 0.7587948587257435, 'bagging_fraction': 0.7135681866731614, 'num_iterations': 460}. Best is trial 8 with value: 47.56964723236516.\n",
      "[I 2025-11-03 12:53:32,208] Trial 10 finished with value: 42.45893664603577 and parameters: {'num_leaves': 244, 'max_depth': 10, 'learning_rate': 0.09322601292624816, 'feature_fraction': 0.9833497436436693, 'bagging_fraction': 0.9951622648438584, 'num_iterations': 831}. Best is trial 10 with value: 42.45893664603577.\n",
      "[I 2025-11-03 12:53:45,608] Trial 11 finished with value: 42.528694536927645 and parameters: {'num_leaves': 252, 'max_depth': 10, 'learning_rate': 0.09611047754271736, 'feature_fraction': 0.9992041511679673, 'bagging_fraction': 0.9987405662073933, 'num_iterations': 821}. Best is trial 10 with value: 42.45893664603577.\n",
      "[I 2025-11-03 12:53:59,544] Trial 12 finished with value: 41.06735413468018 and parameters: {'num_leaves': 251, 'max_depth': 11, 'learning_rate': 0.09862929796873673, 'feature_fraction': 0.9911190152534857, 'bagging_fraction': 0.9069671270049076, 'num_iterations': 792}. Best is trial 12 with value: 41.06735413468018.\n",
      "[I 2025-11-03 12:54:12,366] Trial 13 finished with value: 44.392947655039926 and parameters: {'num_leaves': 216, 'max_depth': 11, 'learning_rate': 0.06858148658869345, 'feature_fraction': 0.9589180921395233, 'bagging_fraction': 0.912027207995444, 'num_iterations': 753}. Best is trial 12 with value: 41.06735413468018.\n",
      "[I 2025-11-03 12:54:23,793] Trial 14 finished with value: 50.39377342768146 and parameters: {'num_leaves': 254, 'max_depth': 11, 'learning_rate': 0.029279640357738394, 'feature_fraction': 0.9510467545997464, 'bagging_fraction': 0.9320281882773551, 'num_iterations': 734}. Best is trial 12 with value: 41.06735413468018.\n",
      "[I 2025-11-03 12:54:33,140] Trial 15 finished with value: 49.160207370925775 and parameters: {'num_leaves': 207, 'max_depth': 9, 'learning_rate': 0.06943645170583447, 'feature_fraction': 0.8059496586274635, 'bagging_fraction': 0.940932769738597, 'num_iterations': 728}. Best is trial 12 with value: 41.06735413468018.\n",
      "[I 2025-11-03 12:54:50,614] Trial 16 finished with value: 48.813241565818956 and parameters: {'num_leaves': 195, 'max_depth': 12, 'learning_rate': 0.027243832983309575, 'feature_fraction': 0.9711614918259046, 'bagging_fraction': 0.8595848791921131, 'num_iterations': 977}. Best is trial 12 with value: 41.06735413468018.\n",
      "[I 2025-11-03 12:55:02,262] Trial 17 finished with value: 46.75212792503759 and parameters: {'num_leaves': 235, 'max_depth': 9, 'learning_rate': 0.06998487037074656, 'feature_fraction': 0.9274670851039221, 'bagging_fraction': 0.8958797187906674, 'num_iterations': 836}. Best is trial 12 with value: 41.06735413468018.\n",
      "[I 2025-11-03 12:55:13,403] Trial 18 finished with value: 41.7301024437823 and parameters: {'num_leaves': 180, 'max_depth': 12, 'learning_rate': 0.09609176196016234, 'feature_fraction': 0.9942249576279402, 'bagging_fraction': 0.955604905780556, 'num_iterations': 680}. Best is trial 12 with value: 41.06735413468018.\n",
      "[I 2025-11-03 12:55:22,577] Trial 19 finished with value: 48.761538492362035 and parameters: {'num_leaves': 190, 'max_depth': 13, 'learning_rate': 0.059217852884309585, 'feature_fraction': 0.7965876180576154, 'bagging_fraction': 0.9521611575184935, 'num_iterations': 499}. Best is trial 12 with value: 41.06735413468018.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'num_leaves': 251, 'max_depth': 11, 'learning_rate': 0.09862929796873673, 'feature_fraction': 0.9911190152534857, 'bagging_fraction': 0.9069671270049076, 'num_iterations': 792}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 8. Bayesian Optimization (Automatic best hyperparameters) base on 1% data sample\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "FEATURES_COL = [\n",
    "    \"date_timestamp\", \"day_of_week\", \"month\", \"is_refund\",\n",
    "    CLEANED_CATEGORICAL_COL\n",
    "]\n",
    "\n",
    "# Creation 1% sample\n",
    "sample_df = df.sample(False, 0.01, seed=42)\n",
    "sample_pd = sample_df.toPandas()\n",
    "\n",
    "# Vérifie que la colonne catégorielle existe, sinon tente de la récupérer\n",
    "if CLEANED_CATEGORICAL_COL not in sample_pd.columns and CATEGORICAL_FEATURE_COL in sample_pd.columns:\n",
    "    sample_pd[CLEANED_CATEGORICAL_COL] = sample_pd[CATEGORICAL_FEATURE_COL]\n",
    "\n",
    "# Si la colonne catégorielle existe, on la convertit en catégorie\n",
    "if CLEANED_CATEGORICAL_COL in sample_pd.columns:\n",
    "    sample_pd[CLEANED_CATEGORICAL_COL] = sample_pd[CLEANED_CATEGORICAL_COL].astype(\"category\")\n",
    "    sample_pd[f\"{CLEANED_CATEGORICAL_COL}_code\"] = sample_pd[CLEANED_CATEGORICAL_COL].cat.codes\n",
    "    sample_features = [\"date_timestamp\", \"day_of_week\", \"month\", \"is_refund\", f\"{CLEANED_CATEGORICAL_COL}_code\"]\n",
    "else:\n",
    "    sample_features = [\"date_timestamp\", \"day_of_week\", \"month\", \"is_refund\"]\n",
    "\n",
    "X = sample_pd[sample_features]\n",
    "y = sample_pd[LABEL_COL]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Optuna objective function\n",
    "# ----------------------------------------------------\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 256),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 15),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.7, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.7, 1.0),\n",
    "        \"num_iterations\": trial.suggest_int(\"num_iterations\", 200, 1000),\n",
    "        \"verbose\": -1,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(X, label=y)\n",
    "    model = lgb.train(params, train_data)\n",
    "    preds = model.predict(X)\n",
    "    rmse = ((preds - y) ** 2).mean() ** 0.5\n",
    "    return rmse\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Run optimization\n",
    "# ----------------------------------------------------\n",
    "print(\"\\nOptuna Optimization start on 1% data sample...\")\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=20)\n",
    "best_params = study.best_params\n",
    "print(f\"Best hyperparameters:\\n{best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v02TT2FBWVmw",
   "metadata": {
    "id": "v02TT2FBWVmw"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Machine Learning</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5xbAFt3A12k1",
   "metadata": {
    "id": "5xbAFt3A12k1"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 9. Optimized train data\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# The previous model was really slow. The purpose of the new optimized version below is to decrease the training duration.\n",
    "\n",
    "def train_with_best_params_optimized(train_df, test_df, params):\n",
    "    final_features = [\"date_timestamp\", \"day_of_week\", \"month\", \"is_refund\", \"mean_encoded\"]\n",
    "\n",
    "    # Verification of columns in training dataframe\n",
    "    for c in final_features + [LABEL_COL]:\n",
    "        if c not in train_df.columns:\n",
    "            raise ValueError(f\"Missing column {c} in training data\")\n",
    "\n",
    "    # OPTIMISATION 1 : Cleaning and push it to cache\n",
    "    def clean_and_cache(dframe):\n",
    "        df_local = dframe\n",
    "        for c in final_features + [LABEL_COL]:\n",
    "            df_local = df_local.filter(~isnan(col(c)) & col(c).isNotNull())\n",
    "        df_local = df_local.cache()\n",
    "        return df_local\n",
    "\n",
    "    train_df = clean_and_cache(train_df)\n",
    "    test_df = clean_and_cache(test_df)\n",
    "\n",
    "    # OPTIMISATION 2 : Verification of the data volume\n",
    "    train_count = train_df.count()\n",
    "    if train_count < 1000:\n",
    "        print(\"Too small dataframe, split ignored\")\n",
    "        train_df.unpersist()\n",
    "        test_df.unpersist()\n",
    "        return {\"mse\": None, \"rmse\": None, \"mae\": None, \"r2\": None}\n",
    "\n",
    "    # OPTIMISATION 3 : Variance check\n",
    "    stats_df = train_df.select([countDistinct(col(f)).alias(f\"distinct_{f}\") for f in final_features] +\n",
    "                                [stddev(col(f)).alias(f\"std_{f}\") for f in final_features])\n",
    "    stats = stats_df.collect()[0].asDict()\n",
    "\n",
    "    valid_features = []\n",
    "    for f in final_features:\n",
    "        unique_val = stats.get(f\"distinct_{f}\", 0)\n",
    "        std_val = stats.get(f\"std_{f}\", None)\n",
    "        if unique_val is not None and unique_val > 1 and std_val is not None and std_val > 0:\n",
    "            valid_features.append(f)\n",
    "        else:\n",
    "            print(f\"Feature '{f}' deleted due to null variance or single value\")\n",
    "\n",
    "    if len(valid_features) == 0:\n",
    "        print(\"No valid feature, ignored split.\")\n",
    "        train_df.unpersist()\n",
    "        test_df.unpersist()\n",
    "        return {\"mse\": None, \"rmse\": None, \"mae\": None, \"r2\": None}\n",
    "\n",
    "    # Assembler\n",
    "    assembler = VectorAssembler(inputCols=valid_features, outputCol=\"features_vector\", handleInvalid=\"skip\")\n",
    "    pipeline = Pipeline(stages=[assembler])\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "    train_data = pipeline_model.transform(train_df).cache()\n",
    "    test_data = pipeline_model.transform(test_df).cache()\n",
    "\n",
    "    # OPTIMISATION 4 : Vector verification\n",
    "    sample_check = train_data.select(\"features_vector\").limit(10).collect()\n",
    "    if len(sample_check) == 0:\n",
    "        print(\"No data after transformation\")\n",
    "        train_df.unpersist()\n",
    "        test_df.unpersist()\n",
    "        train_data.unpersist()\n",
    "        test_data.unpersist()\n",
    "        return {\"mse\": None, \"rmse\": None, \"mae\": None, \"r2\": None}\n",
    "\n",
    "    # LightGBM optimized hyperparameters (SynapseML LightGBMRegressor)\n",
    "    lgbm = LightGBMRegressor(\n",
    "        featuresCol=\"features_vector\",\n",
    "        labelCol=LABEL_COL,\n",
    "        objective=\"regression\",\n",
    "        numLeaves=int(params.get(\"num_leaves\", 31)),\n",
    "        maxDepth=int(params.get(\"max_depth\", 6)),\n",
    "        learningRate=float(params.get(\"learning_rate\", 0.1)),\n",
    "        featureFraction=float(params.get(\"feature_fraction\", 1.0)),\n",
    "        baggingFraction=float(params.get(\"bagging_fraction\", 1.0)),\n",
    "        numIterations=int(params.get(\"num_iterations\", 100)),\n",
    "        verbosity=-1,\n",
    "        numThreads=4,\n",
    "    )\n",
    "\n",
    "    print(f\"Training on {train_count:,} rows...\")\n",
    "    model = lgbm.fit(train_data)\n",
    "    preds = model.transform(test_data)\n",
    "\n",
    "    evaluator = RegressionEvaluator(labelCol=LABEL_COL, predictionCol=\"prediction\")\n",
    "    metrics = {\n",
    "        \"mse\": evaluator.evaluate(preds, {evaluator.metricName: \"mse\"}),\n",
    "        \"rmse\": evaluator.evaluate(preds, {evaluator.metricName: \"rmse\"}),\n",
    "        \"mae\": evaluator.evaluate(preds, {evaluator.metricName: \"mae\"}),\n",
    "        \"r2\": evaluator.evaluate(preds, {evaluator.metricName: \"r2\"}),\n",
    "    }\n",
    "\n",
    "    # Cleaning memory\n",
    "    train_df.unpersist()\n",
    "    test_df.unpersist()\n",
    "    train_data.unpersist()\n",
    "    test_data.unpersist()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DG4E_0cf-cS7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DG4E_0cf-cS7",
    "outputId": "f2934c36-94f0-4687-c871-5392fe2f1956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Temporal Split : Train ≤ 2015-12-31 | Test > 2015-12-31\n",
      "============================================================\n",
      "Train: 1,388,065 | Test: 5,346,183\n",
      "Target encoding...\n",
      "Training on 1,388,065 rows...\n",
      "Result 2016 → R² = 0.5509 | RMSE = 50.13\n",
      "\n",
      "============================================================\n",
      "Temporal Split : Train ≤ 2016-12-31 | Test > 2016-12-31\n",
      "============================================================\n",
      "Train: 2,780,182 | Test: 3,954,066\n",
      "Target encoding...\n",
      "Training on 2,780,182 rows...\n",
      "Result 2017 → R² = 0.5542 | RMSE = 49.81\n",
      "\n",
      "============================================================\n",
      "Temporal Split : Train ≤ 2017-12-31 | Test > 2017-12-31\n",
      "============================================================\n",
      "Train: 4,179,490 | Test: 2,554,758\n",
      "Target encoding...\n",
      "Training on 4,179,490 rows...\n",
      "Result 2018 → R² = 0.5547 | RMSE = 49.77\n",
      "\n",
      "============================================================\n",
      "Temporal Split : Train ≤ 2018-12-31 | Test > 2018-12-31\n",
      "============================================================\n",
      "Train: 5,574,282 | Test: 1,159,966\n",
      "Target encoding...\n",
      "Training on 5,574,282 rows...\n",
      "Result 2019 → R² = 0.5530 | RMSE = 49.74\n",
      "\n",
      "Final results:\n",
      "           mse       rmse        mae        r2  train_end  test_year\n",
      "0  2513.398113  50.133802  29.311969  0.550948       2015       2016\n",
      "1  2480.598657  49.805609  29.263845  0.554244       2016       2017\n",
      "2  2477.255706  49.772037  29.204425  0.554739       2017       2018\n",
      "3  2473.638377  49.735685  29.221757  0.552976       2018       2019\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 8. Rolling Temporal Validation OPTIMISÉ\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "split_years = [2015, 2016, 2017, 2018]\n",
    "results = []\n",
    "\n",
    "for split_year in split_years:\n",
    "    split_date = f\"{split_year}-12-31\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Temporal Split : Train ≤ {split_date} | Test > {split_date}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # OPTIMISATION 5 : Filter and push to cache\n",
    "    train_df = df.filter(col(\"date\") <= split_date).cache()\n",
    "    test_df = df.filter(col(\"date\") > split_date).cache()\n",
    "\n",
    "    train_count = train_df.count()\n",
    "    test_count = test_df.count()\n",
    "\n",
    "    print(f\"Train: {train_count:,} | Test: {test_count:,}\")\n",
    "\n",
    "    if train_count < 1000 or test_count < 1000:\n",
    "        print(f\"Not enough data for year {split_year}, skipping...\")\n",
    "        train_df.unpersist()\n",
    "        test_df.unpersist()\n",
    "        continue\n",
    "\n",
    "    # Target Encoding\n",
    "    print(f\"Target encoding...\")\n",
    "    train_encoded, test_encoded = target_encode_optimized(train_df, test_df, CLEANED_CATEGORICAL_COL, LABEL_COL)\n",
    "    train_encoded = train_encoded.cache()\n",
    "    test_encoded = test_encoded.cache()\n",
    "\n",
    "    # Training\n",
    "    metrics = train_with_best_params_optimized(train_encoded, test_encoded, best_params)\n",
    "\n",
    "    # Defensive: ensure metrics is a dict\n",
    "    if not isinstance(metrics, dict):\n",
    "        # Try to convert if it's a tuple/list with expected order; otherwise wrap\n",
    "        if isinstance(metrics, (tuple, list)) and len(metrics) >= 4:\n",
    "            metrics = {\n",
    "                \"mse\": metrics[0],\n",
    "                \"rmse\": metrics[1],\n",
    "                \"mae\": metrics[2],\n",
    "                \"r2\": metrics[3]\n",
    "            }\n",
    "        else:\n",
    "            metrics = {\"mse\": None, \"rmse\": None, \"mae\": None, \"r2\": None}\n",
    "            print(\"Warning: metrics returned an unexpected type; normalized to None values.\")\n",
    "\n",
    "    # Add metadata about split\n",
    "    metrics[\"train_end\"] = split_year\n",
    "    metrics[\"test_year\"] = split_year + 1\n",
    "\n",
    "    if metrics[\"r2\"] is not None:\n",
    "        print(f\"Result {split_year+1} → R² = {metrics['r2']:.4f} | RMSE = {metrics['rmse']:.2f}\")\n",
    "\n",
    "    results.append(metrics)\n",
    "\n",
    "    # Cleaning\n",
    "    train_df.unpersist()\n",
    "    test_df.unpersist()\n",
    "    # target_encode returned cached objects\n",
    "    try:\n",
    "        train_encoded.unpersist()\n",
    "        test_encoded.unpersist()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Final results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nFinal results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NJtIQ0HVm9Id",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJtIQ0HVm9Id",
    "outputId": "4de29bd6-2c6d-4fb8-d6d7-6dbb9ab81a04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULT\n",
      "============================================================\n",
      "        mse      rmse       mae       r2  train_end  test_year\n",
      "2513.398113 50.133802 29.311969 0.550948       2015       2016\n",
      "2480.598657 49.805609 29.263845 0.554244       2016       2017\n",
      "2477.255706 49.772037 29.204425 0.554739       2017       2018\n",
      "2473.638377 49.735685 29.221757 0.552976       2018       2019\n",
      "\n",
      " Average on 4 years:\n",
      " Average R² : 0.5532\n",
      " Average RMSE : 49.86\n",
      " Average MAE : 29.25\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 8. Display final Results\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Metrics\n",
    "valid_results = results_df[results_df['r2'].notna()]\n",
    "if len(valid_results) > 0:\n",
    "    print(f\"\\n Average on {len(valid_results)} years:\")\n",
    "    print(f\" Average R² : {valid_results['r2'].mean():.4f}\")\n",
    "    print(f\" Average RMSE : {valid_results['rmse'].mean():.2f}\")\n",
    "    print(f\" Average MAE : {valid_results['mae'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PZwgX5QBZjdm",
   "metadata": {
    "id": "PZwgX5QBZjdm"
   },
   "source": [
    "### **Comments :**\n",
    "\n",
    "As planned by filtering the dataframe after 2015, the result is much better.\n",
    "However, an R² of 0.55 is not enough satisfying in my opinion.\n",
    "\n",
    "To obtain better results, there is 2 options:\n",
    ">>> **Add addtional data** for the LightGBM algorithm to gain more insights\n",
    "\n",
    ">>> Go back on the first version and **aggregate data** on a monthly periodicity.\n",
    "\n",
    "Let's return to the first needs: Predict the monthly transaction amount (high level view).\n",
    "So For the next experiment, I will revisit the initial idea and aggregate data without lag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wqWbNdpckbHb",
   "metadata": {
    "id": "wqWbNdpckbHb"
   },
   "source": [
    "## <span style=\"color:blue;\">5. **Experiment 4** with Monthly aggregation - MACHINE LEARNING USING LIGHTGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E69ZiVChmKDr",
   "metadata": {
    "id": "E69ZiVChmKDr"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Initialization</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wHv6CzD8l--i",
   "metadata": {
    "id": "wHv6CzD8l--i"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 1. Install JAVA - SPARK\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "!apt-get install openjdk-11-jdk -y\n",
    "!pip install pyspark==3.5.0\n",
    "!pip install findspark\n",
    "!pip install --force-reinstall numpy==1.26.4 pandas==2.2.2\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] += \":/usr/lib/jvm/java-11-openjdk-amd64/bin\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bGBOAVgwaCfY",
   "metadata": {
    "id": "bGBOAVgwaCfY"
   },
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 2. Spark init\n",
    "# ----------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML_Portfolio_Monthly_Aggregated_Prediction\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NxAzgnRhl-6z",
   "metadata": {
    "id": "NxAzgnRhl-6z"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 3. Install Synapse Optuna\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "!pip install synapseml optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ORPN6eF3l-1h",
   "metadata": {
    "id": "ORPN6eF3l-1h"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Import Libraries\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from math import pi\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_date, regexp_replace, unix_timestamp, dayofweek, month, year, sin, cos, lit,\n",
    "    sum as _sum, avg as _avg, count as _count, stddev as _stddev, countDistinct,\n",
    "    when, expr, max as _max\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import sklearn.metrics\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ipkYY7JFUPG",
   "metadata": {
    "id": "7ipkYY7JFUPG"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Extraction - Load</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3RULNCGSaE7C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RULNCGSaE7C",
    "outputId": "b8e64615-d1a6-4059-e2a3-7f714e468835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/computingvictor/transactions-fraud-datasets?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348M/348M [00:02<00:00, 138MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 5. Data Loading\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "path = kagglehub.dataset_download(\"computingvictor/transactions-fraud-datasets\")\n",
    "transaction_data = f\"{path}/transactions_data.csv\"\n",
    "mcc_codes_data = f\"{path}/mcc_codes.json\"\n",
    "\n",
    "# load mcc descriptions\n",
    "with open(mcc_codes_data, \"r\") as f:\n",
    "    mcc_dict = json.load(f)\n",
    "mcc = pd.DataFrame.from_dict(mcc_dict, orient=\"index\", columns=[\"description\"]) \\\n",
    "    .reset_index().rename(columns={\"index\": \"mcc_code\"})\n",
    "mcc_spark = spark.createDataFrame(mcc)\n",
    "\n",
    "# load transactions\n",
    "td_spark = spark.read.csv(transaction_data, header=True, inferSchema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JKwmi2qSWaZn",
   "metadata": {
    "id": "JKwmi2qSWaZn"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Data Wrangling</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eFZf9Zj7aHej",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFZf9Zj7aHej",
    "outputId": "ce39b0ee-8e83-4bff-df27-719195f0cf2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after filter: 6734248\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 6. Feature Engineering\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "LABEL_COL = \"amount\"\n",
    "\n",
    "td_spark = td_spark.withColumn(LABEL_COL, regexp_replace(col(LABEL_COL), \"\\\\$\", \"\"))\n",
    "td_spark = td_spark.withColumn(LABEL_COL, col(LABEL_COL).cast(DoubleType()))\n",
    "td_spark = td_spark.withColumn(\"date\", to_date(col(\"date\")))\n",
    "\n",
    "df = td_spark.join(mcc_spark, td_spark.mcc == mcc_spark.mcc_code, \"left\")\n",
    "\n",
    "cols_to_drop = [\"id\", \"use_chip\", \"merchant_city\", \"merchant_state\", \"zip\", \"mcc\", \"errors\", \"mcc_code\"]\n",
    "for c in cols_to_drop:\n",
    "    if c in df.columns:\n",
    "        df = df.drop(c)\n",
    "\n",
    "df = df.withColumn(\"year\", year(col(\"date\")).cast(IntegerType()))\n",
    "df = df.withColumn(\"month\", month(col(\"date\")).cast(IntegerType()))\n",
    "df = df.withColumn(\"date_timestamp\", unix_timestamp(col(\"date\")).cast(DoubleType()))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"date\")).cast(IntegerType()))\n",
    "df = df.withColumn(\"is_refund\", when(col(LABEL_COL) < 0, 1.0).otherwise(0.0).cast(DoubleType()))\n",
    "df = df.withColumn(\"description_clean\", regexp_replace(col(\"description\"), \"[ ,:\\\\[\\\\]\\\\{\\\\}]\", \"_\"))\n",
    "df = df.filter((col(\"date\") >= \"2015-01-01\") & (col(\"date\") <= \"2019-10-31\")).cache()\n",
    "\n",
    "print(\"Rows after filter:\", df.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KskmZh1iaKED",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KskmZh1iaKED",
    "outputId": "d17a0853-49d7-4dcf-9b0a-eac106e53f48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly aggregated rows: 606168\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 7. Monthly aggregation per merchant_id\n",
    "# ----------------------\n",
    "\n",
    "# Aggregation per (year, month, merchant_id)\n",
    "group_cols = [\"merchant_id\", \"year\", \"month\"]\n",
    "\n",
    "agg = df.groupBy(*group_cols).agg(\n",
    "    _sum(LABEL_COL).alias(\"amount_sum\"),\n",
    "    _count(\"*\").alias(\"tx_count\"),\n",
    "    _avg(LABEL_COL).alias(\"amount_avg\"),\n",
    "    _stddev(LABEL_COL).alias(\"amount_stddev\"),\n",
    "    _avg(\"is_refund\").alias(\"refund_rate\"),\n",
    "    countDistinct(\"description_clean\").alias(\"desc_count\")\n",
    ")\n",
    "\n",
    "# fill null stddev with 0\n",
    "agg = agg.fillna({\"amount_stddev\": 0.0})\n",
    "\n",
    "# add ym numeric for splitting and features\n",
    "agg = agg.withColumn(\"ym\", col(\"year\") * 100 + col(\"month\"))\n",
    "\n",
    "# temporal features\n",
    "# month cyclic\n",
    "agg = agg.withColumn(\"month_sin\", sin(2 * pi * col(\"month\") / lit(12)))\n",
    "agg = agg.withColumn(\"month_cos\", cos(2 * pi * col(\"month\") / lit(12)))\n",
    "\n",
    "# normalized year (year - min_year)\n",
    "min_year = agg.agg({\"year\": \"min\"}).collect()[0][0]\n",
    "agg = agg.withColumn(\"year_norm\", (col(\"year\") - lit(min_year)).cast(DoubleType()))\n",
    "\n",
    "# log target\n",
    "agg = agg.withColumn(\"log_amount_sum\", when(col(\"amount_sum\") >= 0, col(\"amount_sum\")).otherwise(0.0))\n",
    "agg = agg.withColumn(\"log_amount_sum\", expr(\"log(1 + log_amount_sum)\"))\n",
    "\n",
    "# cache\n",
    "agg = agg.cache()\n",
    "print(\"Monthly aggregated rows:\", agg.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cu8-DtP9aMkx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cu8-DtP9aMkx",
    "outputId": "425b2067-55d3-477e-cca7-97fde53b8995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ym available: 201910\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 8.  prepare splitting boundaries\n",
    "# ----------------------\n",
    "last_ym = agg.agg(_max(col(\"ym\")).alias(\"last\")).collect()[0][0]\n",
    "print(\"Last ym available:\", last_ym)\n",
    "\n",
    "split_years = [2015, 2016, 2017, 2018]  # will produce tests 2016,2017,2018,2019 (but 2019 months up to available)\n",
    "results = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M6LUCxlPWf5Q",
   "metadata": {
    "id": "M6LUCxlPWf5Q"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Machine Learning</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BFyxWn4taOhI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2804,
     "referenced_widgets": [
      "c122adc3d0704fa787a3786903b0cd8c",
      "260d43957511421e85801a34594b8ce5",
      "4588d23f8fee42329a275d0847c49017",
      "89a570689a4f4c6594cc70ba52865512",
      "fbe1f42aaa6143e1980a7ab0864a7968",
      "b9f99fd9dbcc451995f5f1f0c84af8b3",
      "c172d9985ca74f45ba2d6425740330ec",
      "5f96b46594dd4fc8bc594198090c94ee",
      "4c418896eae64b1f810205252b368407",
      "c7c5a76808734804a36481ca112dfd1b",
      "8fb82fd7dd8648e8b553f64e110459e9",
      "39c40b09e72f49a7806f3168f38d992e",
      "0cc047180a244e9facabac41a377467a",
      "df6c31d7bea9429996562bb960b4b3db",
      "5da36976cc05474eb2354be21869482a",
      "ecb450d2e85f49fcad0a876ede01a3d0",
      "1b9787128a2b427ab3062833b6c6b0e4",
      "28cb511fc0e048a09bf8d17101f98f7b",
      "473239fe8ba0487b9c52cc19f9e36cb4",
      "0e9af5d0bda54a5a8f23fec973f77c16",
      "4015ebde9fc34213a820e336b23878c2",
      "145ab244be7a491093242fa5237ae492",
      "943f0a8d2f474eea83a854a778e2f3d1",
      "be2b7da0b74745539c982289d269a79d",
      "4e09a7c7dcd64d31957a44cee696c53f",
      "a291a759baa0417a983bf421e0e12fce",
      "ab51fd8c41ea47e7bcc65783799a4144",
      "814965c43de84d5f8423ca3f9662bb43",
      "00df86f9528d4f8eb261da1f3cda648e",
      "6c85e3568ecc4efb9c30015dc2d5cccf",
      "cbbe47edba474aae8d2b124d816b8578",
      "9ea63bc4851241629d1ae0d4f2b2370c",
      "46ad28ca20b54bcf937d46169c05c72b",
      "2edb0e078f9b46b5a03004292ff5da7a",
      "90ffa8c7e0594fce8d9150e5f9dc8eae",
      "5f667e821a4345ff9f50f2c704e068ad",
      "349d28e7ee164bb8b731b17b89621aa5",
      "f770f9fa9cf1451c9553a54512d86f81",
      "f0f9087914eb48f39a0a35a7545c6a90",
      "15bcde5a206848c8816dcf0b1e28c5ac",
      "f122745f75e147179ab8dd0b01cde1ef",
      "c7db98d3f56e4060bc600a816455f89e",
      "9981a8ef609f4117b0e2051a5f9df1a2",
      "9fd60c586b8e4747a252c7ee1b45e526"
     ]
    },
    "id": "BFyxWn4taOhI",
    "outputId": "ee734ed0-dc92-4c23-c254-3296c26ee49d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Temporal split: train <= 201512 | test: 201601..201612\n",
      "============================================================\n",
      "Train rows: 124,922 | Test rows: 125,226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 20:32:56,898] A new study created in memory with name: no-name-78948e06-112d-436d-81d8-116b7b2266c8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c122adc3d0704fa787a3786903b0cd8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 20:33:07,297] Trial 0 finished with value: 0.060139679274739864 and parameters: {'learning_rate': 0.076490257591669, 'num_leaves': 23, 'max_depth': 5, 'min_child_samples': 28, 'subsample': 0.7939625800229255, 'colsample_bytree': 0.9921470804593396, 'lambda_l1': 8.18989306657841, 'lambda_l2': 4.678278389143211}. Best is trial 0 with value: 0.060139679274739864.\n",
      "[I 2025-11-03 20:33:25,623] Trial 1 finished with value: 0.054651993729588295 and parameters: {'learning_rate': 0.2582710480149162, 'num_leaves': 168, 'max_depth': 8, 'min_child_samples': 39, 'subsample': 0.6461382690670414, 'colsample_bytree': 0.9370774636597896, 'lambda_l1': 2.178264531678218, 'lambda_l2': 2.2964780045043343}. Best is trial 1 with value: 0.054651993729588295.\n",
      "[I 2025-11-03 20:33:44,138] Trial 2 finished with value: 0.13469158160349892 and parameters: {'learning_rate': 0.012868214414757785, 'num_leaves': 244, 'max_depth': 5, 'min_child_samples': 36, 'subsample': 0.5871840558030141, 'colsample_bytree': 0.6450979886951298, 'lambda_l1': 0.1838599122088036, 'lambda_l2': 8.927008303848554}. Best is trial 1 with value: 0.054651993729588295.\n",
      "[I 2025-11-03 20:34:00,854] Trial 3 finished with value: 0.05666457355191319 and parameters: {'learning_rate': 0.0752606992738189, 'num_leaves': 241, 'max_depth': 7, 'min_child_samples': 24, 'subsample': 0.9995157574415425, 'colsample_bytree': 0.7917487595457646, 'lambda_l1': 4.916225832890647, 'lambda_l2': 8.38978959849083}. Best is trial 1 with value: 0.054651993729588295.\n",
      "[I 2025-11-03 20:34:26,700] Trial 4 finished with value: 0.08603365670567281 and parameters: {'learning_rate': 0.011039156212744764, 'num_leaves': 140, 'max_depth': 12, 'min_child_samples': 30, 'subsample': 0.9186162567899065, 'colsample_bytree': 0.9401897503684559, 'lambda_l1': 4.438320552279684, 'lambda_l2': 8.42764818358403}. Best is trial 1 with value: 0.054651993729588295.\n",
      "[I 2025-11-03 20:34:49,651] Trial 5 finished with value: 0.04716510771375575 and parameters: {'learning_rate': 0.1457180135033624, 'num_leaves': 77, 'max_depth': 11, 'min_child_samples': 38, 'subsample': 0.7388625290534151, 'colsample_bytree': 0.704802949302193, 'lambda_l1': 0.9944015462840838, 'lambda_l2': 5.3104527501223195}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:35:12,101] Trial 6 finished with value: 0.06802616268049783 and parameters: {'learning_rate': 0.038488036031760126, 'num_leaves': 91, 'max_depth': 15, 'min_child_samples': 9, 'subsample': 0.6025868842892781, 'colsample_bytree': 0.9404738365024257, 'lambda_l1': 6.902137213138472, 'lambda_l2': 2.640821460068116}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:35:32,599] Trial 7 finished with value: 0.05008619020140666 and parameters: {'learning_rate': 0.08997088136558291, 'num_leaves': 170, 'max_depth': 11, 'min_child_samples': 47, 'subsample': 0.7865346182206077, 'colsample_bytree': 0.60478549501459, 'lambda_l1': 3.909405440754977, 'lambda_l2': 1.3823181748989544}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:35:47,220] Trial 8 finished with value: 0.20704020964182646 and parameters: {'learning_rate': 0.02103664388500847, 'num_leaves': 168, 'max_depth': 5, 'min_child_samples': 31, 'subsample': 0.6296158720190803, 'colsample_bytree': 0.5462004477882092, 'lambda_l1': 7.74582611614044, 'lambda_l2': 7.493150146932866}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:36:04,097] Trial 9 finished with value: 0.047220119403954916 and parameters: {'learning_rate': 0.10743346804621735, 'num_leaves': 249, 'max_depth': 7, 'min_child_samples': 43, 'subsample': 0.5263617914512835, 'colsample_bytree': 0.7741427394724387, 'lambda_l1': 1.123288030684536, 'lambda_l2': 6.541433560291061}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:36:24,096] Trial 10 finished with value: 0.05269197775421956 and parameters: {'learning_rate': 0.29115438455441495, 'num_leaves': 53, 'max_depth': 11, 'min_child_samples': 16, 'subsample': 0.737173879926963, 'colsample_bytree': 0.6975441125831255, 'lambda_l1': 2.968412668221106, 'lambda_l2': 4.7107732115811265}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:36:41,943] Trial 11 finished with value: 0.04780191677522102 and parameters: {'learning_rate': 0.14182580479120738, 'num_leaves': 90, 'max_depth': 9, 'min_child_samples': 49, 'subsample': 0.5004236677867777, 'colsample_bytree': 0.7682814323479417, 'lambda_l1': 0.156734478014668, 'lambda_l2': 6.513611001033408}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:37:07,302] Trial 12 finished with value: 0.048516987180613154 and parameters: {'learning_rate': 0.15577895242798975, 'num_leaves': 204, 'max_depth': 14, 'min_child_samples': 41, 'subsample': 0.5082782326035176, 'colsample_bytree': 0.8289330832636733, 'lambda_l1': 2.0228500319639218, 'lambda_l2': 5.900441768656325}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:37:14,804] Trial 13 finished with value: 0.0907747106015231 and parameters: {'learning_rate': 0.04439198869287272, 'num_leaves': 97, 'max_depth': 3, 'min_child_samples': 43, 'subsample': 0.7075235516773024, 'colsample_bytree': 0.7088185532701846, 'lambda_l1': 1.393271107803828, 'lambda_l2': 3.423979921862637}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:37:31,615] Trial 14 finished with value: 0.057047902212825914 and parameters: {'learning_rate': 0.1444723793819925, 'num_leaves': 54, 'max_depth': 7, 'min_child_samples': 35, 'subsample': 0.8655507893363218, 'colsample_bytree': 0.845174971105226, 'lambda_l1': 5.8766479668655975, 'lambda_l2': 0.17732736596386278}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:37:46,501] Trial 15 finished with value: 0.07026759276393688 and parameters: {'learning_rate': 0.18777130754986673, 'num_leaves': 124, 'max_depth': 13, 'min_child_samples': 23, 'subsample': 0.6816677412032132, 'colsample_bytree': 0.7006390020913578, 'lambda_l1': 9.543947317599919, 'lambda_l2': 9.829430159120575}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:38:03,984] Trial 16 finished with value: 0.05498507217130461 and parameters: {'learning_rate': 0.10604883351548644, 'num_leaves': 201, 'max_depth': 10, 'min_child_samples': 44, 'subsample': 0.55204822648058, 'colsample_bytree': 0.5045124124405821, 'lambda_l1': 3.1842647436909575, 'lambda_l2': 6.257610061658599}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:38:23,030] Trial 17 finished with value: 0.047572200371785535 and parameters: {'learning_rate': 0.05452071238400838, 'num_leaves': 56, 'max_depth': 9, 'min_child_samples': 35, 'subsample': 0.862864062329626, 'colsample_bytree': 0.8625504483606604, 'lambda_l1': 1.2756300744015494, 'lambda_l2': 3.850231644059914}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:38:36,398] Trial 18 finished with value: 0.08240189154776631 and parameters: {'learning_rate': 0.02780720567335787, 'num_leaves': 18, 'max_depth': 7, 'min_child_samples': 49, 'subsample': 0.8063778569502568, 'colsample_bytree': 0.6557699768394809, 'lambda_l1': 0.18835487877062995, 'lambda_l2': 7.188178715132793}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:38:45,790] Trial 19 finished with value: 0.05672340223081547 and parameters: {'learning_rate': 0.11923850307962153, 'num_leaves': 126, 'max_depth': 3, 'min_child_samples': 18, 'subsample': 0.6872861938245265, 'colsample_bytree': 0.7407241644194024, 'lambda_l1': 3.2534034676267085, 'lambda_l2': 5.440348933896855}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:39:02,387] Trial 20 finished with value: 0.055056023853564506 and parameters: {'learning_rate': 0.20519122755517905, 'num_leaves': 207, 'max_depth': 10, 'min_child_samples': 38, 'subsample': 0.9945115481085143, 'colsample_bytree': 0.5806952632588427, 'lambda_l1': 5.797064737722029, 'lambda_l2': 6.967947041179326}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:39:22,675] Trial 21 finished with value: 0.04737529989168915 and parameters: {'learning_rate': 0.0623111529039092, 'num_leaves': 67, 'max_depth': 9, 'min_child_samples': 34, 'subsample': 0.913504529516818, 'colsample_bytree': 0.864081920895445, 'lambda_l1': 1.2984373674518412, 'lambda_l2': 3.9022850030488834}. Best is trial 5 with value: 0.04716510771375575.\n",
      "[I 2025-11-03 20:39:40,471] Trial 22 finished with value: 0.04705012617171667 and parameters: {'learning_rate': 0.06725972573488755, 'num_leaves': 68, 'max_depth': 8, 'min_child_samples': 32, 'subsample': 0.9315612354861995, 'colsample_bytree': 0.8012608340825836, 'lambda_l1': 1.2097928318710112, 'lambda_l2': 4.047713240105348}. Best is trial 22 with value: 0.04705012617171667.\n",
      "[I 2025-11-03 20:39:58,800] Trial 23 finished with value: 0.050729309007483236 and parameters: {'learning_rate': 0.10321344142326924, 'num_leaves': 107, 'max_depth': 8, 'min_child_samples': 45, 'subsample': 0.8504330494057206, 'colsample_bytree': 0.7956196575505581, 'lambda_l1': 2.165101322055048, 'lambda_l2': 5.030958569226565}. Best is trial 22 with value: 0.04705012617171667.\n",
      "[I 2025-11-03 20:40:15,055] Trial 24 finished with value: 0.0466530450260916 and parameters: {'learning_rate': 0.056212793690699765, 'num_leaves': 74, 'max_depth': 6, 'min_child_samples': 41, 'subsample': 0.9501483999696786, 'colsample_bytree': 0.7286220438170226, 'lambda_l1': 0.9104150495563351, 'lambda_l2': 3.0479658216835035}. Best is trial 24 with value: 0.0466530450260916.\n",
      "Best params for split 2015 : {'learning_rate': 0.056212793690699765, 'num_leaves': 74, 'max_depth': 6, 'min_child_samples': 41, 'subsample': 0.9501483999696786, 'colsample_bytree': 0.7286220438170226, 'lambda_l1': 0.9104150495563351, 'lambda_l2': 3.0479658216835035}\n",
      "\n",
      "Metrics for test year 2016:\n",
      "  RMSE: 0.0467\n",
      "  MAE: 0.0231\n",
      "  R2: 0.9990\n",
      "Exported predictions for test year 2016 -> predictions_2016.csv\n",
      "\n",
      "============================================================\n",
      "Temporal split: train <= 201612 | test: 201701..201712\n",
      "============================================================\n",
      "Train rows: 250,148 | Test rows: 125,659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 20:40:49,784] A new study created in memory with name: no-name-719ca7d9-555f-4604-9458-1dbca188f6dc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c40b09e72f49a7806f3168f38d992e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 20:41:23,973] Trial 0 finished with value: 0.04632111219027969 and parameters: {'learning_rate': 0.07160314309209334, 'num_leaves': 182, 'max_depth': 15, 'min_child_samples': 44, 'subsample': 0.9220130681034786, 'colsample_bytree': 0.8933279395314173, 'lambda_l1': 5.744650854749844, 'lambda_l2': 8.809095427704758}. Best is trial 0 with value: 0.04632111219027969.\n",
      "[I 2025-11-03 20:41:47,221] Trial 1 finished with value: 0.06133274606732788 and parameters: {'learning_rate': 0.02836354598707781, 'num_leaves': 241, 'max_depth': 6, 'min_child_samples': 34, 'subsample': 0.5813999060399747, 'colsample_bytree': 0.5865304931638449, 'lambda_l1': 0.84604852483252, 'lambda_l2': 7.187685952505913}. Best is trial 0 with value: 0.04632111219027969.\n",
      "[I 2025-11-03 20:42:17,451] Trial 2 finished with value: 0.05472570254399032 and parameters: {'learning_rate': 0.02493071852912228, 'num_leaves': 193, 'max_depth': 11, 'min_child_samples': 43, 'subsample': 0.8168852172614569, 'colsample_bytree': 0.7202351077497415, 'lambda_l1': 7.455372683218881, 'lambda_l2': 8.935528659743193}. Best is trial 0 with value: 0.04632111219027969.\n",
      "[I 2025-11-03 20:42:43,265] Trial 3 finished with value: 0.04251428673336407 and parameters: {'learning_rate': 0.12818640170696127, 'num_leaves': 59, 'max_depth': 10, 'min_child_samples': 9, 'subsample': 0.7168662539204294, 'colsample_bytree': 0.7728901732821691, 'lambda_l1': 2.194389201711686, 'lambda_l2': 3.607273661885013}. Best is trial 3 with value: 0.04251428673336407.\n",
      "[I 2025-11-03 20:43:16,260] Trial 4 finished with value: 0.054776355751165476 and parameters: {'learning_rate': 0.023011298910066383, 'num_leaves': 92, 'max_depth': 13, 'min_child_samples': 20, 'subsample': 0.789244851098893, 'colsample_bytree': 0.9388011690005287, 'lambda_l1': 7.707761593820298, 'lambda_l2': 7.210909237317958}. Best is trial 3 with value: 0.04251428673336407.\n",
      "[I 2025-11-03 20:43:37,248] Trial 5 finished with value: 0.05433591481776095 and parameters: {'learning_rate': 0.03037489838197789, 'num_leaves': 94, 'max_depth': 6, 'min_child_samples': 29, 'subsample': 0.717453793104114, 'colsample_bytree': 0.7005178114274027, 'lambda_l1': 6.955224594426811, 'lambda_l2': 0.7786236076272601}. Best is trial 3 with value: 0.04251428673336407.\n",
      "[I 2025-11-03 20:43:52,645] Trial 6 finished with value: 0.05513963039075313 and parameters: {'learning_rate': 0.051514912266846534, 'num_leaves': 97, 'max_depth': 4, 'min_child_samples': 38, 'subsample': 0.9091178394700736, 'colsample_bytree': 0.7031970759939535, 'lambda_l1': 7.069666574994876, 'lambda_l2': 6.59866392246636}. Best is trial 3 with value: 0.04251428673336407.\n",
      "[I 2025-11-03 20:44:17,116] Trial 7 finished with value: 0.04909610346327456 and parameters: {'learning_rate': 0.05027686156900511, 'num_leaves': 59, 'max_depth': 9, 'min_child_samples': 19, 'subsample': 0.5768424805746206, 'colsample_bytree': 0.5661067081881774, 'lambda_l1': 8.914355445335874, 'lambda_l2': 3.567147392381258}. Best is trial 3 with value: 0.04251428673336407.\n",
      "[I 2025-11-03 20:44:39,736] Trial 8 finished with value: 0.03873808144906869 and parameters: {'learning_rate': 0.11975950342551953, 'num_leaves': 164, 'max_depth': 7, 'min_child_samples': 28, 'subsample': 0.5210252025864097, 'colsample_bytree': 0.6918706551459641, 'lambda_l1': 1.1282659879477375, 'lambda_l2': 7.190306601860703}. Best is trial 8 with value: 0.03873808144906869.\n",
      "[I 2025-11-03 20:44:53,086] Trial 9 finished with value: 0.2015104964805937 and parameters: {'learning_rate': 0.011867656852350491, 'num_leaves': 253, 'max_depth': 3, 'min_child_samples': 9, 'subsample': 0.9725130416110652, 'colsample_bytree': 0.9110407163774481, 'lambda_l1': 1.6551571686204614, 'lambda_l2': 7.811631986606478}. Best is trial 8 with value: 0.03873808144906869.\n",
      "[I 2025-11-03 20:45:13,352] Trial 10 finished with value: 0.044710827105995166 and parameters: {'learning_rate': 0.2630525295644748, 'num_leaves': 157, 'max_depth': 7, 'min_child_samples': 20, 'subsample': 0.5187548187282549, 'colsample_bytree': 0.5002667303888002, 'lambda_l1': 3.5152829544629736, 'lambda_l2': 4.899879236973753}. Best is trial 8 with value: 0.03873808144906869.\n",
      "[I 2025-11-03 20:45:38,577] Trial 11 finished with value: 0.04255139324245287 and parameters: {'learning_rate': 0.1712800796087069, 'num_leaves': 50, 'max_depth': 9, 'min_child_samples': 6, 'subsample': 0.6924934394210798, 'colsample_bytree': 0.7970053043762927, 'lambda_l1': 3.009026871948892, 'lambda_l2': 2.354677485207648}. Best is trial 8 with value: 0.03873808144906869.\n",
      "[I 2025-11-03 20:45:53,442] Trial 12 finished with value: 0.041553189920725396 and parameters: {'learning_rate': 0.12075393404300924, 'num_leaves': 16, 'max_depth': 12, 'min_child_samples': 13, 'subsample': 0.6468344462959484, 'colsample_bytree': 0.8058209837752345, 'lambda_l1': 0.3141605406665038, 'lambda_l2': 4.855120891430721}. Best is trial 8 with value: 0.03873808144906869.\n",
      "[I 2025-11-03 20:46:07,947] Trial 13 finished with value: 0.03872562065709963 and parameters: {'learning_rate': 0.11019544041058582, 'num_leaves': 16, 'max_depth': 12, 'min_child_samples': 26, 'subsample': 0.6224717092446601, 'colsample_bytree': 0.8431835194234629, 'lambda_l1': 0.23573620835226805, 'lambda_l2': 5.0598510820472}. Best is trial 13 with value: 0.03872562065709963.\n",
      "[I 2025-11-03 20:46:43,800] Trial 14 finished with value: 0.04525310362288004 and parameters: {'learning_rate': 0.08378139961832015, 'num_leaves': 132, 'max_depth': 14, 'min_child_samples': 27, 'subsample': 0.5345063251135901, 'colsample_bytree': 0.8464692762730509, 'lambda_l1': 4.641099306777784, 'lambda_l2': 6.004010864962575}. Best is trial 13 with value: 0.03872562065709963.\n",
      "[I 2025-11-03 20:47:08,188] Trial 15 finished with value: 0.03587889194690827 and parameters: {'learning_rate': 0.18663697064881696, 'num_leaves': 207, 'max_depth': 8, 'min_child_samples': 28, 'subsample': 0.623915115776402, 'colsample_bytree': 0.629993744508443, 'lambda_l1': 0.10624110696739297, 'lambda_l2': 9.87857755355548}. Best is trial 15 with value: 0.03587889194690827.\n",
      "[I 2025-11-03 20:47:39,485] Trial 16 finished with value: 0.03818446300665162 and parameters: {'learning_rate': 0.2721852270209374, 'num_leaves': 214, 'max_depth': 11, 'min_child_samples': 49, 'subsample': 0.6345000950378225, 'colsample_bytree': 0.6244843478358252, 'lambda_l1': 0.2635984947773711, 'lambda_l2': 9.84468879180248}. Best is trial 15 with value: 0.03587889194690827.\n",
      "[I 2025-11-03 20:48:02,470] Trial 17 finished with value: 0.04632892084951559 and parameters: {'learning_rate': 0.29672726099918073, 'num_leaves': 218, 'max_depth': 8, 'min_child_samples': 50, 'subsample': 0.6463907238556031, 'colsample_bytree': 0.6291278897548863, 'lambda_l1': 2.7568777934129858, 'lambda_l2': 9.96381651740543}. Best is trial 15 with value: 0.03587889194690827.\n",
      "[I 2025-11-03 20:48:29,494] Trial 18 finished with value: 0.04414905243788397 and parameters: {'learning_rate': 0.20190464339838457, 'num_leaves': 211, 'max_depth': 10, 'min_child_samples': 50, 'subsample': 0.7678691343047302, 'colsample_bytree': 0.6218049378082752, 'lambda_l1': 4.062804883828537, 'lambda_l2': 9.97408970802261}. Best is trial 15 with value: 0.03587889194690827.\n",
      "[I 2025-11-03 20:48:45,735] Trial 19 finished with value: 0.04529680527108599 and parameters: {'learning_rate': 0.19788630468602733, 'num_leaves': 224, 'max_depth': 5, 'min_child_samples': 35, 'subsample': 0.5958290004144263, 'colsample_bytree': 0.5429432222159319, 'lambda_l1': 5.627815641627425, 'lambda_l2': 8.379795261956312}. Best is trial 15 with value: 0.03587889194690827.\n",
      "[I 2025-11-03 20:49:09,056] Trial 20 finished with value: 0.04950206704619459 and parameters: {'learning_rate': 0.1829005767181756, 'num_leaves': 132, 'max_depth': 10, 'min_child_samples': 45, 'subsample': 0.8392630610684972, 'colsample_bytree': 0.6461238937416885, 'lambda_l1': 9.985199218360147, 'lambda_l2': 9.27962735756091}. Best is trial 15 with value: 0.03587889194690827.\n",
      "[I 2025-11-03 20:49:39,377] Trial 21 finished with value: 0.04042661972679574 and parameters: {'learning_rate': 0.10365935975016957, 'num_leaves': 192, 'max_depth': 12, 'min_child_samples': 26, 'subsample': 0.6273158221737513, 'colsample_bytree': 0.9794340721349789, 'lambda_l1': 0.00676537596001775, 'lambda_l2': 0.658059588189742}. Best is trial 15 with value: 0.03587889194690827.\n",
      "[I 2025-11-03 20:50:13,619] Trial 22 finished with value: 0.03992114546751811 and parameters: {'learning_rate': 0.1503055354133726, 'num_leaves': 151, 'max_depth': 12, 'min_child_samples': 23, 'subsample': 0.6780450842429485, 'colsample_bytree': 0.8579547953232295, 'lambda_l1': 1.5505415382892498, 'lambda_l2': 6.1842839093493644}. Best is trial 15 with value: 0.03587889194690827.\n",
      "[I 2025-11-03 20:50:28,818] Trial 23 finished with value: 0.03710793861951079 and parameters: {'learning_rate': 0.24581221723174765, 'num_leaves': 22, 'max_depth': 14, 'min_child_samples': 32, 'subsample': 0.61059331958505, 'colsample_bytree': 0.6678012841303034, 'lambda_l1': 0.05483073036471031, 'lambda_l2': 1.9303860057914939}. Best is trial 15 with value: 0.03587889194690827.\n",
      "[I 2025-11-03 20:51:04,535] Trial 24 finished with value: 0.04203005899854632 and parameters: {'learning_rate': 0.23952903108125523, 'num_leaves': 235, 'max_depth': 15, 'min_child_samples': 33, 'subsample': 0.551319187814665, 'colsample_bytree': 0.6591237258614339, 'lambda_l1': 2.105720269001668, 'lambda_l2': 1.4121070081977596}. Best is trial 15 with value: 0.03587889194690827.\n",
      "Best params for split 2016 : {'learning_rate': 0.18663697064881696, 'num_leaves': 207, 'max_depth': 8, 'min_child_samples': 28, 'subsample': 0.623915115776402, 'colsample_bytree': 0.629993744508443, 'lambda_l1': 0.10624110696739297, 'lambda_l2': 9.87857755355548}\n",
      "\n",
      "Metrics for test year 2017:\n",
      "  RMSE: 0.0359\n",
      "  MAE: 0.0182\n",
      "  R2: 0.9994\n",
      "Exported predictions for test year 2017 -> predictions_2017.csv\n",
      "\n",
      "============================================================\n",
      "Temporal split: train <= 201712 | test: 201801..201812\n",
      "============================================================\n",
      "Train rows: 375,807 | Test rows: 125,625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 20:51:46,421] A new study created in memory with name: no-name-7e3f8a98-f830-46c2-9a24-87bb5417b3f3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943f0a8d2f474eea83a854a778e2f3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 20:52:16,292] Trial 0 finished with value: 0.035871560364749276 and parameters: {'learning_rate': 0.051613510444016346, 'num_leaves': 175, 'max_depth': 7, 'min_child_samples': 27, 'subsample': 0.9330008942226613, 'colsample_bytree': 0.5615334379450214, 'lambda_l1': 0.4442775923464537, 'lambda_l2': 8.251853395350517}. Best is trial 0 with value: 0.035871560364749276.\n",
      "[I 2025-11-03 20:52:46,319] Trial 1 finished with value: 0.042283267531362356 and parameters: {'learning_rate': 0.0574769811947897, 'num_leaves': 87, 'max_depth': 9, 'min_child_samples': 33, 'subsample': 0.6580856804754907, 'colsample_bytree': 0.5186245000438976, 'lambda_l1': 7.265993151688781, 'lambda_l2': 1.8643156786766724}. Best is trial 0 with value: 0.035871560364749276.\n",
      "[I 2025-11-03 20:53:07,362] Trial 2 finished with value: 0.03520499823965899 and parameters: {'learning_rate': 0.21833014487976757, 'num_leaves': 224, 'max_depth': 5, 'min_child_samples': 18, 'subsample': 0.7839191570949361, 'colsample_bytree': 0.6884590054464033, 'lambda_l1': 0.07392253322102249, 'lambda_l2': 0.40671570624012277}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:53:25,351] Trial 3 finished with value: 0.04196611363516578 and parameters: {'learning_rate': 0.12878028267463582, 'num_leaves': 65, 'max_depth': 4, 'min_child_samples': 47, 'subsample': 0.5996198100202903, 'colsample_bytree': 0.8815974919940414, 'lambda_l1': 6.488480520657072, 'lambda_l2': 4.844900940367619}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:54:06,033] Trial 4 finished with value: 0.043538118954995854 and parameters: {'learning_rate': 0.024661912881665755, 'num_leaves': 170, 'max_depth': 9, 'min_child_samples': 12, 'subsample': 0.5269170620227556, 'colsample_bytree': 0.8293228530115955, 'lambda_l1': 4.44933534931566, 'lambda_l2': 5.413212946015445}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:54:46,768] Trial 5 finished with value: 0.0416973023272467 and parameters: {'learning_rate': 0.06534551364614952, 'num_leaves': 101, 'max_depth': 12, 'min_child_samples': 44, 'subsample': 0.7805578786615099, 'colsample_bytree': 0.7421784527672672, 'lambda_l1': 6.005044743571379, 'lambda_l2': 5.054866184580314}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:55:10,717] Trial 6 finished with value: 0.041373387056122354 and parameters: {'learning_rate': 0.05233823019912856, 'num_leaves': 34, 'max_depth': 15, 'min_child_samples': 47, 'subsample': 0.802705850852262, 'colsample_bytree': 0.542140949082718, 'lambda_l1': 0.7400066019665075, 'lambda_l2': 6.123888633450582}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:55:50,891] Trial 7 finished with value: 0.04863176134793499 and parameters: {'learning_rate': 0.020164660486546585, 'num_leaves': 155, 'max_depth': 9, 'min_child_samples': 43, 'subsample': 0.7653482636306397, 'colsample_bytree': 0.6246119687546792, 'lambda_l1': 0.7694619579509032, 'lambda_l2': 5.588236318391968}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:56:25,210] Trial 8 finished with value: 0.04180687672089937 and parameters: {'learning_rate': 0.03592122298928411, 'num_leaves': 112, 'max_depth': 9, 'min_child_samples': 32, 'subsample': 0.5284923429175068, 'colsample_bytree': 0.5761104978942777, 'lambda_l1': 1.2577786923957512, 'lambda_l2': 0.3225709335721161}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:57:01,537] Trial 9 finished with value: 0.046896000882135296 and parameters: {'learning_rate': 0.032800005969953, 'num_leaves': 232, 'max_depth': 13, 'min_child_samples': 43, 'subsample': 0.8511803104804936, 'colsample_bytree': 0.5726344522073534, 'lambda_l1': 4.456807130801426, 'lambda_l2': 3.700957483498315}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:57:14,091] Trial 10 finished with value: 0.052105051599286696 and parameters: {'learning_rate': 0.2398045543369124, 'num_leaves': 245, 'max_depth': 3, 'min_child_samples': 5, 'subsample': 0.9351924704795457, 'colsample_bytree': 0.7023879055754664, 'lambda_l1': 9.653624384645525, 'lambda_l2': 0.026184894872152253}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:57:35,177] Trial 11 finished with value: 0.03986532948624216 and parameters: {'learning_rate': 0.11311187906424434, 'num_leaves': 198, 'max_depth': 5, 'min_child_samples': 20, 'subsample': 0.9457558408870888, 'colsample_bytree': 0.9960231751822942, 'lambda_l1': 2.640946837865488, 'lambda_l2': 8.237313507477612}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:58:03,260] Trial 12 finished with value: 0.03975379013420043 and parameters: {'learning_rate': 0.2869480039807418, 'num_leaves': 200, 'max_depth': 7, 'min_child_samples': 23, 'subsample': 0.9885195241952774, 'colsample_bytree': 0.6717114223951335, 'lambda_l1': 2.8134614502011583, 'lambda_l2': 9.929255244440846}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:58:39,427] Trial 13 finished with value: 0.103961687084806 and parameters: {'learning_rate': 0.01030289251497337, 'num_leaves': 192, 'max_depth': 7, 'min_child_samples': 17, 'subsample': 0.8778570706840733, 'colsample_bytree': 0.6400461318857857, 'lambda_l1': 0.11342963981948463, 'lambda_l2': 7.630789006306603}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:59:02,809] Trial 14 finished with value: 0.038819454999064563 and parameters: {'learning_rate': 0.1328375697768406, 'num_leaves': 141, 'max_depth': 6, 'min_child_samples': 27, 'subsample': 0.6693744149145713, 'colsample_bytree': 0.8028912842021768, 'lambda_l1': 2.464574396743612, 'lambda_l2': 2.67067447882373}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:59:30,338] Trial 15 finished with value: 0.037079749683769765 and parameters: {'learning_rate': 0.08472639238871825, 'num_leaves': 222, 'max_depth': 6, 'min_child_samples': 13, 'subsample': 0.7062964424843748, 'colsample_bytree': 0.6209561518107004, 'lambda_l1': 2.019906045717165, 'lambda_l2': 7.6356292992389525}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 20:59:47,561] Trial 16 finished with value: 0.04165389369415789 and parameters: {'learning_rate': 0.18796369341001332, 'num_leaves': 177, 'max_depth': 4, 'min_child_samples': 34, 'subsample': 0.8593987849816618, 'colsample_bytree': 0.5049892523638526, 'lambda_l1': 3.2599151118516576, 'lambda_l2': 9.85167796066632}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 21:00:21,202] Trial 17 finished with value: 0.09123003737648314 and parameters: {'learning_rate': 0.0113923970265782, 'num_leaves': 216, 'max_depth': 7, 'min_child_samples': 26, 'subsample': 0.9189634893166071, 'colsample_bytree': 0.7087284446904867, 'lambda_l1': 0.12079998637933002, 'lambda_l2': 1.994583460064228}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 21:00:55,936] Trial 18 finished with value: 0.043680220051188386 and parameters: {'learning_rate': 0.08770887358114607, 'num_leaves': 253, 'max_depth': 11, 'min_child_samples': 6, 'subsample': 0.7296870460011285, 'colsample_bytree': 0.7632454819460371, 'lambda_l1': 8.565500237346118, 'lambda_l2': 6.65669866642539}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 21:01:09,522] Trial 19 finished with value: 0.04527202615596242 and parameters: {'learning_rate': 0.17251797779116407, 'num_leaves': 122, 'max_depth': 3, 'min_child_samples': 36, 'subsample': 0.9954605192934194, 'colsample_bytree': 0.9335237867015247, 'lambda_l1': 4.067671027839851, 'lambda_l2': 3.429991823148894}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 21:01:44,775] Trial 20 finished with value: 0.03993332895292652 and parameters: {'learning_rate': 0.03948116586792747, 'num_leaves': 152, 'max_depth': 11, 'min_child_samples': 17, 'subsample': 0.8249448485223857, 'colsample_bytree': 0.585224882410027, 'lambda_l1': 1.487596902983455, 'lambda_l2': 8.68852627029188}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 21:02:10,713] Trial 21 finished with value: 0.03717866461092767 and parameters: {'learning_rate': 0.09878393424593476, 'num_leaves': 215, 'max_depth': 6, 'min_child_samples': 13, 'subsample': 0.7061035495628246, 'colsample_bytree': 0.6270276373575047, 'lambda_l1': 2.0356234185363435, 'lambda_l2': 7.638389108004672}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 21:02:33,010] Trial 22 finished with value: 0.03898211979684881 and parameters: {'learning_rate': 0.07142177199254408, 'num_leaves': 229, 'max_depth': 5, 'min_child_samples': 11, 'subsample': 0.617746434464914, 'colsample_bytree': 0.6645172979328448, 'lambda_l1': 1.524720153807959, 'lambda_l2': 6.7894875731684765}. Best is trial 2 with value: 0.03520499823965899.\n",
      "[I 2025-11-03 21:03:05,949] Trial 23 finished with value: 0.03250295931239628 and parameters: {'learning_rate': 0.08248031414209436, 'num_leaves': 179, 'max_depth': 8, 'min_child_samples': 22, 'subsample': 0.707063753775236, 'colsample_bytree': 0.6082481577690402, 'lambda_l1': 0.3964241441943502, 'lambda_l2': 9.01725999682326}. Best is trial 23 with value: 0.03250295931239628.\n",
      "[I 2025-11-03 21:03:34,876] Trial 24 finished with value: 0.03287475226986765 and parameters: {'learning_rate': 0.1726804187517622, 'num_leaves': 178, 'max_depth': 8, 'min_child_samples': 25, 'subsample': 0.899839371809489, 'colsample_bytree': 0.5543005207996052, 'lambda_l1': 0.1330483646776187, 'lambda_l2': 9.119679951736282}. Best is trial 23 with value: 0.03250295931239628.\n",
      "Best params for split 2017 : {'learning_rate': 0.08248031414209436, 'num_leaves': 179, 'max_depth': 8, 'min_child_samples': 22, 'subsample': 0.707063753775236, 'colsample_bytree': 0.6082481577690402, 'lambda_l1': 0.3964241441943502, 'lambda_l2': 9.01725999682326}\n",
      "\n",
      "Metrics for test year 2018:\n",
      "  RMSE: 0.0325\n",
      "  MAE: 0.0154\n",
      "  R2: 0.9995\n",
      "Exported predictions for test year 2018 -> predictions_2018.csv\n",
      "\n",
      "============================================================\n",
      "Temporal split: train <= 201812 | test: 201901..201910\n",
      "============================================================\n",
      "Train rows: 501,432 | Test rows: 104,736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 21:04:29,068] A new study created in memory with name: no-name-81b8d319-7e10-4ab1-b920-55f9c3628f8f\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edb0e078f9b46b5a03004292ff5da7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 21:05:14,751] Trial 0 finished with value: 0.04884180140054054 and parameters: {'learning_rate': 0.02027298569730179, 'num_leaves': 78, 'max_depth': 15, 'min_child_samples': 35, 'subsample': 0.603000387628423, 'colsample_bytree': 0.7286468483410968, 'lambda_l1': 5.746132448629176, 'lambda_l2': 5.379884474355032}. Best is trial 0 with value: 0.04884180140054054.\n",
      "[I 2025-11-03 21:06:05,062] Trial 1 finished with value: 0.03845331793181127 and parameters: {'learning_rate': 0.07138609306276979, 'num_leaves': 207, 'max_depth': 15, 'min_child_samples': 27, 'subsample': 0.8870753079016321, 'colsample_bytree': 0.7752813116245643, 'lambda_l1': 6.256504945196257, 'lambda_l2': 8.835961353006315}. Best is trial 1 with value: 0.03845331793181127.\n",
      "[I 2025-11-03 21:06:50,154] Trial 2 finished with value: 0.03358731105099362 and parameters: {'learning_rate': 0.07807766073052358, 'num_leaves': 238, 'max_depth': 15, 'min_child_samples': 24, 'subsample': 0.5547556341620397, 'colsample_bytree': 0.5726150348023649, 'lambda_l1': 1.8056236016773008, 'lambda_l2': 2.890474303344602}. Best is trial 2 with value: 0.03358731105099362.\n",
      "[I 2025-11-03 21:07:28,610] Trial 3 finished with value: 0.04492456046085282 and parameters: {'learning_rate': 0.02706735125638705, 'num_leaves': 213, 'max_depth': 7, 'min_child_samples': 39, 'subsample': 0.9652571435378121, 'colsample_bytree': 0.7097674937411805, 'lambda_l1': 3.6279106605332503, 'lambda_l2': 3.118511564337163}. Best is trial 2 with value: 0.03358731105099362.\n",
      "[I 2025-11-03 21:08:01,303] Trial 4 finished with value: 0.05685787001818877 and parameters: {'learning_rate': 0.02792540582592622, 'num_leaves': 65, 'max_depth': 9, 'min_child_samples': 8, 'subsample': 0.7982296548248327, 'colsample_bytree': 0.5813343034652602, 'lambda_l1': 5.033062245150717, 'lambda_l2': 9.122889913869015}. Best is trial 2 with value: 0.03358731105099362.\n",
      "[I 2025-11-03 21:08:35,252] Trial 5 finished with value: 0.03340560939643125 and parameters: {'learning_rate': 0.1313532531859879, 'num_leaves': 75, 'max_depth': 13, 'min_child_samples': 15, 'subsample': 0.7704001793681251, 'colsample_bytree': 0.5943207750640616, 'lambda_l1': 2.0246998631546544, 'lambda_l2': 5.871686411076235}. Best is trial 5 with value: 0.03340560939643125.\n",
      "[I 2025-11-03 21:09:12,444] Trial 6 finished with value: 0.04015266148031862 and parameters: {'learning_rate': 0.09526320163378516, 'num_leaves': 64, 'max_depth': 12, 'min_child_samples': 9, 'subsample': 0.9832498006724523, 'colsample_bytree': 0.7706613569708302, 'lambda_l1': 6.92737578386697, 'lambda_l2': 3.853606221672017}. Best is trial 5 with value: 0.03340560939643125.\n",
      "[I 2025-11-03 21:09:39,492] Trial 7 finished with value: 0.07408158921419845 and parameters: {'learning_rate': 0.026967698913151494, 'num_leaves': 188, 'max_depth': 5, 'min_child_samples': 23, 'subsample': 0.8505653002865128, 'colsample_bytree': 0.6082903650471524, 'lambda_l1': 1.2326051325341136, 'lambda_l2': 6.6388242228098795}. Best is trial 5 with value: 0.03340560939643125.\n",
      "[I 2025-11-03 21:10:10,899] Trial 8 finished with value: 0.03200741706667386 and parameters: {'learning_rate': 0.05406291206542523, 'num_leaves': 68, 'max_depth': 13, 'min_child_samples': 7, 'subsample': 0.5949596345457282, 'colsample_bytree': 0.8600734031944522, 'lambda_l1': 0.313252707411783, 'lambda_l2': 5.420769818312312}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:10:49,994] Trial 9 finished with value: 0.04065820204987017 and parameters: {'learning_rate': 0.041526550435144215, 'num_leaves': 210, 'max_depth': 10, 'min_child_samples': 38, 'subsample': 0.8345017547604627, 'colsample_bytree': 0.6838068236304934, 'lambda_l1': 7.686702810382151, 'lambda_l2': 9.214895449412955}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:11:06,273] Trial 10 finished with value: 0.04568308587795758 and parameters: {'learning_rate': 0.28084811684755756, 'num_leaves': 19, 'max_depth': 3, 'min_child_samples': 50, 'subsample': 0.6633250804732702, 'colsample_bytree': 0.9363143888493792, 'lambda_l1': 9.410178333726194, 'lambda_l2': 0.6864054917491558}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:11:37,040] Trial 11 finished with value: 0.03240891380058664 and parameters: {'learning_rate': 0.16625234329458385, 'num_leaves': 124, 'max_depth': 12, 'min_child_samples': 16, 'subsample': 0.7047295567831885, 'colsample_bytree': 0.9003958976253186, 'lambda_l1': 0.3739082914458336, 'lambda_l2': 6.860191320359687}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:12:22,502] Trial 12 finished with value: 0.04798750297046849 and parameters: {'learning_rate': 0.01223508945775223, 'num_leaves': 133, 'max_depth': 12, 'min_child_samples': 16, 'subsample': 0.66815491929785, 'colsample_bytree': 0.9124496223539145, 'lambda_l1': 0.25554572876707987, 'lambda_l2': 7.045672080249634}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:13:02,113] Trial 13 finished with value: 0.03687000566400437 and parameters: {'learning_rate': 0.19041035970803225, 'num_leaves': 130, 'max_depth': 10, 'min_child_samples': 5, 'subsample': 0.6942915743636445, 'colsample_bytree': 0.8589484255157313, 'lambda_l1': 3.3455268426941593, 'lambda_l2': 7.395636273023691}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:13:31,615] Trial 14 finished with value: 0.03209456351382907 and parameters: {'learning_rate': 0.054242359838578315, 'num_leaves': 106, 'max_depth': 13, 'min_child_samples': 16, 'subsample': 0.5123569653331673, 'colsample_bytree': 0.9923671632087563, 'lambda_l1': 0.0463857212028671, 'lambda_l2': 4.419617905553989}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:13:54,843] Trial 15 finished with value: 0.046040422610321395 and parameters: {'learning_rate': 0.047890576738542474, 'num_leaves': 15, 'max_depth': 13, 'min_child_samples': 12, 'subsample': 0.5030490992714804, 'colsample_bytree': 0.835487879459483, 'lambda_l1': 2.925533188665476, 'lambda_l2': 4.336791873113592}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:14:24,691] Trial 16 finished with value: 0.03277669600135339 and parameters: {'learning_rate': 0.05202905092996795, 'num_leaves': 101, 'max_depth': 8, 'min_child_samples': 20, 'subsample': 0.601457440107224, 'colsample_bytree': 0.9920039918828266, 'lambda_l1': 0.2376025668457391, 'lambda_l2': 1.9376960626082615}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:14:49,147] Trial 17 finished with value: 0.03578648233115458 and parameters: {'learning_rate': 0.10641938141282148, 'num_leaves': 46, 'max_depth': 11, 'min_child_samples': 6, 'subsample': 0.5042004084243762, 'colsample_bytree': 0.9716789488782309, 'lambda_l1': 2.4533438205691525, 'lambda_l2': 4.795407266066152}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:15:36,612] Trial 18 finished with value: 0.03799340949734713 and parameters: {'learning_rate': 0.037743365739578476, 'num_leaves': 166, 'max_depth': 14, 'min_child_samples': 30, 'subsample': 0.5892634237849198, 'colsample_bytree': 0.8410140120255261, 'lambda_l1': 4.351073422971963, 'lambda_l2': 1.6868221251247535}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:16:15,834] Trial 19 finished with value: 0.05140214152041259 and parameters: {'learning_rate': 0.013551046379238457, 'num_leaves': 103, 'max_depth': 7, 'min_child_samples': 12, 'subsample': 0.5401988876904595, 'colsample_bytree': 0.9406709330563656, 'lambda_l1': 1.0573217476476475, 'lambda_l2': 5.710922781334721}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:16:50,281] Trial 20 finished with value: 0.04152076198786266 and parameters: {'learning_rate': 0.06559155444903964, 'num_leaves': 108, 'max_depth': 13, 'min_child_samples': 21, 'subsample': 0.6536969370869609, 'colsample_bytree': 0.5189240433840052, 'lambda_l1': 9.502866177339858, 'lambda_l2': 3.493598676796113}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:17:27,281] Trial 21 finished with value: 0.03330100305235274 and parameters: {'learning_rate': 0.15304916753423428, 'num_leaves': 160, 'max_depth': 11, 'min_child_samples': 17, 'subsample': 0.6999134299269756, 'colsample_bytree': 0.883994639456499, 'lambda_l1': 0.203333896618604, 'lambda_l2': 8.179815901299635}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:18:02,471] Trial 22 finished with value: 0.034271795883283836 and parameters: {'learning_rate': 0.2783193528275272, 'num_leaves': 121, 'max_depth': 12, 'min_child_samples': 12, 'subsample': 0.7182039974644779, 'colsample_bytree': 0.8098053731411966, 'lambda_l1': 1.1123084553366018, 'lambda_l2': 6.340627532403696}. Best is trial 8 with value: 0.03200741706667386.\n",
      "[I 2025-11-03 21:18:29,571] Trial 23 finished with value: 0.03200531871637704 and parameters: {'learning_rate': 0.1948360417986518, 'num_leaves': 153, 'max_depth': 14, 'min_child_samples': 18, 'subsample': 0.6243201948985085, 'colsample_bytree': 0.9021053558399351, 'lambda_l1': 0.02511228497486806, 'lambda_l2': 9.984313207740188}. Best is trial 23 with value: 0.03200531871637704.\n",
      "[I 2025-11-03 21:19:09,138] Trial 24 finished with value: 0.03214208664604036 and parameters: {'learning_rate': 0.10015679905129961, 'num_leaves': 151, 'max_depth': 14, 'min_child_samples': 10, 'subsample': 0.5567775816047366, 'colsample_bytree': 0.9953182938974803, 'lambda_l1': 1.439804826898321, 'lambda_l2': 9.724411819523587}. Best is trial 23 with value: 0.03200531871637704.\n",
      "Best params for split 2018 : {'learning_rate': 0.1948360417986518, 'num_leaves': 153, 'max_depth': 14, 'min_child_samples': 18, 'subsample': 0.6243201948985085, 'colsample_bytree': 0.9021053558399351, 'lambda_l1': 0.02511228497486806, 'lambda_l2': 9.984313207740188}\n",
      "\n",
      "Metrics for test year 2019:\n",
      "  RMSE: 0.0320\n",
      "  MAE: 0.0142\n",
      "  R2: 0.9995\n",
      "Exported predictions for test year 2019 -> predictions_2019.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 9. Loop over temporal splits\n",
    "# ----------------------\n",
    "\n",
    "for year_split in split_years:\n",
    "    train_ym_max = year_split * 100 + 12\n",
    "    test_ym_min = (year_split + 1) * 100 + 1\n",
    "    test_ym_max = min((year_split + 1) * 100 + 12, last_ym)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Temporal split: train <= {train_ym_max} | test: {test_ym_min}..{test_ym_max}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    train_df = agg.filter(col(\"ym\") <= train_ym_max).cache()\n",
    "    test_df = agg.filter((col(\"ym\") >= test_ym_min) & (col(\"ym\") <= test_ym_max)).cache()\n",
    "\n",
    "    train_count = train_df.count()\n",
    "    test_count = test_df.count()\n",
    "    print(f\"Train rows: {train_count:,} | Test rows: {test_count:,}\")\n",
    "\n",
    "    if train_count < 50 or test_count < 10:\n",
    "        print(\"Not enough data for this split, skipping.\")\n",
    "        train_df.unpersist(); test_df.unpersist()\n",
    "        continue\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10. Merchant target encoding\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    merchant_mean = train_df.groupBy(\"merchant_id\").agg(_avg(\"log_amount_sum\").alias(\"merchant_mean_log\"))\n",
    "    global_mean_log = train_df.agg(_avg(\"log_amount_sum\").alias(\"g\")).collect()[0][0]\n",
    "\n",
    "    train_enc = train_df.join(merchant_mean, on=\"merchant_id\", how=\"left\") \\\n",
    "        .withColumn(\"merchant_mean_log\",\n",
    "                    when(col(\"merchant_mean_log\").isNull(), lit(global_mean_log))\n",
    "                    .otherwise(col(\"merchant_mean_log\")))\n",
    "\n",
    "    test_enc = test_df.join(merchant_mean, on=\"merchant_id\", how=\"left\") \\\n",
    "        .withColumn(\"merchant_mean_log\",\n",
    "                    when(col(\"merchant_mean_log\").isNull(), lit(global_mean_log))\n",
    "                    .otherwise(col(\"merchant_mean_log\")))\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 11. Features and labels\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    feature_cols = [\n",
    "        \"tx_count\", \"amount_avg\", \"amount_stddev\", \"refund_rate\", \"desc_count\",\n",
    "        \"month_sin\", \"month_cos\", \"year_norm\", \"merchant_mean_log\"\n",
    "    ]\n",
    "    label_col = \"log_amount_sum\"\n",
    "\n",
    "    # Convert to pandas\n",
    "    train_pd = train_enc.select(*feature_cols, label_col).toPandas().fillna(0.0)\n",
    "    test_pd = test_enc.select(*feature_cols, label_col).toPandas().fillna(0.0)\n",
    "\n",
    "    X_train, y_train = train_pd[feature_cols].values, train_pd[label_col].values\n",
    "    X_test, y_test = test_pd[feature_cols].values, test_pd[label_col].values\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 12. Run OPTUNA Optimization\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"regression\",\n",
    "            \"metric\": \"rmse\",\n",
    "            \"verbosity\": -1,\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 255),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10.0),\n",
    "            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 10.0),\n",
    "        }\n",
    "\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_val = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            lgb_train,\n",
    "            valid_sets=[lgb_val],\n",
    "            num_boost_round=500,\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        rmse = np.sqrt(np.mean((y_test - preds) ** 2))\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=25, show_progress_bar=True)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(\"Best params for split\", year_split, \":\", best_params)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 13. Train final model with best params\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    final_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        **best_params\n",
    "    }\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_val = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "    final_model = lgb.train(\n",
    "        final_params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_val],\n",
    "        num_boost_round=500,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "    )\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = final_model.predict(X_test, num_iteration=final_model.best_iteration)\n",
    "    y_true = y_test\n",
    "\n",
    "    # Metrics - Tout avec numpy\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "    # R² calculation\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "    print(f\"\\nMetrics for test year {year_split + 1}:\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R2: {r2:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"year_train\": year_split,\n",
    "        \"year_test\": year_split + 1,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"r2\": r2,\n",
    "        \"best_params\": str(best_params)\n",
    "    })\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 14. Prediction export for the tested year\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    test_pd_full = test_enc.select(\"merchant_id\", \"year\", \"month\", \"ym\", *feature_cols, label_col).toPandas().fillna(0.0)\n",
    "\n",
    "    predictions_df = pd.DataFrame({\n",
    "        \"year_train\": [year_split] * len(y_true),\n",
    "        \"year_test\": [year_split + 1] * len(y_true),\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "\n",
    "    # add features\n",
    "    predictions_df = pd.concat([test_pd_full.reset_index(drop=True), predictions_df], axis=1)\n",
    "\n",
    "    output_file = f\"predictions_{year_split + 1}.csv\"\n",
    "    predictions_df.to_csv(output_file, index=False)\n",
    "    print(f\"Exported predictions for test year {year_split + 1} -> {output_file}\")\n",
    "\n",
    "    # Cleanup\n",
    "    train_df.unpersist()\n",
    "    test_df.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dggt3sCgUcxq",
   "metadata": {
    "id": "dggt3sCgUcxq"
   },
   "source": [
    "### **Comments :**\n",
    "\n",
    "The R² is really high. After the previous tests previously done, I see 2 possible significations:\n",
    "\n",
    ">>> Option 1: There is again a data leakage due to the merchant_mean_log\n",
    "\n",
    ">>> Option 2: I saw previously that the trend during the stagnation period is really stable and it could explain the algorithm got an extremely performing result\n",
    "\n",
    "To be sure, I will run the script above without the merchant_mean_log to ensure there is no data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2FPBvicqLr3v",
   "metadata": {
    "id": "2FPBvicqLr3v"
   },
   "source": [
    "## <span style=\"color:blue;\">6. **Experiment 5** Monthly aggregation **with lag** - MACHINE LEARNING USING LIGHTGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "siJrsEqQLzHq",
   "metadata": {
    "id": "siJrsEqQLzHq"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Initialization</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "TQEGpZLYL3OU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "TQEGpZLYL3OU",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "78e4909c-4488-46a1-bc50-054134f6bc86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  ca-certificates-java fonts-dejavu-core fonts-dejavu-extra java-common\n",
      "  libatk-wrapper-java libatk-wrapper-java-jni libpcsclite1 libxt-dev libxtst6\n",
      "  libxxf86dga1 openjdk-11-jdk-headless openjdk-11-jre openjdk-11-jre-headless\n",
      "  x11-utils\n",
      "Suggested packages:\n",
      "  default-jre pcscd libxt-doc openjdk-11-demo openjdk-11-source visualvm\n",
      "  libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
      "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
      "The following NEW packages will be installed:\n",
      "  ca-certificates-java fonts-dejavu-core fonts-dejavu-extra java-common\n",
      "  libatk-wrapper-java libatk-wrapper-java-jni libpcsclite1 libxt-dev libxtst6\n",
      "  libxxf86dga1 openjdk-11-jdk openjdk-11-jdk-headless openjdk-11-jre\n",
      "  openjdk-11-jre-headless x11-utils\n",
      "0 upgraded, 15 newly installed, 0 to remove and 41 not upgraded.\n",
      "Need to get 122 MB of archives.\n",
      "After this operation, 274 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 java-common all 0.72build2 [6,782 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpcsclite1 amd64 1.9.5-3ubuntu1 [19.8 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [42.6 MB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ca-certificates-java all 20190909ubuntu1.2 [12.1 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1 [214 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [73.6 MB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.28+6-1ubuntu1~22.04.1 [1,342 kB]\n",
      "Fetched 122 MB in 3s (41.0 MB/s)\n",
      "Selecting previously unselected package java-common.\n",
      "(Reading database ... 125082 files and directories currently installed.)\n",
      "Preparing to unpack .../00-java-common_0.72build2_all.deb ...\n",
      "Unpacking java-common (0.72build2) ...\n",
      "Selecting previously unselected package libpcsclite1:amd64.\n",
      "Preparing to unpack .../01-libpcsclite1_1.9.5-3ubuntu1_amd64.deb ...\n",
      "Unpacking libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
      "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
      "Preparing to unpack .../02-openjdk-11-jre-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Selecting previously unselected package ca-certificates-java.\n",
      "Preparing to unpack .../03-ca-certificates-java_20190909ubuntu1.2_all.deb ...\n",
      "Unpacking ca-certificates-java (20190909ubuntu1.2) ...\n",
      "Selecting previously unselected package fonts-dejavu-core.\n",
      "Preparing to unpack .../04-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
      "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
      "Selecting previously unselected package fonts-dejavu-extra.\n",
      "Preparing to unpack .../05-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
      "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
      "Selecting previously unselected package libxtst6:amd64.\n",
      "Preparing to unpack .../06-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
      "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Selecting previously unselected package libxxf86dga1:amd64.\n",
      "Preparing to unpack .../07-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
      "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
      "Selecting previously unselected package x11-utils.\n",
      "Preparing to unpack .../08-x11-utils_7.7+5build2_amd64.deb ...\n",
      "Unpacking x11-utils (7.7+5build2) ...\n",
      "Selecting previously unselected package libatk-wrapper-java.\n",
      "Preparing to unpack .../09-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
      "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
      "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
      "Preparing to unpack .../10-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
      "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
      "Selecting previously unselected package libxt-dev:amd64.\n",
      "Preparing to unpack .../11-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
      "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
      "Selecting previously unselected package openjdk-11-jre:amd64.\n",
      "Preparing to unpack .../12-openjdk-11-jre_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Selecting previously unselected package openjdk-11-jdk-headless:amd64.\n",
      "Preparing to unpack .../13-openjdk-11-jdk-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
      "Preparing to unpack .../14-openjdk-11-jdk_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Setting up java-common (0.72build2) ...\n",
      "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
      "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
      "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
      "Setting up libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
      "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
      "Setting up x11-utils (7.7+5build2) ...\n",
      "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
      "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
      "Setting up openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
      "Setting up openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Setting up openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
      "Setting up openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
      "Setting up ca-certificates-java (20190909ubuntu1.2) ...\n",
      "head: cannot open '/etc/ssl/certs/java/cacerts' for reading: No such file or directory\n",
      "Adding debian:SecureSign_RootCA11.pem\n",
      "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
      "Adding debian:AffirmTrust_Commercial.pem\n",
      "Adding debian:DigiCert_Global_Root_G3.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
      "Adding debian:certSIGN_Root_CA_G2.pem\n",
      "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
      "Adding debian:QuoVadis_Root_CA_3.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R6.pem\n",
      "Adding debian:D-TRUST_EV_Root_CA_1_2020.pem\n",
      "Adding debian:HARICA_TLS_RSA_Root_CA_2021.pem\n",
      "Adding debian:Trustwave_Global_Certification_Authority.pem\n",
      "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
      "Adding debian:Comodo_AAA_Services_root.pem\n",
      "Adding debian:Certum_EC-384_CA.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
      "Adding debian:GlobalSign_Root_CA.pem\n",
      "Adding debian:HiPKI_Root_CA_-_G1.pem\n",
      "Adding debian:emSign_Root_CA_-_G1.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
      "Adding debian:CFCA_EV_ROOT.pem\n",
      "Adding debian:COMODO_Certification_Authority.pem\n",
      "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
      "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
      "Adding debian:Izenpe.com.pem\n",
      "Adding debian:HARICA_TLS_ECC_Root_CA_2021.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
      "Adding debian:Security_Communication_RootCA2.pem\n",
      "Adding debian:UCA_Global_G2_Root.pem\n",
      "Adding debian:D-TRUST_BR_Root_CA_1_2020.pem\n",
      "Adding debian:Secure_Global_CA.pem\n",
      "Adding debian:GTS_Root_R3.pem\n",
      "Adding debian:ISRG_Root_X1.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
      "Adding debian:GlobalSign_Root_E46.pem\n",
      "Adding debian:vTrus_Root_CA.pem\n",
      "Adding debian:TWCA_Root_Certification_Authority.pem\n",
      "Adding debian:AffirmTrust_Premium.pem\n",
      "Adding debian:XRamp_Global_CA_Root.pem\n",
      "Adding debian:Starfield_Class_2_CA.pem\n",
      "Adding debian:Buypass_Class_2_Root_CA.pem\n",
      "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:Amazon_Root_CA_2.pem\n",
      "Adding debian:GLOBALTRUST_2020.pem\n",
      "Adding debian:Microsoft_ECC_Root_Certificate_Authority_2017.pem\n",
      "Adding debian:certSIGN_ROOT_CA.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
      "Adding debian:ePKI_Root_Certification_Authority.pem\n",
      "Adding debian:Certum_Trusted_Root_CA.pem\n",
      "Adding debian:Security_Communication_ECC_RootCA1.pem\n",
      "Adding debian:Amazon_Root_CA_1.pem\n",
      "Adding debian:ACCVRAIZ1.pem\n",
      "Adding debian:QuoVadis_Root_CA_2.pem\n",
      "Adding debian:TWCA_Global_Root_CA.pem\n",
      "Adding debian:Amazon_Root_CA_3.pem\n",
      "Adding debian:emSign_Root_CA_-_C1.pem\n",
      "Adding debian:DigiCert_Global_Root_CA.pem\n",
      "Adding debian:Security_Communication_RootCA3.pem\n",
      "Adding debian:UCA_Extended_Validation_Root.pem\n",
      "Adding debian:GTS_Root_R1.pem\n",
      "Adding debian:Baltimore_CyberTrust_Root.pem\n",
      "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
      "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
      "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
      "Adding debian:NAVER_Global_Root_Certification_Authority.pem\n",
      "Adding debian:GTS_Root_R4.pem\n",
      "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:Buypass_Class_3_Root_CA.pem\n",
      "Adding debian:e-Szigno_Root_CA_2017.pem\n",
      "Adding debian:Telia_Root_CA_v2.pem\n",
      "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
      "Adding debian:Certainly_Root_E1.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
      "Adding debian:DigiCert_TLS_ECC_P384_Root_G5.pem\n",
      "Adding debian:AffirmTrust_Networking.pem\n",
      "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
      "Adding debian:GlobalSign_Root_R46.pem\n",
      "Adding debian:Trustwave_Global_ECC_P384_Certification_Authority.pem\n",
      "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
      "Adding debian:Go_Daddy_Class_2_CA.pem\n",
      "Adding debian:Certigna_Root_CA.pem\n",
      "Adding debian:vTrus_ECC_Root_CA.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
      "Adding debian:NetLock_Arany_=Class_Gold=_Főtanúsítvány.pem\n",
      "Adding debian:Microsoft_RSA_Root_Certificate_Authority_2017.pem\n",
      "Adding debian:SZAFIR_ROOT_CA2.pem\n",
      "Adding debian:Certum_Trusted_Network_CA.pem\n",
      "Adding debian:CA_Disig_Root_R2.pem\n",
      "Adding debian:Trustwave_Global_ECC_P256_Certification_Authority.pem\n",
      "Adding debian:Hongkong_Post_Root_CA_3.pem\n",
      "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G4.pem\n",
      "Adding debian:GTS_Root_R2.pem\n",
      "Adding debian:ISRG_Root_X2.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_C3.pem\n",
      "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
      "Adding debian:Actalis_Authentication_Root_CA.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
      "Adding debian:ANF_Secure_Server_Root_CA.pem\n",
      "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
      "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
      "Adding debian:DigiCert_Global_Root_G2.pem\n",
      "Adding debian:Security_Communication_Root_CA.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM_SERVIDORES_SEGUROS.pem\n",
      "Adding debian:DigiCert_TLS_RSA4096_Root_G5.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
      "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
      "Adding debian:SecureTrust_CA.pem\n",
      "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
      "Adding debian:Certainly_Root_R1.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
      "Adding debian:TunTrust_Root_CA.pem\n",
      "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
      "Adding debian:Certigna.pem\n",
      "Adding debian:Amazon_Root_CA_4.pem\n",
      "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
      "Adding debian:AffirmTrust_Premium_ECC.pem\n",
      "Adding debian:Atos_TrustedRoot_2011.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
      "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_G3.pem\n",
      "Adding debian:Sectigo_Public_Server_Authentication_Root_R46.pem\n",
      "Adding debian:Atos_TrustedRoot_Root_CA_ECC_TLS_2021.pem\n",
      "Adding debian:Atos_TrustedRoot_Root_CA_RSA_TLS_2021.pem\n",
      "Adding debian:BJCA_Global_Root_CA2.pem\n",
      "Adding debian:BJCA_Global_Root_CA1.pem\n",
      "Adding debian:CommScope_Public_Trust_ECC_Root-01.pem\n",
      "Adding debian:Sectigo_Public_Server_Authentication_Root_E46.pem\n",
      "Adding debian:SSL.com_TLS_ECC_Root_CA_2022.pem\n",
      "Adding debian:SSL.com_TLS_RSA_Root_CA_2022.pem\n",
      "Adding debian:TrustAsia_Global_Root_CA_G4.pem\n",
      "Adding debian:CommScope_Public_Trust_RSA_Root-01.pem\n",
      "Adding debian:CommScope_Public_Trust_RSA_Root-02.pem\n",
      "Adding debian:TrustAsia_Global_Root_CA_G3.pem\n",
      "Adding debian:CommScope_Public_Trust_ECC_Root-02.pem\n",
      "done.\n",
      "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
      "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for ca-certificates (20240203~22.04.1) ...\n",
      "Updating certificates in /etc/ssl/certs...\n",
      "0 added, 0 removed; done.\n",
      "Running hooks in /etc/ca-certificates/update.d...\n",
      "\n",
      "done.\n",
      "done.\n",
      "Collecting pyspark==3.5.0\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.0) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425346 sha256=3be73352458232b43344df33606546aec59855ebb2a9eeabadc9b304450682c8\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/40/20/65eefe766118e0a8f8e385cc3ed6e9eb7241c7e51cfc04c51a\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.1\n",
      "    Uninstalling pyspark-3.5.1:\n",
      "      Successfully uninstalled pyspark-3.5.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyspark-3.5.0\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas==2.2.2\n",
      "  Downloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas==2.2.2)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas==2.2.2)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.2)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.2.2)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.2\n",
      "    Uninstalling pytz-2025.2:\n",
      "      Successfully uninstalled pytz-2025.2\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.2\n",
      "    Uninstalling tzdata-2025.2:\n",
      "      Successfully uninstalled tzdata-2025.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4 pandas-2.2.2 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "aad049a86b1e43189c80867e2b448f59",
       "pip_warning": {
        "packages": [
         "dateutil",
         "numpy",
         "six"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 1. Install JAVA - SPARK\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "!apt-get install openjdk-11-jdk -y\n",
    "!pip install pyspark==3.5.0\n",
    "!pip install findspark\n",
    "!pip install --force-reinstall numpy==1.26.4 pandas==2.2.2\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] += \":/usr/lib/jvm/java-11-openjdk-amd64/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5s2AYINrLhPk",
   "metadata": {
    "id": "5s2AYINrLhPk"
   },
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 2. Spark init\n",
    "# ----------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML_Portfolio_Monthly_Aggregated_Prediction_LAG\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ii-fEv25L-qx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ii-fEv25L-qx",
    "outputId": "59325dab-738c-4d46-8b62-ac90e2a7b4ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.1)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.10.1 optuna-4.5.0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 3. Install packages\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "USEOZhleL-oH",
   "metadata": {
    "id": "USEOZhleL-oH"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Import Libraries\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from math import pi\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_date, regexp_replace, unix_timestamp, dayofweek, month, year, sin, cos, lit,\n",
    "    sum as _sum, avg as _avg, count as _count, stddev as _stddev, countDistinct,\n",
    "    when, expr, max as _max, lag\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iWdhjaMxMKEj",
   "metadata": {
    "id": "iWdhjaMxMKEj"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Extraction - Load</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "IqP-ALIaL-lp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqP-ALIaL-lp",
    "outputId": "1100a94b-ce81-4535-bf9a-d08fd6c0a133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/computingvictor/transactions-fraud-datasets?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348M/348M [00:08<00:00, 41.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 5. Data Loading\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "path = kagglehub.dataset_download(\"computingvictor/transactions-fraud-datasets\")\n",
    "transaction_data = f\"{path}/transactions_data.csv\"\n",
    "mcc_codes_data = f\"{path}/mcc_codes.json\"\n",
    "\n",
    "# load mcc descriptions\n",
    "with open(mcc_codes_data, \"r\") as f:\n",
    "    mcc_dict = json.load(f)\n",
    "mcc = pd.DataFrame.from_dict(mcc_dict, orient=\"index\", columns=[\"description\"]) \\\n",
    "    .reset_index().rename(columns={\"index\": \"mcc_code\"})\n",
    "mcc_spark = spark.createDataFrame(mcc)\n",
    "\n",
    "# load transactions\n",
    "td_spark = spark.read.csv(transaction_data, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56Iip3xBMTXE",
   "metadata": {
    "id": "56Iip3xBMTXE"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Data Wrangling</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gUG6S0TKL-i_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUG6S0TKL-i_",
    "outputId": "1a88c1e7-c43a-479e-e52b-158e0d4fa064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after filter: 6734248\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 6. Feature Engineering\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "LABEL_COL = \"amount\"\n",
    "\n",
    "td_spark = td_spark.withColumn(LABEL_COL, regexp_replace(col(LABEL_COL), \"\\\\$\", \"\"))\n",
    "td_spark = td_spark.withColumn(LABEL_COL, col(LABEL_COL).cast(DoubleType()))\n",
    "td_spark = td_spark.withColumn(\"date\", to_date(col(\"date\")))\n",
    "\n",
    "df = td_spark.join(mcc_spark, td_spark.mcc == mcc_spark.mcc_code, \"left\")\n",
    "\n",
    "cols_to_drop = [\"id\", \"use_chip\", \"merchant_city\", \"merchant_state\", \"zip\", \"mcc\", \"errors\", \"mcc_code\"]\n",
    "for c in cols_to_drop:\n",
    "    if c in df.columns:\n",
    "        df = df.drop(c)\n",
    "\n",
    "df = df.withColumn(\"year\", year(col(\"date\")).cast(IntegerType()))\n",
    "df = df.withColumn(\"month\", month(col(\"date\")).cast(IntegerType()))\n",
    "df = df.withColumn(\"date_timestamp\", unix_timestamp(col(\"date\")).cast(DoubleType()))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"date\")).cast(IntegerType()))\n",
    "df = df.withColumn(\"is_refund\", when(col(LABEL_COL) < 0, 1.0).otherwise(0.0).cast(DoubleType()))\n",
    "df = df.withColumn(\"description_clean\", regexp_replace(col(\"description\"), \"[ ,:\\\\[\\\\]\\\\{\\\\}]\", \"_\"))\n",
    "df = df.filter((col(\"date\") >= \"2015-01-01\") & (col(\"date\") <= \"2019-10-31\")).cache()\n",
    "\n",
    "print(\"Rows after filter:\", df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "kQ_6FgNUL-gq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQ_6FgNUL-gq",
    "outputId": "70747e2d-bf85-4f39-db74-97478a4974fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly aggregated rows (before lag): 606168\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 7. Monthly aggregation per merchant_id\n",
    "# ----------------------\n",
    "\n",
    "# Aggregation per (year, month, merchant_id)\n",
    "group_cols = [\"merchant_id\", \"year\", \"month\"]\n",
    "\n",
    "agg = df.groupBy(*group_cols).agg(\n",
    "    _sum(LABEL_COL).alias(\"amount_sum\"),\n",
    "    _count(\"*\").alias(\"tx_count\"),\n",
    "    _avg(LABEL_COL).alias(\"amount_avg\"),\n",
    "    _stddev(LABEL_COL).alias(\"amount_stddev\"),\n",
    "    _avg(\"is_refund\").alias(\"refund_rate\"),\n",
    "    countDistinct(\"description_clean\").alias(\"desc_count\")\n",
    ")\n",
    "\n",
    "# fill null stddev with 0\n",
    "agg = agg.fillna({\"amount_stddev\": 0.0})\n",
    "\n",
    "# add ym numeric for splitting and features\n",
    "agg = agg.withColumn(\"ym\", col(\"year\") * 100 + col(\"month\"))\n",
    "\n",
    "# temporal features\n",
    "# month cyclic\n",
    "agg = agg.withColumn(\"month_sin\", sin(2 * pi * col(\"month\") / lit(12)))\n",
    "agg = agg.withColumn(\"month_cos\", cos(2 * pi * col(\"month\") / lit(12)))\n",
    "\n",
    "# normalized year (year - min_year)\n",
    "min_year = agg.agg({\"year\": \"min\"}).collect()[0][0]\n",
    "agg = agg.withColumn(\"year_norm\", (col(\"year\") - lit(min_year)).cast(DoubleType()))\n",
    "\n",
    "# log target\n",
    "agg = agg.withColumn(\"log_amount_sum\", when(col(\"amount_sum\") >= 0, col(\"amount_sum\")).otherwise(0.0))\n",
    "agg = agg.withColumn(\"log_amount_sum\", expr(\"log(1 + log_amount_sum)\"))\n",
    "\n",
    "print(\"Monthly aggregated rows (before lag):\", agg.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1EfXi0PKL-cz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EfXi0PKL-cz",
    "outputId": "190c371e-5e50-4e9a-e970-cb2fb4b92fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating LAG features...\n",
      "Monthly aggregated rows (after lag): 606168\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 8. CREATE LAG FEATURES (historique)\n",
    "# ----------------------\n",
    "\n",
    "print(\"\\nCreating LAG features...\")\n",
    "\n",
    "# Window sorted by period\n",
    "window_spec = Window.partitionBy(\"merchant_id\").orderBy(\"ym\")\n",
    "\n",
    "# LAG 1 month (M-1)\n",
    "agg = agg.withColumn(\"tx_count_lag1\", lag(\"tx_count\", 1).over(window_spec))\n",
    "agg = agg.withColumn(\"amount_avg_lag1\", lag(\"amount_avg\", 1).over(window_spec))\n",
    "agg = agg.withColumn(\"amount_stddev_lag1\", lag(\"amount_stddev\", 1).over(window_spec))\n",
    "agg = agg.withColumn(\"refund_rate_lag1\", lag(\"refund_rate\", 1).over(window_spec))\n",
    "agg = agg.withColumn(\"desc_count_lag1\", lag(\"desc_count\", 1).over(window_spec))\n",
    "agg = agg.withColumn(\"log_amount_sum_lag1\", lag(\"log_amount_sum\", 1).over(window_spec))\n",
    "\n",
    "# LAG 2 Months\n",
    "agg = agg.withColumn(\"tx_count_lag2\", lag(\"tx_count\", 2).over(window_spec))\n",
    "agg = agg.withColumn(\"amount_avg_lag2\", lag(\"amount_avg\", 2).over(window_spec))\n",
    "agg = agg.withColumn(\"log_amount_sum_lag2\", lag(\"log_amount_sum\", 2).over(window_spec))\n",
    "\n",
    "# LAG 3 Months\n",
    "agg = agg.withColumn(\"tx_count_lag3\", lag(\"tx_count\", 3).over(window_spec))\n",
    "agg = agg.withColumn(\"amount_avg_lag3\", lag(\"amount_avg\", 3).over(window_spec))\n",
    "agg = agg.withColumn(\"log_amount_sum_lag3\", lag(\"log_amount_sum\", 3).over(window_spec))\n",
    "\n",
    "# Rolling average on 3 months\n",
    "window_rolling = Window.partitionBy(\"merchant_id\").orderBy(\"ym\").rowsBetween(-3, -1)\n",
    "agg = agg.withColumn(\"amount_avg_roll3\", _avg(\"amount_avg\").over(window_rolling))\n",
    "agg = agg.withColumn(\"tx_count_roll3\", _avg(\"tx_count\").over(window_rolling))\n",
    "\n",
    "# Trend (gap between actual perio and previous month)\n",
    "agg = agg.withColumn(\"amount_trend\",\n",
    "    when(col(\"amount_avg_lag1\").isNotNull(),\n",
    "         col(\"amount_avg_lag1\") - lag(\"amount_avg\", 2).over(window_spec))\n",
    "    .otherwise(0.0))\n",
    "\n",
    "# Fill Null values\n",
    "lag_cols = [c for c in agg.columns if \"lag\" in c or \"roll\" in c or \"trend\" in c]\n",
    "agg = agg.fillna(0.0, subset=lag_cols)\n",
    "\n",
    "# cache\n",
    "agg = agg.cache()\n",
    "print(\"Monthly aggregated rows (after lag):\", agg.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "i3kD1lH4L-aR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3kD1lH4L-aR",
    "outputId": "873b9163-3386-46f2-d9ed-8a3c86d63730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last ym available: 201910\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 9. prepare splitting boundaries\n",
    "# ----------------------\n",
    "last_ym = agg.agg(_max(col(\"ym\")).alias(\"last\")).collect()[0][0]\n",
    "print(\"Last ym available:\", last_ym)\n",
    "\n",
    "split_years = [2015, 2016, 2017, 2018]\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mo3IOKtvMqJL",
   "metadata": {
    "id": "mo3IOKtvMqJL"
   },
   "source": [
    "### <b><span style=\"color:green;\">✓</span> <u>Machine Learning</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ZJyBMjrCL-Xm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c619d3d945ee495d80a991dc6ed2c866",
      "84b1b7e901d94f17a8ad4e0b6ef1fab2",
      "ea6f4c0ff5344aab9325b1e3b5280587",
      "b0369ce3cb2342958c3eeef369a604f8",
      "701b6a56894f44a18a55fcbe3e9e68cd",
      "f1d7b72add1f414b8c14c5a1e380a66e",
      "cc09357f650142958662cda219f71131",
      "f22bb1771adc4bbf805d8bd640037155",
      "1c530074691847deb6191384262b2b35",
      "1e71e216bbaf42eebff56bd1fadd27bd",
      "c57c53077ecd4f6b8a91c92ddfe1d532",
      "adff6b5c007448dfa9e5ec0625757573",
      "cb475eb89874448f991f16a98522f4b4",
      "0734714a697244c9800711c6128a3b00",
      "dcdcdaf01fec4c2f9e33d03ac7dc7204",
      "5d2be6196a3a4747ad375f7c02b62384",
      "96313e447e0f40a8a1dae1651282cf07",
      "f5a322c2e8094fd28f9003e3dd97672e",
      "edccb06367774056855f98ba6272761d",
      "a022fec224fa41b690fb7588bda69345",
      "c9bf39c0e2a64ec6a4b93c3de173f3db",
      "e02f6767c9f342919ddec23ed570761a",
      "4137597d565f4cf0bc95b1b1ecceff2d",
      "163a73ab056e4c8a958f25a0ef9b3f89",
      "abc71b69b2c24aefa519c23171180b31",
      "e11a1a7ec9d944af96a5527cae01e1eb",
      "80bbc387aaa74ea8ba45428e38c3b022",
      "0e6df1de06fa4311956acf6b2f549d61",
      "dd16865c960542a092ccc4fc24cc6acb",
      "2d7b32646b7843928f0418fbcac5ac30",
      "294b2926f5fd49b2ade511abc1c333e2",
      "1f8a688d70454f2ab8db16136cb94ea6",
      "aff9c3bdd75c441785b51b535f162d93",
      "a28d98d6188c44e798b80afbc0e227c3",
      "8ce943d326ee48aaac6328158eacd14b",
      "40e6113a1bbc4f91a024978020973ad2",
      "ecab9d6da71e4bbdb27e6ef746aec746",
      "2b0191833b474a6f95a3748e0ab0c6fd",
      "469caa3672ec4222a4487eb9029be793",
      "d7a1c23ae31c496b908f7d12fa0e144b",
      "3a5453fd4a0042dcb6d1734765c87b19",
      "042789d039cb4a40a21bbcb490014059",
      "197c3f86bec84115bbd052a161405f73",
      "6f1a833ee15940b7a1c398620fad2ad7"
     ]
    },
    "id": "ZJyBMjrCL-Xm",
    "outputId": "aa42e9d5-db56-41c1-a368-39c4cadf3554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Temporal split: train <= 201512 | test: 201601..201612\n",
      "============================================================\n",
      "Train rows: 124,922 | Test rows: 125,226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 12:31:09,521] A new study created in memory with name: no-name-7c1cbf23-a670-4ab0-b94e-3c9b444391da\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shapes: X_train=(124922, 18), X_test=(125226, 18)\n",
      "\n",
      "Optimizing hyperparameters with Optuna...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c619d3d945ee495d80a991dc6ed2c866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 12:31:36,157] Trial 0 finished with value: 0.883103505152531 and parameters: {'learning_rate': 0.012493604758207322, 'num_leaves': 62, 'max_depth': 9, 'min_child_samples': 45, 'subsample': 0.7051582576421489, 'colsample_bytree': 0.8581123636333403, 'lambda_l1': 8.012984387001866, 'lambda_l2': 7.2828974401096715}. Best is trial 0 with value: 0.883103505152531.\n",
      "[I 2025-11-09 12:31:51,843] Trial 1 finished with value: 0.8855786738196934 and parameters: {'learning_rate': 0.016759213615137845, 'num_leaves': 17, 'max_depth': 14, 'min_child_samples': 24, 'subsample': 0.5510978824627288, 'colsample_bytree': 0.898637163985659, 'lambda_l1': 1.3773841924081898, 'lambda_l2': 0.888887462539617}. Best is trial 0 with value: 0.883103505152531.\n",
      "[I 2025-11-09 12:32:01,159] Trial 2 finished with value: 0.8840135743914553 and parameters: {'learning_rate': 0.05586895318746213, 'num_leaves': 202, 'max_depth': 10, 'min_child_samples': 28, 'subsample': 0.9038509249153674, 'colsample_bytree': 0.9885539746230301, 'lambda_l1': 0.808144875832052, 'lambda_l2': 5.263168374790984}. Best is trial 0 with value: 0.883103505152531.\n",
      "[I 2025-11-09 12:32:12,305] Trial 3 finished with value: 0.8823064358536216 and parameters: {'learning_rate': 0.034999570964921234, 'num_leaves': 253, 'max_depth': 6, 'min_child_samples': 19, 'subsample': 0.7764880543916304, 'colsample_bytree': 0.6782354674935551, 'lambda_l1': 0.43111385709757477, 'lambda_l2': 8.806333951540266}. Best is trial 3 with value: 0.8823064358536216.\n",
      "[I 2025-11-09 12:32:17,992] Trial 4 finished with value: 0.8836605780498584 and parameters: {'learning_rate': 0.06703475189813336, 'num_leaves': 191, 'max_depth': 14, 'min_child_samples': 7, 'subsample': 0.6230541020251816, 'colsample_bytree': 0.5021169415945653, 'lambda_l1': 7.486923965440688, 'lambda_l2': 2.424142048575475}. Best is trial 3 with value: 0.8823064358536216.\n",
      "[I 2025-11-09 12:32:41,136] Trial 5 finished with value: 0.8824265799251503 and parameters: {'learning_rate': 0.013305768925923483, 'num_leaves': 188, 'max_depth': 7, 'min_child_samples': 29, 'subsample': 0.6159665303896971, 'colsample_bytree': 0.8499038419259017, 'lambda_l1': 2.243738043760264, 'lambda_l2': 9.833239096578916}. Best is trial 3 with value: 0.8823064358536216.\n",
      "[I 2025-11-09 12:32:43,633] Trial 6 finished with value: 0.8843576447882289 and parameters: {'learning_rate': 0.2062790703230513, 'num_leaves': 246, 'max_depth': 8, 'min_child_samples': 35, 'subsample': 0.5363773829539744, 'colsample_bytree': 0.6363115647112487, 'lambda_l1': 8.20754076487062, 'lambda_l2': 9.413926252966787}. Best is trial 3 with value: 0.8823064358536216.\n",
      "[I 2025-11-09 12:32:51,819] Trial 7 finished with value: 0.8827993938062201 and parameters: {'learning_rate': 0.04030924896468369, 'num_leaves': 135, 'max_depth': 9, 'min_child_samples': 21, 'subsample': 0.7765227817632692, 'colsample_bytree': 0.6324394394591539, 'lambda_l1': 7.175292717381141, 'lambda_l2': 4.986027100661607}. Best is trial 3 with value: 0.8823064358536216.\n",
      "[I 2025-11-09 12:33:01,427] Trial 8 finished with value: 0.8900086240366322 and parameters: {'learning_rate': 0.01747728417756112, 'num_leaves': 233, 'max_depth': 3, 'min_child_samples': 47, 'subsample': 0.6497846513701819, 'colsample_bytree': 0.5349953291613325, 'lambda_l1': 5.697904141447489, 'lambda_l2': 7.671786234901998}. Best is trial 3 with value: 0.8823064358536216.\n",
      "[I 2025-11-09 12:33:10,791] Trial 9 finished with value: 0.8837346999791759 and parameters: {'learning_rate': 0.06095155379443979, 'num_leaves': 234, 'max_depth': 15, 'min_child_samples': 44, 'subsample': 0.9983104591624545, 'colsample_bytree': 0.7601051564736452, 'lambda_l1': 2.869997975009139, 'lambda_l2': 5.562087049963046}. Best is trial 3 with value: 0.8823064358536216.\n",
      "[I 2025-11-09 12:33:15,104] Trial 10 finished with value: 0.8851418459239121 and parameters: {'learning_rate': 0.17781436948395601, 'num_leaves': 135, 'max_depth': 4, 'min_child_samples': 9, 'subsample': 0.7955442108982804, 'colsample_bytree': 0.6961480617151531, 'lambda_l1': 4.0254020427214545, 'lambda_l2': 8.099486634704888}. Best is trial 3 with value: 0.8823064358536216.\n",
      "[I 2025-11-09 12:33:35,115] Trial 11 finished with value: 0.8825054425350408 and parameters: {'learning_rate': 0.031115964031960576, 'num_leaves': 176, 'max_depth': 6, 'min_child_samples': 17, 'subsample': 0.8561780817900603, 'colsample_bytree': 0.7947318273661286, 'lambda_l1': 0.3159930187579092, 'lambda_l2': 9.747104682318128}. Best is trial 3 with value: 0.8823064358536216.\n",
      "[I 2025-11-09 12:33:49,121] Trial 12 finished with value: 0.8828327026615224 and parameters: {'learning_rate': 0.02631550381528708, 'num_leaves': 161, 'max_depth': 6, 'min_child_samples': 31, 'subsample': 0.698677790477529, 'colsample_bytree': 0.856237635449018, 'lambda_l1': 2.535279755457707, 'lambda_l2': 9.876354949481206}. Best is trial 3 with value: 0.8823064358536216.\n",
      "[I 2025-11-09 12:34:11,115] Trial 13 finished with value: 0.8829290796424843 and parameters: {'learning_rate': 0.010911477425237603, 'num_leaves': 88, 'max_depth': 6, 'min_child_samples': 37, 'subsample': 0.6239221231362175, 'colsample_bytree': 0.6990649928127369, 'lambda_l1': 2.658597200445926, 'lambda_l2': 8.128124036051325}. Best is trial 3 with value: 0.8823064358536216.\n",
      "[I 2025-11-09 12:34:16,073] Trial 14 finished with value: 0.884891925865662 and parameters: {'learning_rate': 0.08763811849323067, 'num_leaves': 254, 'max_depth': 11, 'min_child_samples': 16, 'subsample': 0.5016674767402527, 'colsample_bytree': 0.9619321169275639, 'lambda_l1': 4.85032040603831, 'lambda_l2': 6.360144473527361}. Best is trial 3 with value: 0.8823064358536216.\n",
      "[I 2025-11-09 12:34:31,634] Trial 15 finished with value: 0.8820747120077495 and parameters: {'learning_rate': 0.02059354056991074, 'num_leaves': 209, 'max_depth': 7, 'min_child_samples': 15, 'subsample': 0.8302712225846074, 'colsample_bytree': 0.8070170874743354, 'lambda_l1': 1.6270953429453883, 'lambda_l2': 3.482982008310448}. Best is trial 15 with value: 0.8820747120077495.\n",
      "[I 2025-11-09 12:34:45,991] Trial 16 finished with value: 0.8829161778245583 and parameters: {'learning_rate': 0.02495225183600587, 'num_leaves': 217, 'max_depth': 5, 'min_child_samples': 13, 'subsample': 0.8718562689187452, 'colsample_bytree': 0.5873454862926042, 'lambda_l1': 0.21463927690682483, 'lambda_l2': 3.302449122128155}. Best is trial 15 with value: 0.8820747120077495.\n",
      "[I 2025-11-09 12:34:51,793] Trial 17 finished with value: 0.8846176825204237 and parameters: {'learning_rate': 0.117447372476304, 'num_leaves': 154, 'max_depth': 12, 'min_child_samples': 21, 'subsample': 0.9317504549822707, 'colsample_bytree': 0.7035652148487167, 'lambda_l1': 3.8381004911741696, 'lambda_l2': 3.5923748730312575}. Best is trial 15 with value: 0.8820747120077495.\n",
      "[I 2025-11-09 12:35:00,898] Trial 18 finished with value: 0.8862639030443477 and parameters: {'learning_rate': 0.037475775791710644, 'num_leaves': 219, 'max_depth': 3, 'min_child_samples': 5, 'subsample': 0.8047029332236797, 'colsample_bytree': 0.8076935902908435, 'lambda_l1': 1.4011267511589358, 'lambda_l2': 0.22563941746660632}. Best is trial 15 with value: 0.8820747120077495.\n",
      "[I 2025-11-09 12:35:21,492] Trial 19 finished with value: 0.8824414739571127 and parameters: {'learning_rate': 0.018161356831789606, 'num_leaves': 109, 'max_depth': 8, 'min_child_samples': 12, 'subsample': 0.8239946133224865, 'colsample_bytree': 0.7420358035052353, 'lambda_l1': 6.116369386825777, 'lambda_l2': 2.0862799442814133}. Best is trial 15 with value: 0.8820747120077495.\n",
      "[I 2025-11-09 12:35:34,515] Trial 20 finished with value: 0.8835165001993615 and parameters: {'learning_rate': 0.04069254827822839, 'num_leaves': 211, 'max_depth': 5, 'min_child_samples': 23, 'subsample': 0.7308810623642089, 'colsample_bytree': 0.9245529829114524, 'lambda_l1': 1.5123709525684406, 'lambda_l2': 4.162450495189166}. Best is trial 15 with value: 0.8820747120077495.\n",
      "[I 2025-11-09 12:35:52,370] Trial 21 finished with value: 0.8822060041988785 and parameters: {'learning_rate': 0.0198268978756913, 'num_leaves': 189, 'max_depth': 7, 'min_child_samples': 17, 'subsample': 0.6649295332504638, 'colsample_bytree': 0.8209485281331365, 'lambda_l1': 2.2532999989156766, 'lambda_l2': 8.67780244339146}. Best is trial 15 with value: 0.8820747120077495.\n",
      "[I 2025-11-09 12:36:11,810] Trial 22 finished with value: 0.8819623207407866 and parameters: {'learning_rate': 0.020564562345879417, 'num_leaves': 169, 'max_depth': 7, 'min_child_samples': 17, 'subsample': 0.7493279614928161, 'colsample_bytree': 0.7892422756941176, 'lambda_l1': 3.6881764942535176, 'lambda_l2': 8.82179457409932}. Best is trial 22 with value: 0.8819623207407866.\n",
      "[I 2025-11-09 12:36:28,325] Trial 23 finished with value: 0.8822291503158608 and parameters: {'learning_rate': 0.02129246649441396, 'num_leaves': 163, 'max_depth': 8, 'min_child_samples': 14, 'subsample': 0.6907579907501035, 'colsample_bytree': 0.8018622251013989, 'lambda_l1': 3.892734127781013, 'lambda_l2': 6.753463084333006}. Best is trial 22 with value: 0.8819623207407866.\n",
      "[I 2025-11-09 12:36:50,153] Trial 24 finished with value: 0.8825630022704862 and parameters: {'learning_rate': 0.014888640228199353, 'num_leaves': 184, 'max_depth': 7, 'min_child_samples': 10, 'subsample': 0.7421334553542985, 'colsample_bytree': 0.7508023016725132, 'lambda_l1': 9.150903578179488, 'lambda_l2': 6.261059350273942}. Best is trial 22 with value: 0.8819623207407866.\n",
      "\n",
      "Best params for split 2015 : {'learning_rate': 0.020564562345879417, 'num_leaves': 169, 'max_depth': 7, 'min_child_samples': 17, 'subsample': 0.7493279614928161, 'colsample_bytree': 0.7892422756941176, 'lambda_l1': 3.6881764942535176, 'lambda_l2': 8.82179457409932}\n",
      "\n",
      "Training final model...\n",
      "\n",
      "============================================================\n",
      "METRICS FOR TEST YEAR 2016\n",
      "============================================================\n",
      "  RMSE: 0.8820\n",
      "  MAE: 0.6460\n",
      "  R2: 0.6259\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "                feature    importance\n",
      "9   log_amount_sum_lag1  2.062219e+06\n",
      "10  log_amount_sum_lag2  5.757554e+05\n",
      "12     amount_avg_roll3  3.004553e+05\n",
      "11  log_amount_sum_lag3  1.752458e+05\n",
      "13       tx_count_roll3  1.297221e+05\n",
      "3       amount_avg_lag1  1.073665e+05\n",
      "15            month_sin  3.071832e+04\n",
      "16            month_cos  2.593237e+04\n",
      "4       amount_avg_lag2  2.237346e+04\n",
      "0         tx_count_lag1  2.105827e+04\n",
      "\n",
      "Baseline (predict with lag1 only):\n",
      "  RMSE: 1.5190\n",
      "  R2: -0.1096\n",
      "  Model improvement: 41.94%\n",
      "\n",
      "Exported predictions for test year 2016 -> predictions_lag_2016.csv\n",
      "\n",
      "============================================================\n",
      "Temporal split: train <= 201612 | test: 201701..201712\n",
      "============================================================\n",
      "Train rows: 250,148 | Test rows: 125,659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 12:37:37,390] A new study created in memory with name: no-name-6d3c49b9-e8a8-41fc-9530-f38c2c0515f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shapes: X_train=(250148, 18), X_test=(125659, 18)\n",
      "\n",
      "Optimizing hyperparameters with Optuna...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adff6b5c007448dfa9e5ec0625757573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 12:38:07,724] Trial 0 finished with value: 0.8567479053725812 and parameters: {'learning_rate': 0.021998263024010156, 'num_leaves': 113, 'max_depth': 15, 'min_child_samples': 13, 'subsample': 0.6469159032538111, 'colsample_bytree': 0.626935088246087, 'lambda_l1': 8.718036671656654, 'lambda_l2': 6.814010463442376}. Best is trial 0 with value: 0.8567479053725812.\n",
      "[I 2025-11-09 12:38:26,464] Trial 1 finished with value: 0.8595832977777444 and parameters: {'learning_rate': 0.05726237577094743, 'num_leaves': 221, 'max_depth': 3, 'min_child_samples': 33, 'subsample': 0.8727283371177742, 'colsample_bytree': 0.5730526060394625, 'lambda_l1': 9.964101709505895, 'lambda_l2': 3.2574291917188503}. Best is trial 0 with value: 0.8567479053725812.\n",
      "[I 2025-11-09 12:38:59,944] Trial 2 finished with value: 0.8570558186905657 and parameters: {'learning_rate': 0.012563781404229512, 'num_leaves': 147, 'max_depth': 7, 'min_child_samples': 18, 'subsample': 0.8599881062368192, 'colsample_bytree': 0.8408327587856865, 'lambda_l1': 5.106058387565795, 'lambda_l2': 1.1773953750420052}. Best is trial 0 with value: 0.8567479053725812.\n",
      "[I 2025-11-09 12:39:06,511] Trial 3 finished with value: 0.8580411289831043 and parameters: {'learning_rate': 0.21107164571861675, 'num_leaves': 29, 'max_depth': 7, 'min_child_samples': 42, 'subsample': 0.6692559889401285, 'colsample_bytree': 0.6252865377047669, 'lambda_l1': 7.34576790967329, 'lambda_l2': 8.892358809678282}. Best is trial 0 with value: 0.8567479053725812.\n",
      "[I 2025-11-09 12:39:21,787] Trial 4 finished with value: 0.8624915213614401 and parameters: {'learning_rate': 0.021884082756768328, 'num_leaves': 150, 'max_depth': 3, 'min_child_samples': 26, 'subsample': 0.6648082194728829, 'colsample_bytree': 0.9061947057019352, 'lambda_l1': 4.996268012266948, 'lambda_l2': 9.580831332407127}. Best is trial 0 with value: 0.8567479053725812.\n",
      "[I 2025-11-09 12:39:27,429] Trial 5 finished with value: 0.857445569670682 and parameters: {'learning_rate': 0.11882656842382021, 'num_leaves': 98, 'max_depth': 15, 'min_child_samples': 12, 'subsample': 0.814674901360735, 'colsample_bytree': 0.9574186398490505, 'lambda_l1': 5.189362534240893, 'lambda_l2': 8.149560706173032}. Best is trial 0 with value: 0.8567479053725812.\n",
      "[I 2025-11-09 12:39:55,370] Trial 6 finished with value: 0.856482029169574 and parameters: {'learning_rate': 0.023365235369617317, 'num_leaves': 75, 'max_depth': 8, 'min_child_samples': 24, 'subsample': 0.8617928843490104, 'colsample_bytree': 0.7428116559261764, 'lambda_l1': 5.338520485433467, 'lambda_l2': 0.6183635581075819}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:40:02,817] Trial 7 finished with value: 0.8579069204643401 and parameters: {'learning_rate': 0.18130628226356144, 'num_leaves': 150, 'max_depth': 8, 'min_child_samples': 29, 'subsample': 0.6559496299910547, 'colsample_bytree': 0.9637986296283879, 'lambda_l1': 9.137874392444868, 'lambda_l2': 8.852017559198645}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:40:09,903] Trial 8 finished with value: 0.8584434971657021 and parameters: {'learning_rate': 0.14875743554343557, 'num_leaves': 249, 'max_depth': 9, 'min_child_samples': 40, 'subsample': 0.7648024964427265, 'colsample_bytree': 0.5314818165043045, 'lambda_l1': 3.7384437756588818, 'lambda_l2': 3.207187433640052}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:40:27,185] Trial 9 finished with value: 0.8595778931496401 and parameters: {'learning_rate': 0.02087439105520685, 'num_leaves': 245, 'max_depth': 4, 'min_child_samples': 41, 'subsample': 0.9881023860644345, 'colsample_bytree': 0.5112434820616202, 'lambda_l1': 0.2420128370557939, 'lambda_l2': 3.8385993349485603}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:40:41,407] Trial 10 finished with value: 0.8578290848880904 and parameters: {'learning_rate': 0.0539173693701509, 'num_leaves': 20, 'max_depth': 12, 'min_child_samples': 6, 'subsample': 0.514784107733699, 'colsample_bytree': 0.7306409570452057, 'lambda_l1': 1.3794446366191417, 'lambda_l2': 0.7462395594902971}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:41:03,650] Trial 11 finished with value: 0.856660861520531 and parameters: {'learning_rate': 0.02968645987341237, 'num_leaves': 83, 'max_depth': 12, 'min_child_samples': 19, 'subsample': 0.5313331363683297, 'colsample_bytree': 0.6988438759518043, 'lambda_l1': 7.514128099176478, 'lambda_l2': 6.662725124583767}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:41:17,978] Trial 12 finished with value: 0.8566731919205869 and parameters: {'learning_rate': 0.052051595904188085, 'num_leaves': 69, 'max_depth': 12, 'min_child_samples': 21, 'subsample': 0.5503763709303624, 'colsample_bytree': 0.7445372971100913, 'lambda_l1': 7.4867537961480854, 'lambda_l2': 6.102535388209579}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:41:36,101] Trial 13 finished with value: 0.856642938264103 and parameters: {'learning_rate': 0.03758973874824937, 'num_leaves': 68, 'max_depth': 11, 'min_child_samples': 22, 'subsample': 0.953920081803493, 'colsample_bytree': 0.7939255533044203, 'lambda_l1': 6.470479762517972, 'lambda_l2': 5.201241545240631}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:42:15,246] Trial 14 finished with value: 0.8570638393988247 and parameters: {'learning_rate': 0.010465130084216017, 'num_leaves': 71, 'max_depth': 10, 'min_child_samples': 49, 'subsample': 0.9782003416614644, 'colsample_bytree': 0.804973672045969, 'lambda_l1': 3.1775511629340634, 'lambda_l2': 4.772551167442581}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:42:39,016] Trial 15 finished with value: 0.8574488151872734 and parameters: {'learning_rate': 0.037360198395436674, 'num_leaves': 55, 'max_depth': 5, 'min_child_samples': 25, 'subsample': 0.9444715491864133, 'colsample_bytree': 0.8088909762118669, 'lambda_l1': 6.249894358367054, 'lambda_l2': 1.7041857370133306}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:42:48,725] Trial 16 finished with value: 0.857544480537093 and parameters: {'learning_rate': 0.09012485367874826, 'num_leaves': 190, 'max_depth': 10, 'min_child_samples': 32, 'subsample': 0.9017985849691043, 'colsample_bytree': 0.8747980904964621, 'lambda_l1': 3.2112339493030064, 'lambda_l2': 2.423427607755832}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:43:14,267] Trial 17 finished with value: 0.8575212602197141 and parameters: {'learning_rate': 0.015606560039329831, 'num_leaves': 119, 'max_depth': 6, 'min_child_samples': 14, 'subsample': 0.8119474279846816, 'colsample_bytree': 0.6687149799437637, 'lambda_l1': 6.228709765985578, 'lambda_l2': 4.837609015882029}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:43:45,967] Trial 18 finished with value: 0.8567018602419397 and parameters: {'learning_rate': 0.03375235917912489, 'num_leaves': 45, 'max_depth': 13, 'min_child_samples': 23, 'subsample': 0.9220535366468179, 'colsample_bytree': 0.7905433451588403, 'lambda_l1': 6.217862805279481, 'lambda_l2': 0.17972875662092136}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:43:53,351] Trial 19 finished with value: 0.8570033326129033 and parameters: {'learning_rate': 0.09331341487778448, 'num_leaves': 97, 'max_depth': 10, 'min_child_samples': 34, 'subsample': 0.7341618065809926, 'colsample_bytree': 0.766995059509759, 'lambda_l1': 4.120595976854329, 'lambda_l2': 5.531158450656624}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:43:57,833] Trial 20 finished with value: 0.8592518580279389 and parameters: {'learning_rate': 0.27033258488716755, 'num_leaves': 178, 'max_depth': 8, 'min_child_samples': 7, 'subsample': 0.8450324230071453, 'colsample_bytree': 0.8619193733970772, 'lambda_l1': 1.85031448029509, 'lambda_l2': 7.472155750004099}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:44:18,028] Trial 21 finished with value: 0.8567806644494259 and parameters: {'learning_rate': 0.03328082540458922, 'num_leaves': 84, 'max_depth': 12, 'min_child_samples': 17, 'subsample': 0.5902607035625541, 'colsample_bytree': 0.7055022028130276, 'lambda_l1': 7.287034392187405, 'lambda_l2': 6.819633812038315}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:44:43,947] Trial 22 finished with value: 0.8565424686084657 and parameters: {'learning_rate': 0.02542993325859866, 'num_leaves': 48, 'max_depth': 13, 'min_child_samples': 20, 'subsample': 0.7417461788412241, 'colsample_bytree': 0.6957173720161546, 'lambda_l1': 8.224802132413664, 'lambda_l2': 4.285935593602151}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:45:12,405] Trial 23 finished with value: 0.8569384083220841 and parameters: {'learning_rate': 0.015845073169878623, 'num_leaves': 43, 'max_depth': 14, 'min_child_samples': 29, 'subsample': 0.7656286043095503, 'colsample_bytree': 0.6426110833006433, 'lambda_l1': 8.538977437829175, 'lambda_l2': 4.604111897717505}. Best is trial 6 with value: 0.856482029169574.\n",
      "[I 2025-11-09 12:45:28,241] Trial 24 finished with value: 0.8567020563189929 and parameters: {'learning_rate': 0.04576638108765584, 'num_leaves': 57, 'max_depth': 13, 'min_child_samples': 23, 'subsample': 0.7149214426465629, 'colsample_bytree': 0.7147557104357065, 'lambda_l1': 6.610705916872531, 'lambda_l2': 2.0571641299799}. Best is trial 6 with value: 0.856482029169574.\n",
      "\n",
      "Best params for split 2016 : {'learning_rate': 0.023365235369617317, 'num_leaves': 75, 'max_depth': 8, 'min_child_samples': 24, 'subsample': 0.8617928843490104, 'colsample_bytree': 0.7428116559261764, 'lambda_l1': 5.338520485433467, 'lambda_l2': 0.6183635581075819}\n",
      "\n",
      "Training final model...\n",
      "\n",
      "============================================================\n",
      "METRICS FOR TEST YEAR 2017\n",
      "============================================================\n",
      "  RMSE: 0.8565\n",
      "  MAE: 0.6272\n",
      "  R2: 0.6448\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "                feature    importance\n",
      "9   log_amount_sum_lag1  3.851032e+06\n",
      "10  log_amount_sum_lag2  1.165617e+06\n",
      "11  log_amount_sum_lag3  5.762460e+05\n",
      "12     amount_avg_roll3  4.793702e+05\n",
      "3       amount_avg_lag1  2.113161e+05\n",
      "13       tx_count_roll3  1.563719e+05\n",
      "4       amount_avg_lag2  5.536321e+04\n",
      "5       amount_avg_lag3  3.194367e+04\n",
      "0         tx_count_lag1  2.752643e+04\n",
      "15            month_sin  2.734036e+04\n",
      "\n",
      "Baseline (predict with lag1 only):\n",
      "  RMSE: 1.4111\n",
      "  R2: 0.0358\n",
      "  Model improvement: 39.30%\n",
      "\n",
      "Exported predictions for test year 2017 -> predictions_lag_2017.csv\n",
      "\n",
      "============================================================\n",
      "Temporal split: train <= 201712 | test: 201801..201812\n",
      "============================================================\n",
      "Train rows: 375,807 | Test rows: 125,625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 12:46:23,194] A new study created in memory with name: no-name-1de71712-254c-4961-b756-b23914574177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shapes: X_train=(375807, 18), X_test=(125625, 18)\n",
      "\n",
      "Optimizing hyperparameters with Optuna...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4137597d565f4cf0bc95b1b1ecceff2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 12:46:30,360] Trial 0 finished with value: 0.8571688653829835 and parameters: {'learning_rate': 0.26680685786756536, 'num_leaves': 54, 'max_depth': 6, 'min_child_samples': 38, 'subsample': 0.9500119036668087, 'colsample_bytree': 0.7650675574758488, 'lambda_l1': 9.27736660405297, 'lambda_l2': 1.6451108917283508}. Best is trial 0 with value: 0.8571688653829835.\n",
      "[I 2025-11-09 12:47:15,074] Trial 1 finished with value: 0.8555061260833402 and parameters: {'learning_rate': 0.023458023964121492, 'num_leaves': 153, 'max_depth': 9, 'min_child_samples': 47, 'subsample': 0.6080075579320012, 'colsample_bytree': 0.89353025163895, 'lambda_l1': 3.638056358290961, 'lambda_l2': 8.312532180637199}. Best is trial 1 with value: 0.8555061260833402.\n",
      "[I 2025-11-09 12:48:02,963] Trial 2 finished with value: 0.8554837808298644 and parameters: {'learning_rate': 0.01606461244554384, 'num_leaves': 165, 'max_depth': 8, 'min_child_samples': 46, 'subsample': 0.9773590506797365, 'colsample_bytree': 0.851903048755003, 'lambda_l1': 5.001466085548362, 'lambda_l2': 3.9274851624124585}. Best is trial 2 with value: 0.8554837808298644.\n",
      "[I 2025-11-09 12:48:08,916] Trial 3 finished with value: 0.857474491801595 and parameters: {'learning_rate': 0.2617261873567106, 'num_leaves': 84, 'max_depth': 6, 'min_child_samples': 30, 'subsample': 0.6703020103670014, 'colsample_bytree': 0.7781927375609121, 'lambda_l1': 3.2342299545118633, 'lambda_l2': 0.586525880678177}. Best is trial 2 with value: 0.8554837808298644.\n",
      "[I 2025-11-09 12:48:57,699] Trial 4 finished with value: 0.8561695420866017 and parameters: {'learning_rate': 0.013638698259158515, 'num_leaves': 57, 'max_depth': 13, 'min_child_samples': 13, 'subsample': 0.873812545647453, 'colsample_bytree': 0.9186640542193885, 'lambda_l1': 6.72178637999086, 'lambda_l2': 4.663790690799502}. Best is trial 2 with value: 0.8554837808298644.\n",
      "[I 2025-11-09 12:49:14,331] Trial 5 finished with value: 0.855902231734879 and parameters: {'learning_rate': 0.0556034988941439, 'num_leaves': 212, 'max_depth': 14, 'min_child_samples': 43, 'subsample': 0.5747367965394056, 'colsample_bytree': 0.6351575924859693, 'lambda_l1': 6.320625900977287, 'lambda_l2': 8.90820900422642}. Best is trial 2 with value: 0.8554837808298644.\n",
      "[I 2025-11-09 12:50:24,972] Trial 6 finished with value: 0.8558473956948959 and parameters: {'learning_rate': 0.01084152397834567, 'num_leaves': 195, 'max_depth': 13, 'min_child_samples': 18, 'subsample': 0.771439752458504, 'colsample_bytree': 0.8316591361828467, 'lambda_l1': 0.6732753329939567, 'lambda_l2': 0.12787128172144224}. Best is trial 2 with value: 0.8554837808298644.\n",
      "[I 2025-11-09 12:51:04,987] Trial 7 finished with value: 0.8551343557634599 and parameters: {'learning_rate': 0.030538499904489508, 'num_leaves': 134, 'max_depth': 7, 'min_child_samples': 31, 'subsample': 0.848623358635331, 'colsample_bytree': 0.5700891798350993, 'lambda_l1': 5.690729402894567, 'lambda_l2': 6.4131680406312706}. Best is trial 7 with value: 0.8551343557634599.\n",
      "[I 2025-11-09 12:51:13,173] Trial 8 finished with value: 0.8565420112140353 and parameters: {'learning_rate': 0.12880787366525007, 'num_leaves': 145, 'max_depth': 14, 'min_child_samples': 47, 'subsample': 0.9398878116763645, 'colsample_bytree': 0.6587216903495037, 'lambda_l1': 3.300436655544271, 'lambda_l2': 8.117505891990831}. Best is trial 7 with value: 0.8551343557634599.\n",
      "[I 2025-11-09 12:51:54,646] Trial 9 finished with value: 0.8560258616699665 and parameters: {'learning_rate': 0.02573016944484578, 'num_leaves': 77, 'max_depth': 6, 'min_child_samples': 50, 'subsample': 0.7119799076420217, 'colsample_bytree': 0.9780333772096508, 'lambda_l1': 1.554741691954108, 'lambda_l2': 7.757848928469356}. Best is trial 7 with value: 0.8551343557634599.\n",
      "[I 2025-11-09 12:52:17,270] Trial 10 finished with value: 0.8567944885542622 and parameters: {'learning_rate': 0.05016504487939639, 'num_leaves': 252, 'max_depth': 4, 'min_child_samples': 26, 'subsample': 0.8132643483973795, 'colsample_bytree': 0.5298830760543106, 'lambda_l1': 9.053320747617516, 'lambda_l2': 6.212235357326361}. Best is trial 7 with value: 0.8551343557634599.\n",
      "[I 2025-11-09 12:52:59,951] Trial 11 finished with value: 0.8551277033780188 and parameters: {'learning_rate': 0.024777939846443612, 'num_leaves': 118, 'max_depth': 9, 'min_child_samples': 34, 'subsample': 0.9796203406745195, 'colsample_bytree': 0.5381816397277706, 'lambda_l1': 6.234651753937436, 'lambda_l2': 3.6292547838957407}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:53:20,479] Trial 12 finished with value: 0.8555764310432894 and parameters: {'learning_rate': 0.04451050096257458, 'num_leaves': 116, 'max_depth': 11, 'min_child_samples': 30, 'subsample': 0.8858232884616751, 'colsample_bytree': 0.5365415059437153, 'lambda_l1': 7.044465781257222, 'lambda_l2': 3.118182661006912}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:53:57,216] Trial 13 finished with value: 0.8552731635440849 and parameters: {'learning_rate': 0.030187234788064794, 'num_leaves': 108, 'max_depth': 11, 'min_child_samples': 36, 'subsample': 0.8731622780063691, 'colsample_bytree': 0.6180924877582394, 'lambda_l1': 5.3364313871041285, 'lambda_l2': 6.209788489645366}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:54:14,652] Trial 14 finished with value: 0.8586193559993811 and parameters: {'learning_rate': 0.0671279516623366, 'num_leaves': 116, 'max_depth': 3, 'min_child_samples': 24, 'subsample': 0.8270034844881524, 'colsample_bytree': 0.5744461476728081, 'lambda_l1': 7.941835716240794, 'lambda_l2': 2.5470507213654274}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:54:41,682] Trial 15 finished with value: 0.8581393326365434 and parameters: {'learning_rate': 0.019734685863700516, 'num_leaves': 18, 'max_depth': 8, 'min_child_samples': 5, 'subsample': 0.9973604160113566, 'colsample_bytree': 0.6935501063419425, 'lambda_l1': 4.538171011025916, 'lambda_l2': 6.023883449317498}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:54:51,448] Trial 16 finished with value: 0.855847330555224 and parameters: {'learning_rate': 0.0935870831219907, 'num_leaves': 175, 'max_depth': 11, 'min_child_samples': 37, 'subsample': 0.5040330883271461, 'colsample_bytree': 0.5024649224484721, 'lambda_l1': 8.16267767526843, 'lambda_l2': 4.874463869981737}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:55:27,299] Trial 17 finished with value: 0.8552905784642045 and parameters: {'learning_rate': 0.032545408745627655, 'num_leaves': 130, 'max_depth': 7, 'min_child_samples': 22, 'subsample': 0.9065064505417681, 'colsample_bytree': 0.5901791715783047, 'lambda_l1': 5.852115127092218, 'lambda_l2': 9.92325262554947}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:55:55,966] Trial 18 finished with value: 0.8557048208265774 and parameters: {'learning_rate': 0.036238660468606146, 'num_leaves': 215, 'max_depth': 10, 'min_child_samples': 33, 'subsample': 0.8079709277702254, 'colsample_bytree': 0.7134664408286565, 'lambda_l1': 1.8797407959107835, 'lambda_l2': 6.735193314352699}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:56:20,487] Trial 19 finished with value: 0.8559444100421613 and parameters: {'learning_rate': 0.08267724365675713, 'num_leaves': 88, 'max_depth': 5, 'min_child_samples': 19, 'subsample': 0.7516172893036712, 'colsample_bytree': 0.5698799820528926, 'lambda_l1': 7.522718371588995, 'lambda_l2': 3.6609986222939908}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:56:34,036] Trial 20 finished with value: 0.8564182524110536 and parameters: {'learning_rate': 0.14191323612580445, 'num_leaves': 16, 'max_depth': 9, 'min_child_samples': 39, 'subsample': 0.9272771368578645, 'colsample_bytree': 0.6981444635719158, 'lambda_l1': 4.135383381566168, 'lambda_l2': 2.278209503231608}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:57:02,105] Trial 21 finished with value: 0.8553946435598129 and parameters: {'learning_rate': 0.03197086854815708, 'num_leaves': 109, 'max_depth': 11, 'min_child_samples': 34, 'subsample': 0.8539761431050115, 'colsample_bytree': 0.6219712949422875, 'lambda_l1': 5.5015458023152135, 'lambda_l2': 5.553013172304951}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:57:42,777] Trial 22 finished with value: 0.8553350605398025 and parameters: {'learning_rate': 0.019147089700743147, 'num_leaves': 100, 'max_depth': 10, 'min_child_samples': 41, 'subsample': 0.8566117771274989, 'colsample_bytree': 0.6007782672668587, 'lambda_l1': 5.274780965893676, 'lambda_l2': 7.008019012846214}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:58:19,783] Trial 23 finished with value: 0.8551946544112665 and parameters: {'learning_rate': 0.02732621812448638, 'num_leaves': 130, 'max_depth': 12, 'min_child_samples': 34, 'subsample': 0.9131628942623059, 'colsample_bytree': 0.5474085995719856, 'lambda_l1': 6.100265271107497, 'lambda_l2': 4.379059522650297}. Best is trial 11 with value: 0.8551277033780188.\n",
      "[I 2025-11-09 12:58:41,056] Trial 24 finished with value: 0.8554147036340464 and parameters: {'learning_rate': 0.04043087365996879, 'num_leaves': 136, 'max_depth': 15, 'min_child_samples': 31, 'subsample': 0.9654245651383777, 'colsample_bytree': 0.5041509362707517, 'lambda_l1': 6.8060094161800935, 'lambda_l2': 4.273947303539263}. Best is trial 11 with value: 0.8551277033780188.\n",
      "\n",
      "Best params for split 2017 : {'learning_rate': 0.024777939846443612, 'num_leaves': 118, 'max_depth': 9, 'min_child_samples': 34, 'subsample': 0.9796203406745195, 'colsample_bytree': 0.5381816397277706, 'lambda_l1': 6.234651753937436, 'lambda_l2': 3.6292547838957407}\n",
      "\n",
      "Training final model...\n",
      "\n",
      "============================================================\n",
      "METRICS FOR TEST YEAR 2018\n",
      "============================================================\n",
      "  RMSE: 0.8551\n",
      "  MAE: 0.6244\n",
      "  R2: 0.6474\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "                feature    importance\n",
      "9   log_amount_sum_lag1  4.082492e+06\n",
      "10  log_amount_sum_lag2  2.109765e+06\n",
      "11  log_amount_sum_lag3  1.531861e+06\n",
      "12     amount_avg_roll3  8.593455e+05\n",
      "13       tx_count_roll3  3.729240e+05\n",
      "3       amount_avg_lag1  2.874274e+05\n",
      "4       amount_avg_lag2  1.458101e+05\n",
      "5       amount_avg_lag3  8.540188e+04\n",
      "0         tx_count_lag1  8.026332e+04\n",
      "1         tx_count_lag2  6.006436e+04\n",
      "\n",
      "Baseline (predict with lag1 only):\n",
      "  RMSE: 1.3556\n",
      "  R2: 0.1139\n",
      "  Model improvement: 36.92%\n",
      "\n",
      "Exported predictions for test year 2018 -> predictions_lag_2018.csv\n",
      "\n",
      "============================================================\n",
      "Temporal split: train <= 201812 | test: 201901..201910\n",
      "============================================================\n",
      "Train rows: 501,432 | Test rows: 104,736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 12:59:55,013] A new study created in memory with name: no-name-7cbc80d2-10b4-492b-b720-7f547efc4bfe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shapes: X_train=(501432, 18), X_test=(104736, 18)\n",
      "\n",
      "Optimizing hyperparameters with Optuna...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28d98d6188c44e798b80afbc0e227c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:00:24,925] Trial 0 finished with value: 0.8511084490359238 and parameters: {'learning_rate': 0.05498479802875611, 'num_leaves': 22, 'max_depth': 7, 'min_child_samples': 17, 'subsample': 0.916654047471046, 'colsample_bytree': 0.8378721216702335, 'lambda_l1': 4.415258421994755, 'lambda_l2': 2.4994558415578974}. Best is trial 0 with value: 0.8511084490359238.\n",
      "[I 2025-11-09 13:01:14,796] Trial 1 finished with value: 0.8508716175532941 and parameters: {'learning_rate': 0.023491796148377837, 'num_leaves': 111, 'max_depth': 13, 'min_child_samples': 25, 'subsample': 0.9531842858564755, 'colsample_bytree': 0.823764484537924, 'lambda_l1': 5.331455422850385, 'lambda_l2': 7.851590071877008}. Best is trial 1 with value: 0.8508716175532941.\n",
      "[I 2025-11-09 13:02:01,545] Trial 2 finished with value: 0.8505272996800153 and parameters: {'learning_rate': 0.021906931680902703, 'num_leaves': 125, 'max_depth': 11, 'min_child_samples': 29, 'subsample': 0.6071730427992321, 'colsample_bytree': 0.5504964567284746, 'lambda_l1': 4.230851848178855, 'lambda_l2': 1.4708244932853698}. Best is trial 2 with value: 0.8505272996800153.\n",
      "[I 2025-11-09 13:02:09,286] Trial 3 finished with value: 0.8523530347807302 and parameters: {'learning_rate': 0.1791315271335515, 'num_leaves': 94, 'max_depth': 15, 'min_child_samples': 25, 'subsample': 0.804130949321525, 'colsample_bytree': 0.8974922325459946, 'lambda_l1': 4.1572643317548765, 'lambda_l2': 9.085190677417415}. Best is trial 2 with value: 0.8505272996800153.\n",
      "[I 2025-11-09 13:02:18,665] Trial 4 finished with value: 0.8527884262811445 and parameters: {'learning_rate': 0.18187521784503738, 'num_leaves': 159, 'max_depth': 13, 'min_child_samples': 13, 'subsample': 0.8193922897114758, 'colsample_bytree': 0.632132446288395, 'lambda_l1': 2.5703082515353026, 'lambda_l2': 9.718992094410359}. Best is trial 2 with value: 0.8505272996800153.\n",
      "[I 2025-11-09 13:02:29,788] Trial 5 finished with value: 0.8521792540129147 and parameters: {'learning_rate': 0.1859272605383289, 'num_leaves': 121, 'max_depth': 13, 'min_child_samples': 48, 'subsample': 0.8033921068640748, 'colsample_bytree': 0.9744628309638415, 'lambda_l1': 7.713692155763354, 'lambda_l2': 9.4142951856917}. Best is trial 2 with value: 0.8505272996800153.\n",
      "[I 2025-11-09 13:03:11,065] Trial 6 finished with value: 0.850961496483302 and parameters: {'learning_rate': 0.0290630070635408, 'num_leaves': 59, 'max_depth': 15, 'min_child_samples': 43, 'subsample': 0.9934625666139618, 'colsample_bytree': 0.8263806994038123, 'lambda_l1': 5.9443651191823, 'lambda_l2': 2.970457514950974}. Best is trial 2 with value: 0.8505272996800153.\n",
      "[I 2025-11-09 13:03:28,209] Trial 7 finished with value: 0.8515092923332439 and parameters: {'learning_rate': 0.07632534276198417, 'num_leaves': 102, 'max_depth': 14, 'min_child_samples': 39, 'subsample': 0.802490886177485, 'colsample_bytree': 0.7862251389473516, 'lambda_l1': 0.13220253119791914, 'lambda_l2': 6.787986923633115}. Best is trial 2 with value: 0.8505272996800153.\n",
      "[I 2025-11-09 13:03:44,775] Trial 8 finished with value: 0.8517832057809369 and parameters: {'learning_rate': 0.28494819420563394, 'num_leaves': 113, 'max_depth': 4, 'min_child_samples': 18, 'subsample': 0.9794879346559482, 'colsample_bytree': 0.6100676295938826, 'lambda_l1': 8.890105698472384, 'lambda_l2': 6.44711428040539}. Best is trial 2 with value: 0.8505272996800153.\n",
      "[I 2025-11-09 13:04:04,063] Trial 9 finished with value: 0.8508162345177318 and parameters: {'learning_rate': 0.07506046492031622, 'num_leaves': 87, 'max_depth': 10, 'min_child_samples': 5, 'subsample': 0.6932316021694263, 'colsample_bytree': 0.5096822799092711, 'lambda_l1': 1.8899467636342704, 'lambda_l2': 7.588087634908076}. Best is trial 2 with value: 0.8505272996800153.\n",
      "[I 2025-11-09 13:05:12,179] Trial 10 finished with value: 0.8512829160458163 and parameters: {'learning_rate': 0.010309227463885405, 'num_leaves': 235, 'max_depth': 9, 'min_child_samples': 34, 'subsample': 0.5189232442563738, 'colsample_bytree': 0.678187375308936, 'lambda_l1': 7.777552306649568, 'lambda_l2': 0.1584166129404352}. Best is trial 2 with value: 0.8505272996800153.\n",
      "[I 2025-11-09 13:05:54,918] Trial 11 finished with value: 0.8507427465670916 and parameters: {'learning_rate': 0.025409220100004848, 'num_leaves': 182, 'max_depth': 10, 'min_child_samples': 5, 'subsample': 0.6057261592366077, 'colsample_bytree': 0.5043486584968713, 'lambda_l1': 1.6059328182596495, 'lambda_l2': 4.413356978697497}. Best is trial 2 with value: 0.8505272996800153.\n",
      "[I 2025-11-09 13:06:49,850] Trial 12 finished with value: 0.8503813067663237 and parameters: {'learning_rate': 0.020283493030465397, 'num_leaves': 176, 'max_depth': 10, 'min_child_samples': 34, 'subsample': 0.6106744563237639, 'colsample_bytree': 0.5026578832701608, 'lambda_l1': 1.9646287784748608, 'lambda_l2': 4.314304819027141}. Best is trial 12 with value: 0.8503813067663237.\n",
      "[I 2025-11-09 13:07:44,933] Trial 13 finished with value: 0.8509694588466934 and parameters: {'learning_rate': 0.012034352958792446, 'num_leaves': 203, 'max_depth': 8, 'min_child_samples': 32, 'subsample': 0.6494965419778519, 'colsample_bytree': 0.5774332110477344, 'lambda_l1': 3.2758803326698116, 'lambda_l2': 0.030349309837010452}. Best is trial 12 with value: 0.8503813067663237.\n",
      "[I 2025-11-09 13:08:36,695] Trial 14 finished with value: 0.850676142887152 and parameters: {'learning_rate': 0.020168834329083823, 'num_leaves': 147, 'max_depth': 11, 'min_child_samples': 32, 'subsample': 0.5398834484318774, 'colsample_bytree': 0.7021414457615899, 'lambda_l1': 0.525485759321719, 'lambda_l2': 1.9806133200456233}. Best is trial 12 with value: 0.8503813067663237.\n",
      "[I 2025-11-09 13:09:11,353] Trial 15 finished with value: 0.8506172990404112 and parameters: {'learning_rate': 0.039201523076642736, 'num_leaves': 254, 'max_depth': 6, 'min_child_samples': 39, 'subsample': 0.5929780343324446, 'colsample_bytree': 0.554682731576961, 'lambda_l1': 6.287553561709732, 'lambda_l2': 4.458119246157673}. Best is trial 12 with value: 0.8503813067663237.\n",
      "[I 2025-11-09 13:10:11,264] Trial 16 finished with value: 0.8507584260452818 and parameters: {'learning_rate': 0.016900798457031286, 'num_leaves': 184, 'max_depth': 11, 'min_child_samples': 28, 'subsample': 0.7146090202966273, 'colsample_bytree': 0.6956108128445705, 'lambda_l1': 3.4051590807882803, 'lambda_l2': 1.2902613391861788}. Best is trial 12 with value: 0.8503813067663237.\n",
      "[I 2025-11-09 13:10:45,029] Trial 17 finished with value: 0.8511172114483383 and parameters: {'learning_rate': 0.037639435225723605, 'num_leaves': 212, 'max_depth': 5, 'min_child_samples': 21, 'subsample': 0.5983542345505137, 'colsample_bytree': 0.5572606147840674, 'lambda_l1': 1.1493998184286247, 'lambda_l2': 3.71661011780767}. Best is trial 12 with value: 0.8503813067663237.\n",
      "[I 2025-11-09 13:11:44,851] Trial 18 finished with value: 0.8507826340346263 and parameters: {'learning_rate': 0.014512031065977522, 'num_leaves': 146, 'max_depth': 11, 'min_child_samples': 50, 'subsample': 0.6627752685781817, 'colsample_bytree': 0.6397930106265424, 'lambda_l1': 3.372277500908982, 'lambda_l2': 5.7580051094048805}. Best is trial 12 with value: 0.8503813067663237.\n",
      "[I 2025-11-09 13:12:04,290] Trial 19 finished with value: 0.8550202652746821 and parameters: {'learning_rate': 0.036936481609401425, 'num_leaves': 172, 'max_depth': 3, 'min_child_samples': 41, 'subsample': 0.559926573598512, 'colsample_bytree': 0.5139391659044948, 'lambda_l1': 2.491894895900179, 'lambda_l2': 1.2942099992525917}. Best is trial 12 with value: 0.8503813067663237.\n",
      "[I 2025-11-09 13:12:49,917] Trial 20 finished with value: 0.8510717846257804 and parameters: {'learning_rate': 0.01625319201108981, 'num_leaves': 71, 'max_depth': 8, 'min_child_samples': 35, 'subsample': 0.5034376660926227, 'colsample_bytree': 0.5946389144537947, 'lambda_l1': 6.804649833001383, 'lambda_l2': 3.467374908823675}. Best is trial 12 with value: 0.8503813067663237.\n",
      "[I 2025-11-09 13:13:29,589] Trial 21 finished with value: 0.8504979226399626 and parameters: {'learning_rate': 0.04118192612015022, 'num_leaves': 250, 'max_depth': 6, 'min_child_samples': 38, 'subsample': 0.5948792624619046, 'colsample_bytree': 0.5435880164131923, 'lambda_l1': 6.081508251472128, 'lambda_l2': 4.98172304056494}. Best is trial 12 with value: 0.8503813067663237.\n",
      "[I 2025-11-09 13:14:07,036] Trial 22 finished with value: 0.8505135255372521 and parameters: {'learning_rate': 0.054206869188128094, 'num_leaves': 215, 'max_depth': 6, 'min_child_samples': 29, 'subsample': 0.6147087273520789, 'colsample_bytree': 0.5485896803351109, 'lambda_l1': 4.918448246155025, 'lambda_l2': 5.332618395721049}. Best is trial 12 with value: 0.8503813067663237.\n",
      "[I 2025-11-09 13:14:38,376] Trial 23 finished with value: 0.8508870398155742 and parameters: {'learning_rate': 0.06064300792973523, 'num_leaves': 224, 'max_depth': 6, 'min_child_samples': 45, 'subsample': 0.7455777902839329, 'colsample_bytree': 0.7398101306325614, 'lambda_l1': 9.997926766240557, 'lambda_l2': 5.3340948940054105}. Best is trial 12 with value: 0.8503813067663237.\n",
      "[I 2025-11-09 13:15:03,007] Trial 24 finished with value: 0.8508021916748891 and parameters: {'learning_rate': 0.10084010545342882, 'num_leaves': 251, 'max_depth': 5, 'min_child_samples': 37, 'subsample': 0.6629266347641833, 'colsample_bytree': 0.6530608987357804, 'lambda_l1': 5.0229584818620365, 'lambda_l2': 5.9239173161285565}. Best is trial 12 with value: 0.8503813067663237.\n",
      "\n",
      "Best params for split 2018 : {'learning_rate': 0.020283493030465397, 'num_leaves': 176, 'max_depth': 10, 'min_child_samples': 34, 'subsample': 0.6106744563237639, 'colsample_bytree': 0.5026578832701608, 'lambda_l1': 1.9646287784748608, 'lambda_l2': 4.314304819027141}\n",
      "\n",
      "Training final model...\n",
      "\n",
      "============================================================\n",
      "METRICS FOR TEST YEAR 2019\n",
      "============================================================\n",
      "  RMSE: 0.8504\n",
      "  MAE: 0.6205\n",
      "  R2: 0.6529\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "                feature    importance\n",
      "9   log_amount_sum_lag1  5.665603e+06\n",
      "10  log_amount_sum_lag2  3.637196e+06\n",
      "11  log_amount_sum_lag3  3.128889e+06\n",
      "12     amount_avg_roll3  1.268118e+06\n",
      "13       tx_count_roll3  7.430189e+05\n",
      "3       amount_avg_lag1  5.951184e+05\n",
      "4       amount_avg_lag2  3.491274e+05\n",
      "5       amount_avg_lag3  2.216813e+05\n",
      "0         tx_count_lag1  2.043803e+05\n",
      "6    amount_stddev_lag1  1.324553e+05\n",
      "\n",
      "Baseline (predict with lag1 only):\n",
      "  RMSE: 1.3125\n",
      "  R2: 0.1732\n",
      "  Model improvement: 35.21%\n",
      "\n",
      "Exported predictions for test year 2019 -> predictions_lag_2019.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 10. Loop over temporal splits\n",
    "# ----------------------\n",
    "\n",
    "for year_split in split_years:\n",
    "    train_ym_max = year_split * 100 + 12\n",
    "    test_ym_min = (year_split + 1) * 100 + 1\n",
    "    test_ym_max = min((year_split + 1) * 100 + 12, last_ym)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Temporal split: train <= {train_ym_max} | test: {test_ym_min}..{test_ym_max}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    train_df = agg.filter(col(\"ym\") <= train_ym_max).cache()\n",
    "    test_df = agg.filter((col(\"ym\") >= test_ym_min) & (col(\"ym\") <= test_ym_max)).cache()\n",
    "\n",
    "    train_count = train_df.count()\n",
    "    test_count = test_df.count()\n",
    "    print(f\"Train rows: {train_count:,} | Test rows: {test_count:,}\")\n",
    "\n",
    "    if train_count < 50 or test_count < 10:\n",
    "        print(\"Not enough data for this split, skipping.\")\n",
    "        train_df.unpersist(); test_df.unpersist()\n",
    "        continue\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 11. Features and labels (AVEC LAG - pas de leakage)\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    feature_cols = [\n",
    "        # LAG features (historical - SAFE)\n",
    "        \"tx_count_lag1\",\n",
    "        \"tx_count_lag2\",\n",
    "        \"tx_count_lag3\",\n",
    "        \"amount_avg_lag1\",\n",
    "        \"amount_avg_lag2\",\n",
    "        \"amount_avg_lag3\",\n",
    "        \"amount_stddev_lag1\",\n",
    "        \"refund_rate_lag1\",\n",
    "        \"desc_count_lag1\",\n",
    "        \"log_amount_sum_lag1\",\n",
    "        \"log_amount_sum_lag2\",\n",
    "        \"log_amount_sum_lag3\",\n",
    "\n",
    "        # Rolling features\n",
    "        \"amount_avg_roll3\",\n",
    "        \"tx_count_roll3\",\n",
    "\n",
    "        # Trend\n",
    "        \"amount_trend\",\n",
    "\n",
    "        # Temporal features (SAFE)\n",
    "        \"month_sin\",\n",
    "        \"month_cos\",\n",
    "        \"year_norm\"\n",
    "    ]\n",
    "\n",
    "    label_col = \"log_amount_sum\"\n",
    "\n",
    "    # Coverto to pandas\n",
    "    train_pd = train_df.select(*feature_cols, label_col).toPandas().fillna(0.0)\n",
    "    test_pd = test_df.select(*feature_cols, label_col).toPandas().fillna(0.0)\n",
    "\n",
    "    X_train, y_train = train_pd[feature_cols].values, train_pd[label_col].values\n",
    "    X_test, y_test = test_pd[feature_cols].values, test_pd[label_col].values\n",
    "\n",
    "    print(f\"Feature matrix shapes: X_train={X_train.shape}, X_test={X_test.shape}\")\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 12. Run OPTUNA Optimization\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"regression\",\n",
    "            \"metric\": \"rmse\",\n",
    "            \"verbosity\": -1,\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 255),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10.0),\n",
    "            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 10.0),\n",
    "        }\n",
    "\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_val = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            lgb_train,\n",
    "            valid_sets=[lgb_val],\n",
    "            num_boost_round=500,\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "        # Calculate RMSE using numpy\n",
    "        rmse = np.sqrt(np.mean((y_test - preds) ** 2))\n",
    "        return rmse\n",
    "\n",
    "    print(\"\\nOptimizing hyperparameters with Optuna...\")\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=25, show_progress_bar=True)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(\"\\nBest params for split\", year_split, \":\", best_params)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 13. Train final model with best params\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    final_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        **best_params\n",
    "    }\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_val = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "    print(\"\\nTraining final model...\")\n",
    "    final_model = lgb.train(\n",
    "        final_params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_val],\n",
    "        num_boost_round=500,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "    )\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = final_model.predict(X_test, num_iteration=final_model.best_iteration)\n",
    "    y_true = y_test\n",
    "\n",
    "    # Metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"METRICS FOR TEST YEAR {year_split + 1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R2: {r2:.4f}\")\n",
    "\n",
    "    # Feature importance\n",
    "    importance = final_model.feature_importance(importance_type='gain')\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    print(feature_importance_df.head(10))\n",
    "\n",
    "    # Baseline comparison\n",
    "    baseline_pred = test_pd[\"log_amount_sum_lag1\"].values\n",
    "    baseline_rmse = np.sqrt(np.mean((y_true - baseline_pred) ** 2))\n",
    "    baseline_r2 = r2_score(y_true, baseline_pred)\n",
    "\n",
    "    print(f\"\\nBaseline (predict with lag1 only):\")\n",
    "    print(f\"  RMSE: {baseline_rmse:.4f}\")\n",
    "    print(f\"  R2: {baseline_r2:.4f}\")\n",
    "    print(f\"  Model improvement: {((baseline_rmse - rmse) / baseline_rmse * 100):.2f}%\")\n",
    "\n",
    "    results.append({\n",
    "        \"year_train\": year_split,\n",
    "        \"year_test\": year_split + 1,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"r2\": r2,\n",
    "        \"baseline_rmse\": baseline_rmse,\n",
    "        \"baseline_r2\": baseline_r2,\n",
    "        \"improvement_pct\": ((baseline_rmse - rmse) / baseline_rmse * 100),\n",
    "        \"best_params\": str(best_params)\n",
    "    })\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 14. Export predictions\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Get additionnals columns from the dataset test\n",
    "    test_pd_full = test_df.select(\"merchant_id\", \"year\", \"month\", \"ym\", *feature_cols, label_col).toPandas().fillna(0.0)\n",
    "\n",
    "    predictions_df = pd.DataFrame({\n",
    "        \"year_train\": [year_split] * len(y_true),\n",
    "        \"year_test\": [year_split + 1] * len(y_true),\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"residual\": y_true - y_pred,\n",
    "        \"abs_error\": np.abs(y_true - y_pred)\n",
    "    })\n",
    "\n",
    "    # add features\n",
    "    predictions_df = pd.concat([test_pd_full.reset_index(drop=True), predictions_df], axis=1)\n",
    "\n",
    "    output_file = f\"predictions_lag_{year_split + 1}.csv\"\n",
    "    predictions_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nExported predictions for test year {year_split + 1} -> {output_file}\")\n",
    "\n",
    "    # Cleanup\n",
    "    train_df.unpersist()\n",
    "    test_df.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Lh3osEYYL-Uc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lh3osEYYL-Uc",
    "outputId": "d5191bb5-8b58-40c5-b558-bca2da700199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY OF ALL TEMPORAL SPLITS\n",
      "============================================================\n",
      "   year_train  year_test      rmse       mae        r2  baseline_rmse  \\\n",
      "0        2015       2016  0.881962  0.645998  0.625915       1.518965   \n",
      "1        2016       2017  0.856482  0.627184  0.644778       1.411053   \n",
      "2        2017       2018  0.855128  0.624433  0.647409       1.355602   \n",
      "3        2018       2019  0.850381  0.620463  0.652930       1.312489   \n",
      "\n",
      "   baseline_r2  improvement_pct  \\\n",
      "0    -0.109599        41.936618   \n",
      "1     0.035838        39.301928   \n",
      "2     0.113919        36.918966   \n",
      "3     0.173237        35.208524   \n",
      "\n",
      "                                         best_params  \n",
      "0  {'learning_rate': 0.020564562345879417, 'num_l...  \n",
      "1  {'learning_rate': 0.023365235369617317, 'num_l...  \n",
      "2  {'learning_rate': 0.024777939846443612, 'num_l...  \n",
      "3  {'learning_rate': 0.020283493030465397, 'num_l...  \n",
      "\n",
      "Exported summary -> monthly_merchant_results_lag.csv\n",
      "\n",
      "============================================================\n",
      "GLOBAL STATISTICS\n",
      "============================================================\n",
      "Average RMSE: 0.8610\n",
      "Average MAE: 0.6295\n",
      "Average R2: 0.6428\n",
      "Average Baseline R2: 0.0533\n",
      "Average Improvement: 38.34%\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 15. Final Global Export\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"monthly_merchant_results_lag.csv\", index=False)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY OF ALL TEMPORAL SPLITS\")\n",
    "print(\"=\"*60)\n",
    "print(results_df)\n",
    "print(\"\\nExported summary -> monthly_merchant_results_lag.csv\")\n",
    "\n",
    "# Stats\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GLOBAL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Average RMSE: {results_df['rmse'].mean():.4f}\")\n",
    "print(f\"Average MAE: {results_df['mae'].mean():.4f}\")\n",
    "print(f\"Average R2: {results_df['r2'].mean():.4f}\")\n",
    "print(f\"Average Baseline R2: {results_df['baseline_r2'].mean():.4f}\")\n",
    "print(f\"Average Improvement: {results_df['improvement_pct'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "MxNUq501SFrg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "cc9eadb2030742bf96d953808e0efdb3",
      "e75e404a310d4164a799d2a789db4001",
      "b8ecabb69ab7436e963f055df840ef8a",
      "ccf41597e3f44294bb6e17a783390e72",
      "d0dc88585dd24ca39912da384c9a0534",
      "db0a37d945b2415bb23300a5219e4732",
      "629bc32c546f4bb886fcaf8473df8c94",
      "f81c4f0a997446e6aaebe81c8dbe8341",
      "50efd6656f8f48499600878430c8a69c",
      "470193bcae4d41edacc494799208a4a7",
      "96b4eb764b124961b57b9f16251486c8"
     ]
    },
    "id": "MxNUq501SFrg",
    "outputId": "ccb29207-80eb-42df-b783-6f9f568cb87a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING PREDICTIONS FOR NOV 2019 - APR 2020 (6 MONTHS)\n",
      "============================================================\n",
      "Training on all data up to 201910 (2015 - Oct 2019)\n",
      "Train rows: 606,168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:16:23,245] A new study created in memory with name: no-name-fe63e4f0-0414-486f-9d4d-225baed0b70d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: X_train=(606168, 18)\n",
      "\n",
      "Optimizing hyperparameters with Optuna for prediction model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9eadb2030742bf96d953808e0efdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:20:33,594] Trial 0 finished with value: 0.8846031822241083 and parameters: {'learning_rate': 0.013335989004246833, 'num_leaves': 78, 'max_depth': 8, 'min_child_samples': 28, 'subsample': 0.6334890102321118, 'colsample_bytree': 0.9464321033847867, 'lambda_l1': 8.074903436944373, 'lambda_l2': 9.55346031945621}. Best is trial 0 with value: 0.8846031822241083.\n",
      "[I 2025-11-09 13:23:36,211] Trial 1 finished with value: 0.8842822009198518 and parameters: {'learning_rate': 0.018604601716857988, 'num_leaves': 60, 'max_depth': 9, 'min_child_samples': 6, 'subsample': 0.6232409418081595, 'colsample_bytree': 0.501905701911905, 'lambda_l1': 7.50693635381311, 'lambda_l2': 6.770479926173012}. Best is trial 1 with value: 0.8842822009198518.\n",
      "[I 2025-11-09 13:26:51,809] Trial 2 finished with value: 0.8842027843571684 and parameters: {'learning_rate': 0.027336025949748773, 'num_leaves': 156, 'max_depth': 15, 'min_child_samples': 25, 'subsample': 0.7740282508224836, 'colsample_bytree': 0.8169344793940757, 'lambda_l1': 9.982740302209173, 'lambda_l2': 7.383787024593294}. Best is trial 2 with value: 0.8842027843571684.\n",
      "[I 2025-11-09 13:27:30,401] Trial 3 finished with value: 0.8852090339143656 and parameters: {'learning_rate': 0.17645545827877337, 'num_leaves': 162, 'max_depth': 9, 'min_child_samples': 17, 'subsample': 0.7346507632143975, 'colsample_bytree': 0.6247713367809278, 'lambda_l1': 5.118017448833353, 'lambda_l2': 0.4086187394834484}. Best is trial 2 with value: 0.8842027843571684.\n",
      "[I 2025-11-09 13:28:26,057] Trial 4 finished with value: 0.884685230538337 and parameters: {'learning_rate': 0.10926623397325348, 'num_leaves': 169, 'max_depth': 14, 'min_child_samples': 49, 'subsample': 0.6294694741390865, 'colsample_bytree': 0.7705592978017657, 'lambda_l1': 0.9054998543462511, 'lambda_l2': 7.106871035505789}. Best is trial 2 with value: 0.8842027843571684.\n",
      "[I 2025-11-09 13:32:21,463] Trial 5 finished with value: 0.8844118925058051 and parameters: {'learning_rate': 0.014262046670892715, 'num_leaves': 65, 'max_depth': 12, 'min_child_samples': 33, 'subsample': 0.702830915085421, 'colsample_bytree': 0.8532766381387613, 'lambda_l1': 3.605262284392224, 'lambda_l2': 6.949337453772035}. Best is trial 2 with value: 0.8842027843571684.\n",
      "[I 2025-11-09 13:33:00,153] Trial 6 finished with value: 0.8859782243525103 and parameters: {'learning_rate': 0.2653727684296094, 'num_leaves': 215, 'max_depth': 11, 'min_child_samples': 24, 'subsample': 0.7088541783004181, 'colsample_bytree': 0.9122469438790746, 'lambda_l1': 7.12422483662419, 'lambda_l2': 4.510481275921212}. Best is trial 2 with value: 0.8842027843571684.\n",
      "[I 2025-11-09 13:35:03,236] Trial 7 finished with value: 0.8857464625353584 and parameters: {'learning_rate': 0.04555654657617803, 'num_leaves': 22, 'max_depth': 4, 'min_child_samples': 46, 'subsample': 0.7313546586705563, 'colsample_bytree': 0.718822471368765, 'lambda_l1': 9.335572690231324, 'lambda_l2': 9.38475840536207}. Best is trial 2 with value: 0.8842027843571684.\n",
      "[I 2025-11-09 13:38:39,371] Trial 8 finished with value: 0.8840415627099839 and parameters: {'learning_rate': 0.021884939569148266, 'num_leaves': 197, 'max_depth': 9, 'min_child_samples': 5, 'subsample': 0.7734500453805733, 'colsample_bytree': 0.7764558672740998, 'lambda_l1': 1.1929873380325617, 'lambda_l2': 4.163555381552375}. Best is trial 8 with value: 0.8840415627099839.\n",
      "[I 2025-11-09 13:42:31,396] Trial 9 finished with value: 0.8843152604778319 and parameters: {'learning_rate': 0.01780072288217397, 'num_leaves': 68, 'max_depth': 9, 'min_child_samples': 33, 'subsample': 0.6683215150286161, 'colsample_bytree': 0.9707913940471489, 'lambda_l1': 3.5675873414663064, 'lambda_l2': 5.263774555641298}. Best is trial 8 with value: 0.8840415627099839.\n",
      "[I 2025-11-09 13:44:45,299] Trial 10 finished with value: 0.8844371962137612 and parameters: {'learning_rate': 0.05790574580192053, 'num_leaves': 248, 'max_depth': 5, 'min_child_samples': 7, 'subsample': 0.9209158310979857, 'colsample_bytree': 0.6488450331483804, 'lambda_l1': 0.9824875442702321, 'lambda_l2': 2.063232193716595}. Best is trial 8 with value: 0.8840415627099839.\n",
      "[I 2025-11-09 13:47:05,267] Trial 11 finished with value: 0.8842239860931673 and parameters: {'learning_rate': 0.03772404644890709, 'num_leaves': 138, 'max_depth': 15, 'min_child_samples': 16, 'subsample': 0.843649812890112, 'colsample_bytree': 0.8176011410546056, 'lambda_l1': 4.976783745390308, 'lambda_l2': 3.710328269292545}. Best is trial 8 with value: 0.8840415627099839.\n",
      "[I 2025-11-09 13:50:00,685] Trial 12 finished with value: 0.8842879929100906 and parameters: {'learning_rate': 0.028172681873871886, 'num_leaves': 209, 'max_depth': 6, 'min_child_samples': 16, 'subsample': 0.5254444669228814, 'colsample_bytree': 0.7092960594715501, 'lambda_l1': 2.3601522154916887, 'lambda_l2': 2.9430298851829777}. Best is trial 8 with value: 0.8840415627099839.\n",
      "[I 2025-11-09 13:51:24,701] Trial 13 finished with value: 0.8844664494318486 and parameters: {'learning_rate': 0.07623134287157816, 'num_leaves': 195, 'max_depth': 12, 'min_child_samples': 41, 'subsample': 0.8230738688681152, 'colsample_bytree': 0.8593138328136608, 'lambda_l1': 9.813372509076185, 'lambda_l2': 5.814221571926939}. Best is trial 8 with value: 0.8840415627099839.\n",
      "[I 2025-11-09 13:54:32,029] Trial 14 finished with value: 0.8840093774057074 and parameters: {'learning_rate': 0.027553193419569452, 'num_leaves': 120, 'max_depth': 11, 'min_child_samples': 24, 'subsample': 0.8209691076121762, 'colsample_bytree': 0.7852126904513914, 'lambda_l1': 0.11483535596222239, 'lambda_l2': 8.207973666518315}. Best is trial 14 with value: 0.8840093774057074.\n",
      "[I 2025-11-09 13:57:32,841] Trial 15 finished with value: 0.8839617379247875 and parameters: {'learning_rate': 0.02615776905677737, 'num_leaves': 113, 'max_depth': 11, 'min_child_samples': 14, 'subsample': 0.9926210962207955, 'colsample_bytree': 0.6244613401139286, 'lambda_l1': 0.39079948010179, 'lambda_l2': 8.558007507634379}. Best is trial 15 with value: 0.8839617379247875.\n",
      "[I 2025-11-09 14:01:29,089] Trial 16 finished with value: 0.8846385275915359 and parameters: {'learning_rate': 0.010176862760970072, 'num_leaves': 106, 'max_depth': 11, 'min_child_samples': 12, 'subsample': 0.9798155713862762, 'colsample_bytree': 0.6016182535839907, 'lambda_l1': 0.30517785365731886, 'lambda_l2': 8.384927420481628}. Best is trial 15 with value: 0.8839617379247875.\n",
      "[I 2025-11-09 14:04:17,216] Trial 17 finished with value: 0.8838422046765982 and parameters: {'learning_rate': 0.035116437035344304, 'num_leaves': 107, 'max_depth': 7, 'min_child_samples': 20, 'subsample': 0.993015755106489, 'colsample_bytree': 0.5499652728013184, 'lambda_l1': 2.2491627212527208, 'lambda_l2': 8.454823321888567}. Best is trial 17 with value: 0.8838422046765982.\n",
      "[I 2025-11-09 14:06:30,797] Trial 18 finished with value: 0.8840577747574538 and parameters: {'learning_rate': 0.061887056152049466, 'num_leaves': 93, 'max_depth': 6, 'min_child_samples': 19, 'subsample': 0.9972239659827485, 'colsample_bytree': 0.521103924942693, 'lambda_l1': 2.3271966246173914, 'lambda_l2': 9.860720049467865}. Best is trial 17 with value: 0.8838422046765982.\n",
      "[I 2025-11-09 14:08:14,678] Trial 19 finished with value: 0.8846613321316997 and parameters: {'learning_rate': 0.10166940766956384, 'num_leaves': 29, 'max_depth': 7, 'min_child_samples': 11, 'subsample': 0.9169183708460988, 'colsample_bytree': 0.5670398145067198, 'lambda_l1': 2.3645522703129194, 'lambda_l2': 8.319874318730822}. Best is trial 17 with value: 0.8838422046765982.\n",
      "[I 2025-11-09 14:09:48,054] Trial 20 finished with value: 0.8885921960338699 and parameters: {'learning_rate': 0.036422805831753766, 'num_leaves': 130, 'max_depth': 3, 'min_child_samples': 21, 'subsample': 0.9225270056965028, 'colsample_bytree': 0.6717827315028412, 'lambda_l1': 4.327746411443206, 'lambda_l2': 5.96086189397017}. Best is trial 17 with value: 0.8838422046765982.\n",
      "[I 2025-11-09 14:12:36,068] Trial 21 finished with value: 0.8839869331538036 and parameters: {'learning_rate': 0.028724392148727462, 'num_leaves': 118, 'max_depth': 11, 'min_child_samples': 32, 'subsample': 0.8728743989043901, 'colsample_bytree': 0.5828266487141597, 'lambda_l1': 0.31841549971666794, 'lambda_l2': 8.355028554371378}. Best is trial 17 with value: 0.8838422046765982.\n",
      "[I 2025-11-09 14:15:07,190] Trial 22 finished with value: 0.8840925924344913 and parameters: {'learning_rate': 0.038668978899351794, 'num_leaves': 90, 'max_depth': 13, 'min_child_samples': 32, 'subsample': 0.9559418023414976, 'colsample_bytree': 0.5599139104439453, 'lambda_l1': 1.5109171966481003, 'lambda_l2': 8.796645316311979}. Best is trial 17 with value: 0.8838422046765982.\n",
      "[I 2025-11-09 14:18:13,951] Trial 23 finished with value: 0.8838669368122606 and parameters: {'learning_rate': 0.023857741562910376, 'num_leaves': 114, 'max_depth': 10, 'min_child_samples': 39, 'subsample': 0.8730336996415933, 'colsample_bytree': 0.5735819956427687, 'lambda_l1': 0.020132117595396792, 'lambda_l2': 7.601739911464874}. Best is trial 17 with value: 0.8838422046765982.\n",
      "[I 2025-11-09 14:21:03,325] Trial 24 finished with value: 0.8845498998453032 and parameters: {'learning_rate': 0.02179722805408616, 'num_leaves': 43, 'max_depth': 7, 'min_child_samples': 40, 'subsample': 0.8872923008786713, 'colsample_bytree': 0.5397087130722762, 'lambda_l1': 1.883585315953224, 'lambda_l2': 7.667890763098527}. Best is trial 17 with value: 0.8838422046765982.\n",
      "\n",
      "Best params for prediction: {'learning_rate': 0.035116437035344304, 'num_leaves': 107, 'max_depth': 7, 'min_child_samples': 20, 'subsample': 0.993015755106489, 'colsample_bytree': 0.5499652728013184, 'lambda_l1': 2.2491627212527208, 'lambda_l2': 8.454823321888567}\n",
      "\n",
      "Training final prediction model...\n",
      "Model trained successfully!\n",
      "\n",
      "Preparing October 2019 data as baseline...\n",
      "Merchants with data in Oct 2019: 10,596\n",
      "\n",
      "Generating predictions for 6 months...\n",
      "  Predicting for 2019-11...\n",
      "  Predicting for 2019-12...\n",
      "  Predicting for 2020-01...\n",
      "  Predicting for 2020-02...\n",
      "  Predicting for 2020-03...\n",
      "  Predicting for 2020-04...\n",
      "\n",
      "============================================================\n",
      "PREDICTIONS SUMMARY (NOV 2019 - APR 2020)\n",
      "============================================================\n",
      "Total predictions: 63,576\n",
      "Unique merchants: 10,596\n",
      "Months covered: 1 to 12\n",
      "Years covered: [2019, 2020]\n",
      "\n",
      "Total predicted amount (6 months): $26,985,019.59\n",
      "Average monthly amount per merchant: $424.45\n",
      "\n",
      "Monthly breakdown:\n",
      "           amount_predicted               \n",
      "                        sum    mean  count\n",
      "year month                                \n",
      "2019 11          4558642.72  430.22  10596\n",
      "     12          4663609.29  440.13  10596\n",
      "2020 1           4602054.29  434.32  10596\n",
      "     2           4239734.68  400.13  10596\n",
      "     3           4540573.83  428.52  10596\n",
      "     4           4380404.78  413.40  10596\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "                feature    importance\n",
      "9   log_amount_sum_lag1  4.664152e+06\n",
      "10  log_amount_sum_lag2  2.511502e+06\n",
      "11  log_amount_sum_lag3  2.042653e+06\n",
      "12     amount_avg_roll3  9.902599e+05\n",
      "13       tx_count_roll3  3.891844e+05\n",
      "3       amount_avg_lag1  2.883137e+05\n",
      "4       amount_avg_lag2  1.946770e+05\n",
      "5       amount_avg_lag3  1.066860e+05\n",
      "1         tx_count_lag2  6.972060e+04\n",
      "0         tx_count_lag1  6.770932e+04\n",
      "\n",
      "Exported predictions -> predictions_nov2019_apr2020.csv\n",
      "\n",
      "============================================================\n",
      "PREDICTION COMPLETED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# PREDICTION NOVEMBRE 2019 - AVRIL 2020 (6 mois)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING PREDICTIONS FOR NOV 2019 - APR 2020 (6 MONTHS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# IMPORTANT: Training only on data BEFORE November 2019 to avoid data leakage\n",
    "train_ym_max_pred = 2019 * 100 + 10  # October 2019 (201910)\n",
    "print(f\"Training on all data up to {train_ym_max_pred} (2015 - Oct 2019)\")\n",
    "\n",
    "train_df_pred = agg.filter(col(\"ym\") <= train_ym_max_pred).cache()\n",
    "train_count_pred = train_df_pred.count()\n",
    "print(f\"Train rows: {train_count_pred:,}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Features and labels\n",
    "# ----------------------------------------------------\n",
    "\n",
    "feature_cols_pred = [\n",
    "    # LAG features\n",
    "    \"tx_count_lag1\",\n",
    "    \"tx_count_lag2\",\n",
    "    \"tx_count_lag3\",\n",
    "    \"amount_avg_lag1\",\n",
    "    \"amount_avg_lag2\",\n",
    "    \"amount_avg_lag3\",\n",
    "    \"amount_stddev_lag1\",\n",
    "    \"refund_rate_lag1\",\n",
    "    \"desc_count_lag1\",\n",
    "    \"log_amount_sum_lag1\",\n",
    "    \"log_amount_sum_lag2\",\n",
    "    \"log_amount_sum_lag3\",\n",
    "\n",
    "    # Rolling features\n",
    "    \"amount_avg_roll3\",\n",
    "    \"tx_count_roll3\",\n",
    "\n",
    "    # Trend\n",
    "    \"amount_trend\",\n",
    "\n",
    "    # Temporal features\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"year_norm\"\n",
    "]\n",
    "\n",
    "label_col_pred = \"log_amount_sum\"\n",
    "\n",
    "# Convert to pandas\n",
    "train_pd_pred = train_df_pred.select(*feature_cols_pred, label_col_pred).toPandas().fillna(0.0)\n",
    "X_train_pred, y_train_pred = train_pd_pred[feature_cols_pred].values, train_pd_pred[label_col_pred].values\n",
    "\n",
    "print(f\"Feature matrix shape: X_train={X_train_pred.shape}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Optimisation Optuna for prediction model\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def objective_pred(trial):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 255),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 10.0),\n",
    "    }\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train_pred, y_train_pred)\n",
    "\n",
    "    # Cross-validation with KFold for regression (not StratifiedKFold)\n",
    "    from sklearn.model_selection import KFold\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_results = lgb.cv(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=500,\n",
    "        folds=kfold,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)],\n",
    "        return_cvbooster=False\n",
    "    )\n",
    "\n",
    "    # Return the best RMSE\n",
    "    best_rmse = cv_results['valid rmse-mean'][-1]\n",
    "    return best_rmse\n",
    "\n",
    "print(\"\\nOptimizing hyperparameters with Optuna for prediction model...\")\n",
    "study_pred = optuna.create_study(direction=\"minimize\")\n",
    "study_pred.optimize(objective_pred, n_trials=25, show_progress_bar=True)\n",
    "\n",
    "best_params_pred = study_pred.best_params\n",
    "print(\"\\nBest params for prediction:\", best_params_pred)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Final training for prediction\n",
    "# ----------------------------------------------------\n",
    "\n",
    "final_params_pred = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    **best_params_pred\n",
    "}\n",
    "\n",
    "lgb_train_final_pred = lgb.Dataset(X_train_pred, y_train_pred)\n",
    "\n",
    "print(\"\\nTraining final prediction model...\")\n",
    "final_model_pred = lgb.train(\n",
    "    final_params_pred,\n",
    "    lgb_train_final_pred,\n",
    "    num_boost_round=500,\n",
    "    callbacks=[lgb.log_evaluation(period=0)]\n",
    ")\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Prepare last known data (October 2019)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "print(\"\\nPreparing October 2019 data as baseline...\")\n",
    "\n",
    "data_oct_2019 = agg.filter(col(\"ym\") == 201910).cache()\n",
    "\n",
    "merchants_oct = data_oct_2019.select(\n",
    "    \"merchant_id\",\n",
    "    \"tx_count\",\n",
    "    \"amount_avg\",\n",
    "    \"amount_stddev\",\n",
    "    \"refund_rate\",\n",
    "    \"desc_count\",\n",
    "    \"log_amount_sum\",\n",
    "    # Get lag features if they exist\n",
    "    \"tx_count_lag1\",\n",
    "    \"amount_avg_lag1\",\n",
    "    \"log_amount_sum_lag1\"\n",
    ").toPandas()\n",
    "\n",
    "print(f\"Merchants with data in Oct 2019: {len(merchants_oct):,}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Generate predictions for Nov 2019 - Apr 2020 (6 months)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "predictions_list = []\n",
    "\n",
    "# Define target months: Nov 2019 to Apr 2020\n",
    "target_periods = [\n",
    "    (2019, 11), (2019, 12),  # Nov-Dec 2019\n",
    "    (2020, 1), (2020, 2), (2020, 3), (2020, 4)  # Jan-Apr 2020\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating predictions for 6 months...\")\n",
    "\n",
    "# Store predictions to update lags iteratively\n",
    "merchant_state = {}\n",
    "\n",
    "# Initialize state with October 2019 data\n",
    "for idx, row in merchants_oct.iterrows():\n",
    "    merchant_state[row[\"merchant_id\"]] = {\n",
    "        \"last_3_values\": [\n",
    "            row.get(\"log_amount_sum_lag1\", row[\"log_amount_sum\"]),\n",
    "            row[\"log_amount_sum\"],\n",
    "            row[\"log_amount_sum\"]\n",
    "        ],\n",
    "        \"last_3_tx_count\": [\n",
    "            row.get(\"tx_count_lag1\", row[\"tx_count\"]),\n",
    "            row[\"tx_count\"],\n",
    "            row[\"tx_count\"]\n",
    "        ],\n",
    "        \"last_amount_avg\": row[\"amount_avg\"],\n",
    "        \"last_amount_stddev\": row[\"amount_stddev\"],\n",
    "        \"last_refund_rate\": row[\"refund_rate\"],\n",
    "        \"last_desc_count\": row[\"desc_count\"]\n",
    "    }\n",
    "\n",
    "# Iterative prediction with lag updates\n",
    "for year, month in target_periods:\n",
    "    print(f\"  Predicting for {year}-{month:02d}...\")\n",
    "\n",
    "    for merchant_id, state in merchant_state.items():\n",
    "        # Use last 3 values for lags\n",
    "        log_vals = state[\"last_3_values\"]\n",
    "        tx_vals = state[\"last_3_tx_count\"]\n",
    "\n",
    "        features_dict = {\n",
    "            \"tx_count_lag1\": tx_vals[-1],\n",
    "            \"tx_count_lag2\": tx_vals[-2],\n",
    "            \"tx_count_lag3\": tx_vals[-3],\n",
    "            \"amount_avg_lag1\": state[\"last_amount_avg\"],\n",
    "            \"amount_avg_lag2\": state[\"last_amount_avg\"],\n",
    "            \"amount_avg_lag3\": state[\"last_amount_avg\"],\n",
    "            \"amount_stddev_lag1\": state[\"last_amount_stddev\"],\n",
    "            \"refund_rate_lag1\": state[\"last_refund_rate\"],\n",
    "            \"desc_count_lag1\": state[\"last_desc_count\"],\n",
    "            \"log_amount_sum_lag1\": log_vals[-1],\n",
    "            \"log_amount_sum_lag2\": log_vals[-2],\n",
    "            \"log_amount_sum_lag3\": log_vals[-3],\n",
    "            \"amount_avg_roll3\": state[\"last_amount_avg\"],\n",
    "            \"tx_count_roll3\": np.mean(tx_vals),\n",
    "            \"amount_trend\": (log_vals[-1] - log_vals[-3]) / 3 if len(log_vals) >= 3 else 0.0,\n",
    "            \"month_sin\": np.sin(2 * np.pi * month / 12),\n",
    "            \"month_cos\": np.cos(2 * np.pi * month / 12),\n",
    "            \"year_norm\": year - 2015,\n",
    "        }\n",
    "\n",
    "        # Create feature vector\n",
    "        X_pred = np.array([[features_dict[col] for col in feature_cols_pred]])\n",
    "\n",
    "        # Prediction\n",
    "        y_pred_log = final_model_pred.predict(X_pred)[0]\n",
    "        y_pred_original = np.exp(y_pred_log) - 1\n",
    "\n",
    "        # Update state for next iteration\n",
    "        state[\"last_3_values\"].pop(0)\n",
    "        state[\"last_3_values\"].append(y_pred_log)\n",
    "\n",
    "        # Store prediction\n",
    "        predictions_list.append({\n",
    "            \"merchant_id\": merchant_id,\n",
    "            \"year\": year,\n",
    "            \"month\": month,\n",
    "            \"ym\": year * 100 + month,\n",
    "            \"y_pred_log\": y_pred_log,\n",
    "            \"amount_predicted\": y_pred_original,\n",
    "            **features_dict\n",
    "        })\n",
    "\n",
    "data_oct_2019.unpersist()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Results\n",
    "# ----------------------------------------------------\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions_list)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PREDICTIONS SUMMARY (NOV 2019 - APR 2020)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total predictions: {len(predictions_df):,}\")\n",
    "print(f\"Unique merchants: {predictions_df['merchant_id'].nunique():,}\")\n",
    "print(f\"Months covered: {predictions_df['month'].min()} to {predictions_df['month'].max()}\")\n",
    "print(f\"Years covered: {sorted(predictions_df['year'].unique())}\")\n",
    "print(f\"\\nTotal predicted amount (6 months): ${predictions_df['amount_predicted'].sum():,.2f}\")\n",
    "print(f\"Average monthly amount per merchant: ${predictions_df['amount_predicted'].mean():,.2f}\")\n",
    "\n",
    "# Monthly breakdown\n",
    "monthly_summary = predictions_df.groupby(['year', 'month']).agg({\n",
    "    'amount_predicted': ['sum', 'mean', 'count']\n",
    "}).round(2)\n",
    "print(\"\\nMonthly breakdown:\")\n",
    "print(monthly_summary)\n",
    "\n",
    "# Feature importance\n",
    "importance = final_model_pred.feature_importance(importance_type='gain')\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols_pred,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Feature Importances:\")\n",
    "print(feature_importance_df.head(10))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Export\n",
    "# ----------------------------------------------------\n",
    "\n",
    "output_file = \"predictions_nov2019_apr2020.csv\"\n",
    "predictions_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nExported predictions -> {output_file}\")\n",
    "\n",
    "# Cleanup\n",
    "train_df_pred.unpersist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QEwCYeBPpkjR",
   "metadata": {
    "id": "QEwCYeBPpkjR"
   },
   "source": [
    "# <b><span style=\"color:blue;\">3. CONCLUSION<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SAPF7EFWGnU0",
   "metadata": {
    "id": "SAPF7EFWGnU0"
   },
   "source": [
    "#### <b>A time series machine learning algorithm has to be clearly thought out and prepared.\n",
    "The exercise was to create an algorithm that would fit the demand, which was to forecast data and trends monthly. So , I had to find the best algorithm and methodology to avoid data leakage, and the best hyperparameters using Optuna. Then , I will include the predictions in the Power BI report .\n",
    "\n",
    "Reminder: Just like data in real-life production, the algorithm must also live and be updated depending on the initial demand. In this case , it could be updated once a month, for example.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BXIhEJ8yN3PX",
   "metadata": {
    "id": "BXIhEJ8yN3PX"
   },
   "source": [
    "## **Author : Aurelien Legendre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6RoJs72zVQzC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "6RoJs72zVQzC",
    "outputId": "b71b903a-da82-4004-96f6-499e48cc476a"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_DfYqI_GgLvc",
   "metadata": {
    "id": "_DfYqI_GgLvc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "qD6FHhsjG4o3",
   "metadata": {
    "id": "qD6FHhsjG4o3"
   },
   "source": [
    "## <span style=\"color:blue;\">**Extra Experiment** - Combining LightGBM with ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0xwl9G_VgLtd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "0xwl9G_VgLtd",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a23b7859-8ae2-4a79-f728-474bc108d811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  ca-certificates-java fonts-dejavu-core fonts-dejavu-extra java-common\n",
      "  libatk-wrapper-java libatk-wrapper-java-jni libpcsclite1 libxt-dev libxtst6\n",
      "  libxxf86dga1 openjdk-11-jdk-headless openjdk-11-jre openjdk-11-jre-headless\n",
      "  x11-utils\n",
      "Suggested packages:\n",
      "  default-jre pcscd libxt-doc openjdk-11-demo openjdk-11-source visualvm\n",
      "  libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
      "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
      "The following NEW packages will be installed:\n",
      "  ca-certificates-java fonts-dejavu-core fonts-dejavu-extra java-common\n",
      "  libatk-wrapper-java libatk-wrapper-java-jni libpcsclite1 libxt-dev libxtst6\n",
      "  libxxf86dga1 openjdk-11-jdk openjdk-11-jdk-headless openjdk-11-jre\n",
      "  openjdk-11-jre-headless x11-utils\n",
      "0 upgraded, 15 newly installed, 0 to remove and 41 not upgraded.\n",
      "Need to get 122 MB of archives.\n",
      "After this operation, 274 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 java-common all 0.72build2 [6,782 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpcsclite1 amd64 1.9.5-3ubuntu1 [19.8 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [42.6 MB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ca-certificates-java all 20190909ubuntu1.2 [12.1 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1 [214 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [73.6 MB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.28+6-1ubuntu1~22.04.1 [1,342 kB]\n",
      "Fetched 122 MB in 3s (43.7 MB/s)\n",
      "Selecting previously unselected package java-common.\n",
      "(Reading database ... 125082 files and directories currently installed.)\n",
      "Preparing to unpack .../00-java-common_0.72build2_all.deb ...\n",
      "Unpacking java-common (0.72build2) ...\n",
      "Selecting previously unselected package libpcsclite1:amd64.\n",
      "Preparing to unpack .../01-libpcsclite1_1.9.5-3ubuntu1_amd64.deb ...\n",
      "Unpacking libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
      "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
      "Preparing to unpack .../02-openjdk-11-jre-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Selecting previously unselected package ca-certificates-java.\n",
      "Preparing to unpack .../03-ca-certificates-java_20190909ubuntu1.2_all.deb ...\n",
      "Unpacking ca-certificates-java (20190909ubuntu1.2) ...\n",
      "Selecting previously unselected package fonts-dejavu-core.\n",
      "Preparing to unpack .../04-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
      "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
      "Selecting previously unselected package fonts-dejavu-extra.\n",
      "Preparing to unpack .../05-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
      "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
      "Selecting previously unselected package libxtst6:amd64.\n",
      "Preparing to unpack .../06-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
      "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Selecting previously unselected package libxxf86dga1:amd64.\n",
      "Preparing to unpack .../07-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
      "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
      "Selecting previously unselected package x11-utils.\n",
      "Preparing to unpack .../08-x11-utils_7.7+5build2_amd64.deb ...\n",
      "Unpacking x11-utils (7.7+5build2) ...\n",
      "Selecting previously unselected package libatk-wrapper-java.\n",
      "Preparing to unpack .../09-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
      "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
      "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
      "Preparing to unpack .../10-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
      "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
      "Selecting previously unselected package libxt-dev:amd64.\n",
      "Preparing to unpack .../11-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
      "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
      "Selecting previously unselected package openjdk-11-jre:amd64.\n",
      "Preparing to unpack .../12-openjdk-11-jre_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Selecting previously unselected package openjdk-11-jdk-headless:amd64.\n",
      "Preparing to unpack .../13-openjdk-11-jdk-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
      "Preparing to unpack .../14-openjdk-11-jdk_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Setting up java-common (0.72build2) ...\n",
      "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
      "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
      "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
      "Setting up libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
      "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
      "Setting up x11-utils (7.7+5build2) ...\n",
      "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
      "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
      "Setting up openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
      "Setting up openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "Setting up openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
      "Setting up openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
      "Setting up ca-certificates-java (20190909ubuntu1.2) ...\n",
      "head: cannot open '/etc/ssl/certs/java/cacerts' for reading: No such file or directory\n",
      "Adding debian:GlobalSign_Root_CA.pem\n",
      "Adding debian:Certigna.pem\n",
      "Adding debian:Trustwave_Global_ECC_P384_Certification_Authority.pem\n",
      "Adding debian:Certainly_Root_E1.pem\n",
      "Adding debian:Amazon_Root_CA_3.pem\n",
      "Adding debian:certSIGN_Root_CA_G2.pem\n",
      "Adding debian:vTrus_ECC_Root_CA.pem\n",
      "Adding debian:Amazon_Root_CA_2.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_C3.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM_SERVIDORES_SEGUROS.pem\n",
      "Adding debian:AffirmTrust_Premium.pem\n",
      "Adding debian:DigiCert_TLS_RSA4096_Root_G5.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
      "Adding debian:emSign_Root_CA_-_G1.pem\n",
      "Adding debian:Secure_Global_CA.pem\n",
      "Adding debian:Amazon_Root_CA_1.pem\n",
      "Adding debian:certSIGN_ROOT_CA.pem\n",
      "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
      "Adding debian:Certigna_Root_CA.pem\n",
      "Adding debian:GTS_Root_R4.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
      "Adding debian:Microsoft_ECC_Root_Certificate_Authority_2017.pem\n",
      "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
      "Adding debian:ISRG_Root_X2.pem\n",
      "Adding debian:GTS_Root_R1.pem\n",
      "Adding debian:Starfield_Class_2_CA.pem\n",
      "Adding debian:Baltimore_CyberTrust_Root.pem\n",
      "Adding debian:GTS_Root_R3.pem\n",
      "Adding debian:Microsoft_RSA_Root_Certificate_Authority_2017.pem\n",
      "Adding debian:Comodo_AAA_Services_root.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:Actalis_Authentication_Root_CA.pem\n",
      "Adding debian:CA_Disig_Root_R2.pem\n",
      "Adding debian:Trustwave_Global_Certification_Authority.pem\n",
      "Adding debian:XRamp_Global_CA_Root.pem\n",
      "Adding debian:Certum_Trusted_Root_CA.pem\n",
      "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
      "Adding debian:Security_Communication_ECC_RootCA1.pem\n",
      "Adding debian:Certum_Trusted_Network_CA.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
      "Adding debian:ISRG_Root_X1.pem\n",
      "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
      "Adding debian:TWCA_Root_Certification_Authority.pem\n",
      "Adding debian:HARICA_TLS_RSA_Root_CA_2021.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
      "Adding debian:HARICA_TLS_ECC_Root_CA_2021.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
      "Adding debian:GlobalSign_Root_R46.pem\n",
      "Adding debian:emSign_Root_CA_-_C1.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
      "Adding debian:HiPKI_Root_CA_-_G1.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
      "Adding debian:DigiCert_TLS_ECC_P384_Root_G5.pem\n",
      "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
      "Adding debian:Go_Daddy_Class_2_CA.pem\n",
      "Adding debian:SecureTrust_CA.pem\n",
      "Adding debian:TWCA_Global_Root_CA.pem\n",
      "Adding debian:e-Szigno_Root_CA_2017.pem\n",
      "Adding debian:Atos_TrustedRoot_2011.pem\n",
      "Adding debian:Security_Communication_Root_CA.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
      "Adding debian:NAVER_Global_Root_Certification_Authority.pem\n",
      "Adding debian:UCA_Global_G2_Root.pem\n",
      "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:SZAFIR_ROOT_CA2.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
      "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
      "Adding debian:GLOBALTRUST_2020.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority.pem\n",
      "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
      "Adding debian:D-TRUST_EV_Root_CA_1_2020.pem\n",
      "Adding debian:AffirmTrust_Commercial.pem\n",
      "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
      "Adding debian:UCA_Extended_Validation_Root.pem\n",
      "Adding debian:AffirmTrust_Networking.pem\n",
      "Adding debian:ACCVRAIZ1.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R6.pem\n",
      "Adding debian:Certainly_Root_R1.pem\n",
      "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
      "Adding debian:Buypass_Class_2_Root_CA.pem\n",
      "Adding debian:Hongkong_Post_Root_CA_3.pem\n",
      "Adding debian:vTrus_Root_CA.pem\n",
      "Adding debian:AffirmTrust_Premium_ECC.pem\n",
      "Adding debian:DigiCert_Global_Root_CA.pem\n",
      "Adding debian:Security_Communication_RootCA3.pem\n",
      "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
      "Adding debian:Security_Communication_RootCA2.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
      "Adding debian:COMODO_Certification_Authority.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
      "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:Telia_Root_CA_v2.pem\n",
      "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
      "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
      "Adding debian:Buypass_Class_3_Root_CA.pem\n",
      "Adding debian:GTS_Root_R2.pem\n",
      "Adding debian:NetLock_Arany_=Class_Gold=_Főtanúsítvány.pem\n",
      "Adding debian:QuoVadis_Root_CA_2.pem\n",
      "Adding debian:QuoVadis_Root_CA_3.pem\n",
      "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
      "Adding debian:Izenpe.com.pem\n",
      "Adding debian:ANF_Secure_Server_Root_CA.pem\n",
      "Adding debian:TunTrust_Root_CA.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_G3.pem\n",
      "Adding debian:Certum_EC-384_CA.pem\n",
      "Adding debian:CFCA_EV_ROOT.pem\n",
      "Adding debian:DigiCert_Global_Root_G3.pem\n",
      "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
      "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:SecureSign_RootCA11.pem\n",
      "Adding debian:Amazon_Root_CA_4.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G4.pem\n",
      "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
      "Adding debian:ePKI_Root_Certification_Authority.pem\n",
      "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
      "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:DigiCert_Global_Root_G2.pem\n",
      "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
      "Adding debian:Trustwave_Global_ECC_P256_Certification_Authority.pem\n",
      "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
      "Adding debian:GlobalSign_Root_E46.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
      "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
      "Adding debian:D-TRUST_BR_Root_CA_1_2020.pem\n",
      "Adding debian:Atos_TrustedRoot_Root_CA_RSA_TLS_2021.pem\n",
      "Adding debian:CommScope_Public_Trust_RSA_Root-02.pem\n",
      "Adding debian:BJCA_Global_Root_CA2.pem\n",
      "Adding debian:Atos_TrustedRoot_Root_CA_ECC_TLS_2021.pem\n",
      "Adding debian:CommScope_Public_Trust_RSA_Root-01.pem\n",
      "Adding debian:TrustAsia_Global_Root_CA_G3.pem\n",
      "Adding debian:BJCA_Global_Root_CA1.pem\n",
      "Adding debian:CommScope_Public_Trust_ECC_Root-02.pem\n",
      "Adding debian:CommScope_Public_Trust_ECC_Root-01.pem\n",
      "Adding debian:SSL.com_TLS_ECC_Root_CA_2022.pem\n",
      "Adding debian:TrustAsia_Global_Root_CA_G4.pem\n",
      "Adding debian:SSL.com_TLS_RSA_Root_CA_2022.pem\n",
      "Adding debian:Sectigo_Public_Server_Authentication_Root_E46.pem\n",
      "Adding debian:Sectigo_Public_Server_Authentication_Root_R46.pem\n",
      "done.\n",
      "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
      "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for ca-certificates (20240203~22.04.1) ...\n",
      "Updating certificates in /etc/ssl/certs...\n",
      "0 added, 0 removed; done.\n",
      "Running hooks in /etc/ca-certificates/update.d...\n",
      "\n",
      "done.\n",
      "done.\n",
      "Collecting pyspark==3.5.0\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.0) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425346 sha256=cb328b5869f71b555f4979e8d76cabfc01ed466e656a2309813799dc49acc18d\n",
      "  Stored in directory: /root/.cache/pip/wheels/84/40/20/65eefe766118e0a8f8e385cc3ed6e9eb7241c7e51cfc04c51a\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.1\n",
      "    Uninstalling pyspark-3.5.1:\n",
      "      Successfully uninstalled pyspark-3.5.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyspark-3.5.0\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas==2.2.2\n",
      "  Downloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas==2.2.2)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas==2.2.2)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.2)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.2.2)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.2\n",
      "    Uninstalling pytz-2025.2:\n",
      "      Successfully uninstalled pytz-2025.2\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.2\n",
      "    Uninstalling tzdata-2025.2:\n",
      "      Successfully uninstalled tzdata-2025.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4 pandas-2.2.2 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "e886d08bde104ffc98c916076837ccfa",
       "pip_warning": {
        "packages": [
         "dateutil",
         "numpy",
         "pandas",
         "pytz",
         "six"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 1. Install JAVA - SPARK\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "!apt-get install openjdk-11-jdk -y\n",
    "!pip install pyspark==3.5.0\n",
    "!pip install findspark\n",
    "!pip install --force-reinstall numpy==1.26.4 pandas==2.2.2\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] += \":/usr/lib/jvm/java-11-openjdk-amd64/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "CQzCF3tfcEEe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "CQzCF3tfcEEe",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "741f3b0c-4686-4110-a3e8-7ce826534ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.1)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.10.1 optuna-4.5.0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 3. Install packages\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZovIb_yeanM4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ac6f19a251504c058a800a00ec6d77ad",
      "1440833df957444e87fb55a5d0f8946c",
      "2804a711f210402b999ef629d8850359",
      "57c320197d56491ba9c1534577c38897",
      "98141a5683b14cb0927f12938365ca29",
      "9f3646de23024d488dba29e2f2a9ca53",
      "c7e53e87e7c54aafb839600022ae61e3",
      "f601d3b612c54f1880a7a35dbf0fdacc",
      "a797ef5a1b634fc98a5b5156e5cb91a2",
      "80394416cf8e44af946bbc4c8b30a41d",
      "740f0f1049e74ee7b12a4822ffebdb7d"
     ]
    },
    "id": "ZovIb_yeanM4",
    "outputId": "1d9dc5de-c4a8-4ed3-a375-b3107eca17b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab cache for faster access to the 'transactions-fraud-datasets' dataset.\n",
      "Transactions rows after date filter: 6734248\n",
      "Monthly aggregated rows (after features): 606168\n",
      "train_agg rows: 501432 holdout_agg rows: 104736\n",
      "Converted to pandas: train rows 501432 holdout rows 104736\n",
      "Shapes -> X_train: (501432, 30) X_hold: (104736, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-08 13:59:29,735] A new study created in memory with name: no-name-26589be7-f5e2-4d6a-bc15-0176ac425a33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal folds for Optuna: 3 folds\n",
      "Starting Optuna (walk-forward inside train 2015-2018). This can take time.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6f19a251504c058a800a00ec6d77ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-08 14:02:23,920] Trial 0 finished with value: 0.8156930441997612 and parameters: {'learning_rate': 0.010670343423001872, 'num_leaves': 143, 'max_depth': 11, 'min_child_samples': 80, 'subsample': 0.9972289398518563, 'colsample_bytree': 0.7533672513887649, 'lambda_l1': 11.173736497615776, 'lambda_l2': 18.801960595347726}. Best is trial 0 with value: 0.8156930441997612.\n",
      "[I 2025-11-08 14:03:21,166] Trial 1 finished with value: 0.8149869650381937 and parameters: {'learning_rate': 0.10219220940582037, 'num_leaves': 102, 'max_depth': 9, 'min_child_samples': 53, 'subsample': 0.7148540795541638, 'colsample_bytree': 0.7997209104862582, 'lambda_l1': 19.993447094145644, 'lambda_l2': 2.798962696746632}. Best is trial 1 with value: 0.8149869650381937.\n",
      "[I 2025-11-08 14:05:17,781] Trial 2 finished with value: 0.8149422634836387 and parameters: {'learning_rate': 0.03580348400437088, 'num_leaves': 210, 'max_depth': 15, 'min_child_samples': 119, 'subsample': 0.930725750259833, 'colsample_bytree': 0.4380274325128549, 'lambda_l1': 5.913524407873238, 'lambda_l2': 6.904314573338517}. Best is trial 2 with value: 0.8149422634836387.\n",
      "[I 2025-11-08 14:06:35,667] Trial 3 finished with value: 0.8143337230187169 and parameters: {'learning_rate': 0.055908068449650745, 'num_leaves': 181, 'max_depth': 15, 'min_child_samples': 66, 'subsample': 0.47736114046789535, 'colsample_bytree': 0.8311165173652684, 'lambda_l1': 10.204666623411502, 'lambda_l2': 9.670231607119792}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:08:04,766] Trial 4 finished with value: 0.8216278175033157 and parameters: {'learning_rate': 0.009756129370590968, 'num_leaves': 105, 'max_depth': 5, 'min_child_samples': 23, 'subsample': 0.8725958891207519, 'colsample_bytree': 0.6447965005420913, 'lambda_l1': 10.888965427075512, 'lambda_l2': 19.269703568718263}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:09:11,870] Trial 5 finished with value: 0.8166394395100275 and parameters: {'learning_rate': 0.09758136910928251, 'num_leaves': 60, 'max_depth': 5, 'min_child_samples': 13, 'subsample': 0.6048900954498513, 'colsample_bytree': 0.7163760821657805, 'lambda_l1': 16.052959651848777, 'lambda_l2': 0.7747344380197974}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:11:25,266] Trial 6 finished with value: 0.8159918131638948 and parameters: {'learning_rate': 0.01600738221968888, 'num_leaves': 162, 'max_depth': 7, 'min_child_samples': 53, 'subsample': 0.47009121003669085, 'colsample_bytree': 0.8886927799388797, 'lambda_l1': 0.4818230384654054, 'lambda_l2': 11.760340735954935}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:13:26,518] Trial 7 finished with value: 0.8210529527701196 and parameters: {'learning_rate': 0.0064933861237727215, 'num_leaves': 214, 'max_depth': 6, 'min_child_samples': 7, 'subsample': 0.5032241534942612, 'colsample_bytree': 0.8433004284807661, 'lambda_l1': 18.072035863548, 'lambda_l2': 8.206959576256134}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:14:48,939] Trial 8 finished with value: 0.8224253916756528 and parameters: {'learning_rate': 0.014927588932486741, 'num_leaves': 229, 'max_depth': 4, 'min_child_samples': 37, 'subsample': 0.752779696454632, 'colsample_bytree': 0.5890811404675148, 'lambda_l1': 6.938413082668009, 'lambda_l2': 2.7970957081459336}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:16:09,323] Trial 9 finished with value: 0.8168869079717963 and parameters: {'learning_rate': 0.05158622304212649, 'num_leaves': 93, 'max_depth': 5, 'min_child_samples': 69, 'subsample': 0.5135787463291281, 'colsample_bytree': 0.9187326064136481, 'lambda_l1': 13.19443704884475, 'lambda_l2': 12.26553436043803}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:16:57,171] Trial 10 finished with value: 0.8191214652287382 and parameters: {'learning_rate': 0.1497072888298948, 'num_leaves': 21, 'max_depth': 16, 'min_child_samples': 102, 'subsample': 0.3821307509885886, 'colsample_bytree': 0.3218699300607679, 'lambda_l1': 6.047041169588558, 'lambda_l2': 15.494141407946069}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:19:03,286] Trial 11 finished with value: 0.8149797278797042 and parameters: {'learning_rate': 0.0369325436145289, 'num_leaves': 193, 'max_depth': 16, 'min_child_samples': 103, 'subsample': 0.3258822307724745, 'colsample_bytree': 0.41694284100051726, 'lambda_l1': 5.680764850347031, 'lambda_l2': 6.902854218069329}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:20:51,070] Trial 12 finished with value: 0.8149699308755455 and parameters: {'learning_rate': 0.04295069257819051, 'num_leaves': 245, 'max_depth': 13, 'min_child_samples': 117, 'subsample': 0.9930868923502204, 'colsample_bytree': 0.4985170155260645, 'lambda_l1': 1.9014828969993918, 'lambda_l2': 6.397644385740053}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:23:06,022] Trial 13 finished with value: 0.8144720397983951 and parameters: {'learning_rate': 0.025538256919086882, 'num_leaves': 173, 'max_depth': 13, 'min_child_samples': 87, 'subsample': 0.8140816570788153, 'colsample_bytree': 0.5291791975849497, 'lambda_l1': 8.025128057106691, 'lambda_l2': 10.02888719132519}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:25:30,949] Trial 14 finished with value: 0.8149445198885833 and parameters: {'learning_rate': 0.022828991802064904, 'num_leaves': 170, 'max_depth': 13, 'min_child_samples': 85, 'subsample': 0.8169324544664571, 'colsample_bytree': 0.9995022680920486, 'lambda_l1': 8.870470324340191, 'lambda_l2': 10.644051211494196}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:27:00,456] Trial 15 finished with value: 0.8150830643575411 and parameters: {'learning_rate': 0.06434197546583874, 'num_leaves': 180, 'max_depth': 13, 'min_child_samples': 85, 'subsample': 0.637916893208013, 'colsample_bytree': 0.544738835390649, 'lambda_l1': 13.754428649470759, 'lambda_l2': 15.697970179285782}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:29:07,885] Trial 16 finished with value: 0.8143784160166619 and parameters: {'learning_rate': 0.02390446217776206, 'num_leaves': 135, 'max_depth': 11, 'min_child_samples': 65, 'subsample': 0.5765015676840041, 'colsample_bytree': 0.6614659412291966, 'lambda_l1': 3.4688127578824357, 'lambda_l2': 14.019913452215564}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:30:19,680] Trial 17 finished with value: 0.8148472287125457 and parameters: {'learning_rate': 0.05885257034675632, 'num_leaves': 134, 'max_depth': 10, 'min_child_samples': 65, 'subsample': 0.5667594360275297, 'colsample_bytree': 0.688486851731982, 'lambda_l1': 2.731963276109443, 'lambda_l2': 14.183279352651148}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:32:33,240] Trial 18 finished with value: 0.8151921599754021 and parameters: {'learning_rate': 0.020106587206859983, 'num_leaves': 131, 'max_depth': 8, 'min_child_samples': 45, 'subsample': 0.42667567629082154, 'colsample_bytree': 0.6246584674243093, 'lambda_l1': 3.619893524413124, 'lambda_l2': 13.53221159716273}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:33:37,801] Trial 19 finished with value: 0.8144935996851762 and parameters: {'learning_rate': 0.08175044832186645, 'num_leaves': 63, 'max_depth': 11, 'min_child_samples': 28, 'subsample': 0.6857127537146042, 'colsample_bytree': 0.7767776193596937, 'lambda_l1': 13.126297710299516, 'lambda_l2': 16.562602374640438}. Best is trial 3 with value: 0.8143337230187169.\n",
      "[I 2025-11-08 14:35:25,713] Trial 20 finished with value: 0.8142656559491609 and parameters: {'learning_rate': 0.03022991381024461, 'num_leaves': 150, 'max_depth': 14, 'min_child_samples': 73, 'subsample': 0.5537698126785044, 'colsample_bytree': 0.905353916722271, 'lambda_l1': 4.2824214219755685, 'lambda_l2': 8.741297092916003}. Best is trial 20 with value: 0.8142656559491609.\n",
      "[I 2025-11-08 14:37:17,634] Trial 21 finished with value: 0.8142997428708041 and parameters: {'learning_rate': 0.031705217984716184, 'num_leaves': 147, 'max_depth': 15, 'min_child_samples': 72, 'subsample': 0.5533129928007693, 'colsample_bytree': 0.9452132490870174, 'lambda_l1': 3.950035001048423, 'lambda_l2': 8.674491055939939}. Best is trial 20 with value: 0.8142656559491609.\n",
      "[I 2025-11-08 14:38:58,951] Trial 22 finished with value: 0.8145291570852575 and parameters: {'learning_rate': 0.03359974005613975, 'num_leaves': 152, 'max_depth': 15, 'min_child_samples': 75, 'subsample': 0.40032094953071595, 'colsample_bytree': 0.9445362487767439, 'lambda_l1': 9.363470347307466, 'lambda_l2': 8.732426481764833}. Best is trial 20 with value: 0.8142656559491609.\n",
      "[I 2025-11-08 14:40:21,772] Trial 23 finished with value: 0.8154088778122578 and parameters: {'learning_rate': 0.049944706698374225, 'num_leaves': 192, 'max_depth': 14, 'min_child_samples': 98, 'subsample': 0.5534518706295016, 'colsample_bytree': 0.9839478056434623, 'lambda_l1': 4.493973138355783, 'lambda_l2': 5.036615477331573}. Best is trial 20 with value: 0.8142656559491609.\n",
      "[I 2025-11-08 14:42:09,380] Trial 24 finished with value: 0.8139494555002184 and parameters: {'learning_rate': 0.02991459924148026, 'num_leaves': 116, 'max_depth': 15, 'min_child_samples': 54, 'subsample': 0.4557517584918903, 'colsample_bytree': 0.8442098000660674, 'lambda_l1': 0.6709695237764031, 'lambda_l2': 9.079218622350625}. Best is trial 24 with value: 0.8139494555002184.\n",
      "Best params from Optuna: {'learning_rate': 0.02991459924148026, 'num_leaves': 116, 'max_depth': 15, 'min_child_samples': 54, 'subsample': 0.4557517584918903, 'colsample_bytree': 0.8442098000660674, 'lambda_l1': 0.6709695237764031, 'lambda_l2': 9.079218622350625}\n",
      "Training final LightGBM on 2015-2018 ...\n",
      "MAIN on 2019 -> RMSE: 0.8475, MAE: 0.6360, R2: 0.6552\n",
      "Evaluating ARIMA per merchant on holdout ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ARIMA evaluation: 100%|██████████| 27516/27516 [36:14<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA applied for 11523 merchants out of 27516\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "MAIN (LightGBM only) -> RMSE=0.8475, MAE=0.6360, R2=0.6552\n",
      "HYBRID (ARIMA-corrected) -> RMSE=0.8184, MAE=0.5975, R2=0.6786\n",
      "Improvement: 3.44%\n",
      "\n",
      " Exported holdout_2019_results.csv\n",
      "\n",
      "Top 15 Feature Importances:\n",
      "                  feature    importance\n",
      "21  log_amount_sum_roll12  8.676559e+06\n",
      "9     log_amount_sum_lag1  1.354505e+06\n",
      "10    log_amount_sum_lag2  3.365013e+05\n",
      "18      amount_avg_roll12  2.604593e+05\n",
      "3         amount_avg_lag1  1.888617e+05\n",
      "25     avg_client_mean_tx  8.733279e+04\n",
      "11    log_amount_sum_lag3  8.509978e+04\n",
      "19        tx_count_roll12  7.822535e+04\n",
      "14       amount_avg_roll3  4.632670e+04\n",
      "27    avg_client_tx_count  4.460330e+04\n",
      "26      avg_client_std_tx  2.234111e+04\n",
      "24              year_norm  2.122469e+04\n",
      "28      amount_growth_1_2  2.111069e+04\n",
      "12   log_amount_sum_lag12  2.078792e+04\n",
      "2           tx_count_lag3  2.020951e+04\n",
      " Exported feature_importance.csv\n",
      "\n",
      "============================================================\n",
      "PRODUCTION: GENERATING PREDICTIONS FOR NOV 2019 - OCT 2020\n",
      "============================================================\n",
      "\n",
      "[Step 1/5] Training final model on all data (2015-2019)...\n",
      "Full training set rows: 606,168\n",
      "Converted to pandas: 606,168 rows\n",
      "Training final LightGBM model...\n",
      "Model trained successfully!\n",
      "\n",
      "[Step 2/5] Building ARIMA residuals on full history (2015-2019)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building residual series: 100%|██████████| 56395/56395 [02:13<00:00, 423.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built residual series for 56,395 merchants\n",
      "\n",
      "[Step 3/5] Training ARIMA models per merchant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ARIMA:  81%|████████  | 45679/56395 [2:28:47<34:54,  5.12it/s]    \n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Spark -> Aggregation -> Pandas -> LGBM + Optuna + ARIMA\n",
    "# Train: 2015-01 .. 2018-12\n",
    "# Test (holdout): 2019-01 .. 2019-12\n",
    "# ===============================\n",
    "\n",
    "import math, json, warnings, time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_date, regexp_replace, unix_timestamp, dayofweek, month, year, sin, cos, lit,\n",
    "    sum as _sum, avg as _avg, count as _count, stddev as _stddev, countDistinct,\n",
    "    when, expr, max as _max, min as _min, lag\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# -------------------------\n",
    "# 0) Spark init & data load\n",
    "# -------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"agg_to_pandas_lgbm_arima_fixed\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --- load data (adapt paths to your environment) ---\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"computingvictor/transactions-fraud-datasets\")\n",
    "transaction_data = f\"{path}/transactions_data.csv\"\n",
    "mcc_codes_data = f\"{path}/mcc_codes.json\"\n",
    "\n",
    "with open(mcc_codes_data, \"r\") as f:\n",
    "    mcc_dict = json.load(f)\n",
    "mcc = pd.DataFrame.from_dict(mcc_dict, orient=\"index\", columns=[\"description\"]).reset_index().rename(columns={\"index\":\"mcc_code\"})\n",
    "mcc_spark = spark.createDataFrame(mcc)\n",
    "\n",
    "td_spark = spark.read.csv(transaction_data, header=True, inferSchema=True)\n",
    "\n",
    "# -------------------------\n",
    "# 1) Transaction-level FE (Spark)\n",
    "# -------------------------\n",
    "LABEL_COL = \"amount\"\n",
    "td_spark = td_spark.withColumn(LABEL_COL, regexp_replace(col(LABEL_COL), \"\\\\$\", \"\"))\n",
    "td_spark = td_spark.withColumn(LABEL_COL, col(LABEL_COL).cast(DoubleType()))\n",
    "td_spark = td_spark.withColumn(\"date\", to_date(col(\"date\")))\n",
    "\n",
    "df = td_spark.join(mcc_spark, td_spark.mcc == mcc_spark.mcc_code, \"left\")\n",
    "\n",
    "cols_to_drop = [\"id\", \"use_chip\", \"merchant_city\", \"merchant_state\", \"zip\", \"mcc\", \"errors\", \"mcc_code\"]\n",
    "for c in cols_to_drop:\n",
    "    if c in df.columns:\n",
    "        df = df.drop(c)\n",
    "\n",
    "# add year/month/ym at transaction level\n",
    "df = df.withColumn(\"year\", year(col(\"date\")).cast(IntegerType()))\n",
    "df = df.withColumn(\"month\", month(col(\"date\")).cast(IntegerType()))\n",
    "df = df.withColumn(\"ym\", (col(\"year\") * 100 + col(\"month\")).cast(IntegerType()))\n",
    "\n",
    "df = df.withColumn(\"date_timestamp\", unix_timestamp(col(\"date\")).cast(DoubleType()))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"date\")).cast(IntegerType()))\n",
    "df = df.withColumn(\"is_refund\", when(col(LABEL_COL) < 0, 1.0).otherwise(0.0).cast(DoubleType()))\n",
    "df = df.withColumn(\"description_clean\", regexp_replace(col(\"description\"), \"[ ,:\\\\[\\\\]\\\\{\\\\}]\", \"_\"))\n",
    "\n",
    "# transaction-level log target for client encoding\n",
    "df = df.withColumn(\"log_amount_sum_tx\", when(col(LABEL_COL) >= 0, col(LABEL_COL)).otherwise(0.0))\n",
    "df = df.withColumn(\"log_amount_sum_tx\", expr(\"log(1 + log_amount_sum_tx)\"))\n",
    "\n",
    "# filter to relevant period (post-break)\n",
    "df = df.filter((col(\"date\") >= \"2015-01-01\") & (col(\"date\") <= \"2019-12-31\")).cache()\n",
    "print(\"Transactions rows after date filter:\", df.count())\n",
    "\n",
    "# -------------------------\n",
    "# 2) Monthly aggregation per merchant (Spark)\n",
    "# -------------------------\n",
    "group_cols = [\"merchant_id\", \"year\", \"month\"]\n",
    "\n",
    "agg = df.groupBy(*group_cols).agg(\n",
    "    _sum(LABEL_COL).alias(\"amount_sum\"),\n",
    "    _count(\"*\").alias(\"tx_count\"),\n",
    "    _avg(LABEL_COL).alias(\"amount_avg\"),\n",
    "    _stddev(LABEL_COL).alias(\"amount_stddev\"),\n",
    "    _avg(\"is_refund\").alias(\"refund_rate\"),\n",
    "    countDistinct(\"description_clean\").alias(\"desc_count\")\n",
    ")\n",
    "\n",
    "agg = agg.fillna({\"amount_stddev\": 0.0})\n",
    "agg = agg.withColumn(\"ym\", (col(\"year\") * 100 + col(\"month\")).cast(IntegerType()))\n",
    "\n",
    "# cyclic temporal features\n",
    "from math import pi\n",
    "agg = agg.withColumn(\"month_sin\", sin(2 * pi * col(\"month\") / lit(12)))\n",
    "agg = agg.withColumn(\"month_cos\", cos(2 * pi * col(\"month\") / lit(12)))\n",
    "\n",
    "min_year = agg.agg({\"year\":\"min\"}).collect()[0][0]\n",
    "agg = agg.withColumn(\"year_norm\", (col(\"year\") - lit(min_year)).cast(DoubleType()))\n",
    "\n",
    "# aggregated log target\n",
    "agg = agg.withColumn(\"log_amount_sum\", when(col(\"amount_sum\") >= 0, col(\"amount_sum\")).otherwise(0.0))\n",
    "agg = agg.withColumn(\"log_amount_sum\", expr(\"log(1 + log_amount_sum)\"))\n",
    "\n",
    "# -------------------------\n",
    "# 3) Lags / rollings / trend in Spark (monthly per merchant)\n",
    "# -------------------------\n",
    "window_spec = Window.partitionBy(\"merchant_id\").orderBy(\"ym\")\n",
    "\n",
    "# Lags\n",
    "agg = agg.withColumn(\"tx_count_lag1\", lag(\"tx_count\", 1).over(window_spec))\n",
    "agg = agg.withColumn(\"tx_count_lag2\", lag(\"tx_count\", 2).over(window_spec))\n",
    "agg = agg.withColumn(\"tx_count_lag3\", lag(\"tx_count\", 3).over(window_spec))\n",
    "\n",
    "agg = agg.withColumn(\"amount_avg_lag1\", lag(\"amount_avg\", 1).over(window_spec))\n",
    "agg = agg.withColumn(\"amount_avg_lag2\", lag(\"amount_avg\", 2).over(window_spec))\n",
    "agg = agg.withColumn(\"amount_avg_lag3\", lag(\"amount_avg\", 3).over(window_spec))\n",
    "\n",
    "agg = agg.withColumn(\"amount_stddev_lag1\", lag(\"amount_stddev\", 1).over(window_spec))\n",
    "agg = agg.withColumn(\"refund_rate_lag1\", lag(\"refund_rate\", 1).over(window_spec))\n",
    "agg = agg.withColumn(\"desc_count_lag1\", lag(\"desc_count\", 1).over(window_spec))\n",
    "\n",
    "agg = agg.withColumn(\"log_amount_sum_lag1\", lag(\"log_amount_sum\", 1).over(window_spec))\n",
    "agg = agg.withColumn(\"log_amount_sum_lag2\", lag(\"log_amount_sum\", 2).over(window_spec))\n",
    "agg = agg.withColumn(\"log_amount_sum_lag3\", lag(\"log_amount_sum\", 3).over(window_spec))\n",
    "agg = agg.withColumn(\"log_amount_sum_lag12\", lag(\"log_amount_sum\", 12).over(window_spec))\n",
    "agg = agg.withColumn(\"tx_count_lag12\", lag(\"tx_count\", 12).over(window_spec))\n",
    "\n",
    "# rolling windows (exclude current)\n",
    "window_roll3 = Window.partitionBy(\"merchant_id\").orderBy(\"ym\").rowsBetween(-3, -1)\n",
    "agg = agg.withColumn(\"amount_avg_roll3\", _avg(\"amount_avg\").over(window_roll3))\n",
    "agg = agg.withColumn(\"tx_count_roll3\", _avg(\"tx_count\").over(window_roll3))\n",
    "\n",
    "window_roll6 = Window.partitionBy(\"merchant_id\").orderBy(\"ym\").rowsBetween(-6, -1)\n",
    "agg = agg.withColumn(\"amount_avg_roll6\", _avg(\"amount_avg\").over(window_roll6))\n",
    "agg = agg.withColumn(\"tx_count_roll6\", _avg(\"tx_count\").over(window_roll6))\n",
    "\n",
    "window_roll12 = Window.partitionBy(\"merchant_id\").orderBy(\"ym\").rowsBetween(-12, -1)\n",
    "agg = agg.withColumn(\"amount_avg_roll12\", _avg(\"amount_avg\").over(window_roll12))\n",
    "agg = agg.withColumn(\"tx_count_roll12\", _avg(\"tx_count\").over(window_roll12))\n",
    "\n",
    "agg = agg.withColumn(\"log_amount_sum_roll12\", _avg(\"log_amount_sum\").over(window_roll12))\n",
    "\n",
    "# trend and ratios\n",
    "agg = agg.withColumn(\"amount_trend\",\n",
    "    when(col(\"amount_avg_lag1\").isNotNull(),\n",
    "         col(\"amount_avg_lag1\") - lag(\"amount_avg\", 2).over(window_spec)).otherwise(0.0))\n",
    "\n",
    "agg = agg.withColumn(\"amount_growth_1_2\", (col(\"amount_avg_lag1\") + lit(1e-6)) / (col(\"amount_avg_lag2\") + lit(1e-6)))\n",
    "agg = agg.withColumn(\"amount_diff_1_2\", col(\"amount_avg_lag1\") - col(\"amount_avg_lag2\"))\n",
    "\n",
    "# fillna for lag/roll features\n",
    "lag_cols = [c for c in agg.columns if any(k in c for k in (\"lag\",\"roll\",\"trend\",\"growth\",\"diff\"))]\n",
    "fill_map = {c: 0.0 for c in lag_cols}\n",
    "agg = agg.fillna(fill_map)\n",
    "\n",
    "agg = agg.cache()\n",
    "print(\"Monthly aggregated rows (after features):\", agg.count())\n",
    "\n",
    "# -------------------------\n",
    "# 4) DEFINE SPLITS AND CLIENT AGGREGATES (no leakage)\n",
    "# -------------------------\n",
    "train_ym_max = 2018*100 + 12\n",
    "holdout_ym_min = 2019*100 + 1\n",
    "holdout_ym_max = 2019*100 + 12\n",
    "\n",
    "train_agg = agg.filter(col(\"ym\") <= train_ym_max).cache()\n",
    "holdout_agg = agg.filter((col(\"ym\") >= holdout_ym_min) & (col(\"ym\") <= holdout_ym_max)).cache()\n",
    "\n",
    "print(\"train_agg rows:\", train_agg.count(), \"holdout_agg rows:\", holdout_agg.count())\n",
    "\n",
    "# client aggregates calculated from transactions up to train_date_max (to avoid leakage)\n",
    "train_date_max = \"2018-12-31\"\n",
    "train_transactions_for_client = df.filter(col(\"date\") <= lit(train_date_max)).cache()\n",
    "\n",
    "client_stats = train_transactions_for_client.groupBy(\"client_id\").agg(\n",
    "    _avg(\"log_amount_sum_tx\").alias(\"client_mean_tx\"),\n",
    "    _stddev(\"log_amount_sum_tx\").alias(\"client_std_tx\"),\n",
    "    _count(\"*\").alias(\"client_tx_count\")\n",
    ").fillna({\"client_std_tx\": 0.0})\n",
    "\n",
    "tx_with_client = train_transactions_for_client.join(client_stats, \"client_id\", \"left\")\n",
    "client_agg_df = tx_with_client.groupBy(\"merchant_id\",\"ym\").agg(\n",
    "    _avg(\"client_mean_tx\").alias(\"avg_client_mean_tx\"),\n",
    "    _avg(\"client_std_tx\").alias(\"avg_client_std_tx\"),\n",
    "    _avg(\"client_tx_count\").alias(\"avg_client_tx_count\")\n",
    ")\n",
    "\n",
    "# join to train and holdout\n",
    "train_joined = train_agg.join(client_agg_df, [\"merchant_id\",\"ym\"], \"left\").fillna({\n",
    "    \"avg_client_mean_tx\":0.0,\"avg_client_std_tx\":0.0,\"avg_client_tx_count\":0.0\n",
    "})\n",
    "holdout_joined = holdout_agg.join(client_agg_df, [\"merchant_id\",\"ym\"], \"left\").fillna({\n",
    "    \"avg_client_mean_tx\":0.0,\"avg_client_std_tx\":0.0,\"avg_client_tx_count\":0.0\n",
    "})\n",
    "\n",
    "# -------------------------\n",
    "# 5) Convert aggregated tables to Pandas\n",
    "# -------------------------\n",
    "train_pd = train_joined.toPandas().fillna(0.0).reset_index(drop=True)\n",
    "holdout_pd = holdout_joined.toPandas().fillna(0.0).reset_index(drop=True)\n",
    "\n",
    "print(\"Converted to pandas: train rows\", len(train_pd), \"holdout rows\", len(holdout_pd))\n",
    "\n",
    "# -------------------------\n",
    "# 6) Feature list (ensure existence)\n",
    "# -------------------------\n",
    "feature_cols = [\n",
    "    \"tx_count_lag1\",\"tx_count_lag2\",\"tx_count_lag3\",\n",
    "    \"amount_avg_lag1\",\"amount_avg_lag2\",\"amount_avg_lag3\",\n",
    "    \"amount_stddev_lag1\",\"refund_rate_lag1\",\"desc_count_lag1\",\n",
    "    \"log_amount_sum_lag1\",\"log_amount_sum_lag2\",\"log_amount_sum_lag3\",\n",
    "    \"log_amount_sum_lag12\",\"tx_count_lag12\",\n",
    "    \"amount_avg_roll3\",\"tx_count_roll3\",\"amount_avg_roll6\",\"tx_count_roll6\",\n",
    "    \"amount_avg_roll12\",\"tx_count_roll12\",\n",
    "    \"amount_trend\",\"log_amount_sum_roll12\",\n",
    "    \"month_sin\",\"month_cos\",\"year_norm\",\n",
    "    \"avg_client_mean_tx\",\"avg_client_std_tx\",\"avg_client_tx_count\",\n",
    "    \"amount_growth_1_2\",\"amount_diff_1_2\"\n",
    "]\n",
    "\n",
    "existing_features = [c for c in feature_cols if c in train_pd.columns and c in holdout_pd.columns]\n",
    "missing = set(feature_cols) - set(existing_features)\n",
    "if missing:\n",
    "    print(\"Warning: some features are missing and will be ignored:\", missing)\n",
    "\n",
    "X_train = train_pd[existing_features].values.astype(np.float32)\n",
    "y_train = train_pd[\"log_amount_sum\"].values.astype(np.float32)\n",
    "X_hold = holdout_pd[existing_features].values.astype(np.float32)\n",
    "y_hold = holdout_pd[\"log_amount_sum\"].values.astype(np.float32)\n",
    "\n",
    "print(\"Shapes -> X_train:\", X_train.shape, \"X_hold:\", X_hold.shape)\n",
    "\n",
    "# -------------------------\n",
    "# 7) Optuna hyperparam tuning (walk-forward internal folds inside train 2015-2018)\n",
    "# -------------------------\n",
    "train_years = sorted(train_pd[\"year\"].unique())\n",
    "internal_folds = []\n",
    "for i in range(len(train_years)-1):\n",
    "    train_year_list = train_years[:i+1]\n",
    "    val_year = train_years[i+1]\n",
    "    tr_df = train_pd[train_pd[\"year\"].isin(train_year_list)].reset_index(drop=True)\n",
    "    val_df = train_pd[train_pd[\"year\"] == val_year].reset_index(drop=True)\n",
    "    if len(tr_df) >= 50 and len(val_df) >= 10:\n",
    "        internal_folds.append((tr_df, val_df))\n",
    "\n",
    "print(f\"Internal folds for Optuna: {len(internal_folds)} folds\")\n",
    "\n",
    "def evaluate_params_on_internal(params):\n",
    "    rmses = []\n",
    "    for tr_df, val_df in internal_folds:\n",
    "        Xtr = tr_df[existing_features].values.astype(np.float32)\n",
    "        ytr = tr_df[\"log_amount_sum\"].values.astype(np.float32)\n",
    "        Xv = val_df[existing_features].values.astype(np.float32)\n",
    "        yv = val_df[\"log_amount_sum\"].values.astype(np.float32)\n",
    "\n",
    "        dtrain = lgb.Dataset(Xtr, label=ytr)\n",
    "        dval = lgb.Dataset(Xv, label=yv, reference=dtrain)\n",
    "        params_local = params.copy()\n",
    "        params_local[\"verbosity\"] = -1\n",
    "\n",
    "        model = lgb.train(\n",
    "            params_local,\n",
    "            dtrain,\n",
    "            valid_sets=[dval],\n",
    "            num_boost_round=800,\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=60, verbose=False)]\n",
    "        )\n",
    "        preds = model.predict(Xv, num_iteration=model.best_iteration)\n",
    "        rmses.append(math.sqrt(mean_squared_error(yv, preds)))\n",
    "    return float(np.mean(rmses)) if len(rmses)>0 else float(\"inf\")\n",
    "\n",
    "def optuna_objective(trial):\n",
    "    params = {\n",
    "        \"objective\":\"regression\",\n",
    "        \"metric\":\"rmse\",\n",
    "        \"boosting_type\":\"gbdt\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.15, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 256),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 16),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 120),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 20.0),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 20.0),\n",
    "    }\n",
    "    return evaluate_params_on_internal(params)\n",
    "\n",
    "print(\"Starting Optuna (walk-forward inside train 2015-2018). This can take time.\")\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(optuna_objective, n_trials=25, show_progress_bar=True)\n",
    "best_params = study.best_params\n",
    "print(\"Best params from Optuna:\", best_params)\n",
    "\n",
    "final_lgb_params = {\n",
    "    \"objective\":\"regression\",\n",
    "    \"metric\":\"rmse\",\n",
    "    \"boosting_type\":\"gbdt\",\n",
    "    \"verbosity\":-1,\n",
    "    **best_params\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 8) Train final LightGBM on full train (2015-2018)\n",
    "# -------------------------\n",
    "dtrain_full = lgb.Dataset(X_train, label=y_train)\n",
    "dhold = lgb.Dataset(X_hold, label=y_hold, reference=dtrain_full)\n",
    "\n",
    "print(\"Training final LightGBM on 2015-2018 ...\")\n",
    "final_model = lgb.train(final_lgb_params, dtrain_full, valid_sets=[dhold],\n",
    "                        num_boost_round=1200, callbacks=[lgb.early_stopping(stopping_rounds=80, verbose=False)])\n",
    "\n",
    "# Holdout predictions (main)\n",
    "pred_hold_main = final_model.predict(X_hold, num_iteration=final_model.best_iteration)\n",
    "rmse_main = math.sqrt(mean_squared_error(y_hold, pred_hold_main))\n",
    "mae_main = mean_absolute_error(y_hold, pred_hold_main)\n",
    "r2_main = r2_score(y_hold, pred_hold_main)\n",
    "print(f\"MAIN on 2019 -> RMSE: {rmse_main:.4f}, MAE: {mae_main:.4f}, R2: {r2_main:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 9) Build residuals on training history (2015-2018) for ARIMA candidates\n",
    "# -------------------------\n",
    "train_pd = train_pd.reset_index(drop=True)\n",
    "train_pd[\"pred_main_train\"] = final_model.predict(train_pd[existing_features].values.astype(np.float32),\n",
    "                                                  num_iteration=final_model.best_iteration)\n",
    "train_pd[\"residual_train\"] = train_pd[\"log_amount_sum\"].values - train_pd[\"pred_main_train\"].values\n",
    "\n",
    "# Build monthly residual time series per merchant\n",
    "merchant_hist = {}\n",
    "for mid, g in train_pd.groupby(\"merchant_id\"):\n",
    "    g_sorted = g.sort_values([\"year\", \"month\"]).reset_index(drop=True)\n",
    "    idx = pd.to_datetime(\n",
    "        g_sorted[\"year\"].astype(int).astype(str) +\n",
    "        g_sorted[\"month\"].astype(int).astype(str).str.zfill(2) + \"01\",\n",
    "        format=\"%Y%m%d\"\n",
    "    )\n",
    "    s = pd.Series(g_sorted[\"residual_train\"].values, index=idx)\n",
    "    full_idx = pd.date_range(start=s.index.min(), end=s.index.max(), freq=\"MS\")\n",
    "    s = s.reindex(full_idx).fillna(0.0)\n",
    "    merchant_hist[mid] = s\n",
    "\n",
    "# -------------------------\n",
    "# 10) Prepare holdout set properly with merchant_id in index\n",
    "# -------------------------\n",
    "holdout_pd = holdout_pd.reset_index(drop=True)\n",
    "\n",
    "holdout_pd[\"ym_dt\"] = pd.to_datetime(\n",
    "    holdout_pd[\"year\"].astype(int).astype(str) +\n",
    "    holdout_pd[\"month\"].astype(int).astype(str).str.zfill(2) + \"01\",\n",
    "    format=\"%Y%m%d\"\n",
    ")\n",
    "holdout_pd[\"y_true\"] = y_hold\n",
    "holdout_pd[\"pred_main\"] = pred_hold_main\n",
    "\n",
    "# -------------------------\n",
    "# 11) Evaluate ARIMA correction per merchant - CORRECTION ICI\n",
    "# -------------------------\n",
    "min_hist_periods = 18\n",
    "arima_order = (1, 0, 1)\n",
    "merchant_use_arima = {}\n",
    "merchant_models = {}\n",
    "\n",
    "print(\"Evaluating ARIMA per merchant on holdout ...\")\n",
    "\n",
    "# Créer un dictionnaire pour stocker les corrections ARIMA par (merchant_id, date)\n",
    "arima_corrections = {}\n",
    "\n",
    "for mid, group in tqdm(holdout_pd.groupby(\"merchant_id\", sort=False), desc=\"ARIMA evaluation\"):\n",
    "    hist_series = merchant_hist.get(mid)\n",
    "    if hist_series is None or len(hist_series) < min_hist_periods:\n",
    "        merchant_use_arima[mid] = False\n",
    "        continue\n",
    "\n",
    "    # CORRECTION: Utiliser les valeurs directement au lieu des index\n",
    "    group_reset = group.reset_index(drop=True)\n",
    "\n",
    "    if len(group_reset) == 0:\n",
    "        merchant_use_arima[mid] = False\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        model = ARIMA(hist_series, order=arima_order)\n",
    "        res = model.fit(method_kwargs={\"warn_convergence\": False})\n",
    "\n",
    "        hold_dates = pd.to_datetime(group_reset[\"ym_dt\"].values)\n",
    "        last_hist_date = hist_series.index.max()\n",
    "        months_ahead_list = ((hold_dates.year - last_hist_date.year) * 12 +\n",
    "                             (hold_dates.month - last_hist_date.month)).astype(int)\n",
    "        max_ahead = int(np.max(months_ahead_list)) if len(months_ahead_list)>0 else 0\n",
    "        if max_ahead <= 0:\n",
    "            merchant_use_arima[mid] = False\n",
    "            continue\n",
    "\n",
    "        fc = res.forecast(steps=max_ahead)\n",
    "        local_fc = np.array([float(fc[int(ma)-1]) if int(ma)>0 and int(ma)-1 < len(fc) else 0.0\n",
    "                    for ma in months_ahead_list])\n",
    "\n",
    "        # Récupérer les prédictions pour ce groupe\n",
    "        preds_before = group_reset[\"pred_main\"].values\n",
    "        preds_after = preds_before + local_fc\n",
    "\n",
    "        y_true_group = group_reset[\"y_true\"].values\n",
    "        rmse_before = math.sqrt(mean_squared_error(y_true_group, preds_before))\n",
    "        rmse_after = math.sqrt(mean_squared_error(y_true_group, preds_after))\n",
    "\n",
    "        use_arima = rmse_after < rmse_before\n",
    "        merchant_use_arima[mid] = use_arima\n",
    "\n",
    "        if use_arima:\n",
    "            merchant_models[mid] = res\n",
    "            # Stocker les corrections pour ce merchant\n",
    "            for i, date in enumerate(hold_dates):\n",
    "                arima_corrections[(mid, date)] = local_fc[i]\n",
    "\n",
    "    except Exception as e:\n",
    "        merchant_use_arima[mid] = False\n",
    "        print(f\"Error for merchant {mid}: {e}\")\n",
    "\n",
    "n_selected = sum(merchant_use_arima.values())\n",
    "print(f\"ARIMA applied for {n_selected} merchants out of {len(merchant_use_arima)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 12) Apply ARIMA corrections on holdout - CORRECTION ICI\n",
    "# -------------------------\n",
    "residual_corr_hold = np.zeros(len(holdout_pd), dtype=float)\n",
    "pred_hold_final = holdout_pd[\"pred_main\"].copy()\n",
    "\n",
    "for i, row in holdout_pd.iterrows():\n",
    "    mid = row[\"merchant_id\"]\n",
    "    date = row[\"ym_dt\"]\n",
    "\n",
    "    # Récupérer la correction si elle existe\n",
    "    correction = arima_corrections.get((mid, date), 0.0)\n",
    "    residual_corr_hold[i] = correction\n",
    "    pred_hold_final.iloc[i] += correction\n",
    "\n",
    "# -------------------------\n",
    "# 13) Evaluate hybrid model\n",
    "# -------------------------\n",
    "y_pred_hybrid_hold = pred_hold_final.values\n",
    "rmse_hybrid = math.sqrt(mean_squared_error(holdout_pd[\"y_true\"].values, y_pred_hybrid_hold))\n",
    "mae_hybrid = mean_absolute_error(holdout_pd[\"y_true\"].values, y_pred_hybrid_hold)\n",
    "r2_hybrid = r2_score(holdout_pd[\"y_true\"].values, y_pred_hybrid_hold)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MAIN (LightGBM only) -> RMSE={rmse_main:.4f}, MAE={mae_main:.4f}, R2={r2_main:.4f}\")\n",
    "print(f\"HYBRID (ARIMA-corrected) -> RMSE={rmse_hybrid:.4f}, MAE={mae_hybrid:.4f}, R2={r2_hybrid:.4f}\")\n",
    "improvement = ((rmse_main - rmse_hybrid) / rmse_main * 100) if rmse_main > 0 else 0\n",
    "print(f\"Improvement: {improvement:.2f}%\")\n",
    "\n",
    "holdout_pd[\"y_pred_main\"] = holdout_pd[\"pred_main\"].values\n",
    "holdout_pd[\"residual_corr_arima\"] = residual_corr_hold\n",
    "holdout_pd[\"y_pred_hybrid\"] = y_pred_hybrid_hold\n",
    "\n",
    "# Reconvertir en dollars pour Power BI\n",
    "holdout_pd[\"amount_actual_dollars\"] = np.exp(holdout_pd[\"y_true\"]) - 1\n",
    "holdout_pd[\"amount_pred_main_dollars\"] = np.exp(holdout_pd[\"y_pred_main\"]) - 1\n",
    "holdout_pd[\"amount_pred_hybrid_dollars\"] = np.exp(holdout_pd[\"y_pred_hybrid\"]) - 1\n",
    "\n",
    "holdout_pd.to_csv(\"holdout_2019_results.csv\", index=False)\n",
    "print(\"\\n Exported holdout_2019_results.csv\")\n",
    "\n",
    "# Feature importance\n",
    "importance = final_model.feature_importance(importance_type='gain')\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': existing_features,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Feature Importances:\")\n",
    "print(feature_importance_df.head(15))\n",
    "feature_importance_df.to_csv(\"feature_importance.csv\", index=False)\n",
    "print(\" Exported feature_importance.csv\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# SECTION 14: PRODUCTION - PRÉDICTIONS NOV 2019 à OCT 2020\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRODUCTION: GENERATING PREDICTIONS FOR NOV 2019 - OCT 2020\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------\n",
    "# 14.1) Training all data on the period (2015-2019)\n",
    "# -------------------------\n",
    "print(\"\\n[Step 1/5] Training final model on all data (2015-2019)...\")\n",
    "\n",
    "train_full_ym_max = 2019 * 100 + 10\n",
    "train_full_agg = agg.filter(col(\"ym\") <= train_full_ym_max).cache()\n",
    "\n",
    "print(f\"Full training set rows: {train_full_agg.count():,}\")\n",
    "\n",
    "# Data aggregation\n",
    "train_full_date_max = \"2019-10-31\"\n",
    "train_full_transactions = df.filter(col(\"date\") <= lit(train_full_date_max)).cache()\n",
    "\n",
    "client_stats_full = train_full_transactions.groupBy(\"client_id\").agg(\n",
    "    _avg(\"log_amount_sum_tx\").alias(\"client_mean_tx\"),\n",
    "    _stddev(\"log_amount_sum_tx\").alias(\"client_std_tx\"),\n",
    "    _count(\"*\").alias(\"client_tx_count\")\n",
    ").fillna({\"client_std_tx\": 0.0})\n",
    "\n",
    "tx_with_client_full = train_full_transactions.join(client_stats_full, \"client_id\", \"left\")\n",
    "client_agg_df_full = tx_with_client_full.groupBy(\"merchant_id\",\"ym\").agg(\n",
    "    _avg(\"client_mean_tx\").alias(\"avg_client_mean_tx\"),\n",
    "    _avg(\"client_std_tx\").alias(\"avg_client_std_tx\"),\n",
    "    _avg(\"client_tx_count\").alias(\"avg_client_tx_count\")\n",
    ")\n",
    "\n",
    "train_full_joined = train_full_agg.join(client_agg_df_full, [\"merchant_id\",\"ym\"], \"left\").fillna({\n",
    "    \"avg_client_mean_tx\":0.0,\"avg_client_std_tx\":0.0,\"avg_client_tx_count\":0.0\n",
    "})\n",
    "\n",
    "# Convert to pandas\n",
    "train_full_pd = train_full_joined.toPandas().fillna(0.0).reset_index(drop=True)\n",
    "print(f\"Converted to pandas: {len(train_full_pd):,} rows\")\n",
    "\n",
    "X_train_full = train_full_pd[existing_features].values.astype(np.float32)\n",
    "y_train_full = train_full_pd[\"log_amount_sum\"].values.astype(np.float32)\n",
    "\n",
    "# Train the model with LightGBM\n",
    "dtrain_full_final = lgb.Dataset(X_train_full, label=y_train_full)\n",
    "\n",
    "print(\"Training final LightGBM model...\")\n",
    "final_production_model = lgb.train(\n",
    "    final_lgb_params,\n",
    "    dtrain_full_final,\n",
    "    num_boost_round=1200,\n",
    ")\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "\n",
    "# -------------------------\n",
    "# 14.2) Build residual with ARIMA\n",
    "# -------------------------\n",
    "print(\"\\n[Step 2/5] Building ARIMA residuals on full history (2015-2019)...\")\n",
    "\n",
    "train_full_pd[\"pred_main_full\"] = final_production_model.predict(\n",
    "    train_full_pd[existing_features].values.astype(np.float32)\n",
    ")\n",
    "train_full_pd[\"residual_full\"] = train_full_pd[\"log_amount_sum\"].values - train_full_pd[\"pred_main_full\"].values\n",
    "\n",
    "# Build temporal serie per merchant\n",
    "merchant_hist_full = {}\n",
    "for mid, g in tqdm(train_full_pd.groupby(\"merchant_id\"), desc=\"Building residual series\"):\n",
    "    g_sorted = g.sort_values([\"year\", \"month\"]).reset_index(drop=True)\n",
    "    idx = pd.to_datetime(\n",
    "        g_sorted[\"year\"].astype(int).astype(str) +\n",
    "        g_sorted[\"month\"].astype(int).astype(str).str.zfill(2) + \"01\",\n",
    "        format=\"%Y%m%d\"\n",
    "    )\n",
    "    s = pd.Series(g_sorted[\"residual_full\"].values, index=idx)\n",
    "    full_idx = pd.date_range(start=s.index.min(), end=s.index.max(), freq=\"MS\")\n",
    "    s = s.reindex(full_idx).fillna(0.0)\n",
    "    merchant_hist_full[mid] = s\n",
    "\n",
    "print(f\"Built residual series for {len(merchant_hist_full):,} merchants\")\n",
    "\n",
    "# -------------------------\n",
    "# 14.3) Train ARIMA model for each merchant\n",
    "# -------------------------\n",
    "print(\"\\n[Step 3/5] Training ARIMA models per merchant...\")\n",
    "\n",
    "min_hist_periods = 18\n",
    "arima_order = (1, 0, 1)\n",
    "merchant_arima_models = {}\n",
    "\n",
    "for mid, hist_series in tqdm(merchant_hist_full.items(), desc=\"Training ARIMA\"):\n",
    "    if len(hist_series) < min_hist_periods:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        model = ARIMA(hist_series, order=arima_order)\n",
    "        res = model.fit(method_kwargs={\"warn_convergence\": False})\n",
    "        merchant_arima_models[mid] = res\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "print(f\"Trained ARIMA models for {len(merchant_arima_models):,} merchants\")\n",
    "\n",
    "# -------------------------\n",
    "# 14.4) Creation of the prediction dataset for the period Nov 2019 - Oct 2020\n",
    "# -------------------------\n",
    "print(\"\\n[Step 4/5] Creating prediction dataset for Nov 2019 - Oct 2020...\")\n",
    "\n",
    "# Define the prediction period\n",
    "pred_start_ym = 2019 * 100 + 11  # Nov 2019\n",
    "pred_end_ym = 2020 * 100 + 10    # Oct 2020\n",
    "\n",
    "# Creation of a list of merchants which have data in 2019\n",
    "merchants_active = train_full_pd[train_full_pd[\"year\"] == 2019][\"merchant_id\"].unique()\n",
    "print(f\"Active merchants in 2019: {len(merchants_active):,}\")\n",
    "\n",
    "# Last known values for each merchant\n",
    "last_known_values = train_full_pd.groupby(\"merchant_id\").last().reset_index()\n",
    "\n",
    "# Predioction list creation\n",
    "prediction_rows = []\n",
    "\n",
    "for target_year in [2019, 2020]:\n",
    "    start_month = 11 if target_year == 2019 else 1\n",
    "    end_month = 12 if target_year == 2019 else 10\n",
    "\n",
    "    for target_month in range(start_month, end_month + 1):\n",
    "        target_ym = target_year * 100 + target_month\n",
    "\n",
    "        print(f\"  Creating rows for {target_year}-{target_month:02d}...\")\n",
    "\n",
    "        for merchant_id in merchants_active:\n",
    "            merchant_data = last_known_values[last_known_values[\"merchant_id\"] == merchant_id]\n",
    "\n",
    "            if len(merchant_data) == 0:\n",
    "                continue\n",
    "\n",
    "            merchant_row = merchant_data.iloc[0]\n",
    "\n",
    "            # Creation of the features for the prediction\n",
    "            # LAGs are based on the last knows values (for simplification)\n",
    "            row_dict = {\n",
    "                \"merchant_id\": merchant_id,\n",
    "                \"year\": target_year,\n",
    "                \"month\": target_month,\n",
    "                \"ym\": target_ym,\n",
    "\n",
    "                # LAG features\n",
    "                \"tx_count_lag1\": merchant_row.get(\"tx_count\", 0),\n",
    "                \"tx_count_lag2\": merchant_row.get(\"tx_count_lag1\", 0),\n",
    "                \"tx_count_lag3\": merchant_row.get(\"tx_count_lag2\", 0),\n",
    "                \"amount_avg_lag1\": merchant_row.get(\"amount_avg\", 0),\n",
    "                \"amount_avg_lag2\": merchant_row.get(\"amount_avg_lag1\", 0),\n",
    "                \"amount_avg_lag3\": merchant_row.get(\"amount_avg_lag2\", 0),\n",
    "                \"amount_stddev_lag1\": merchant_row.get(\"amount_stddev\", 0),\n",
    "                \"refund_rate_lag1\": merchant_row.get(\"refund_rate\", 0),\n",
    "                \"desc_count_lag1\": merchant_row.get(\"desc_count\", 0),\n",
    "                \"log_amount_sum_lag1\": merchant_row.get(\"log_amount_sum\", 0),\n",
    "                \"log_amount_sum_lag2\": merchant_row.get(\"log_amount_sum_lag1\", 0),\n",
    "                \"log_amount_sum_lag3\": merchant_row.get(\"log_amount_sum_lag2\", 0),\n",
    "                \"log_amount_sum_lag12\": merchant_row.get(\"log_amount_sum_lag12\", 0),\n",
    "                \"tx_count_lag12\": merchant_row.get(\"tx_count_lag12\", 0),\n",
    "\n",
    "                # Rolling features\n",
    "                \"amount_avg_roll3\": merchant_row.get(\"amount_avg_roll3\", 0),\n",
    "                \"tx_count_roll3\": merchant_row.get(\"tx_count_roll3\", 0),\n",
    "                \"amount_avg_roll6\": merchant_row.get(\"amount_avg_roll6\", 0),\n",
    "                \"tx_count_roll6\": merchant_row.get(\"tx_count_roll6\", 0),\n",
    "                \"amount_avg_roll12\": merchant_row.get(\"amount_avg_roll12\", 0),\n",
    "                \"tx_count_roll12\": merchant_row.get(\"tx_count_roll12\", 0),\n",
    "                \"log_amount_sum_roll12\": merchant_row.get(\"log_amount_sum_roll12\", 0),\n",
    "\n",
    "                # Trend\n",
    "                \"amount_trend\": merchant_row.get(\"amount_trend\", 0),\n",
    "\n",
    "                # Temporal features\n",
    "                \"month_sin\": np.sin(2 * np.pi * target_month / 12),\n",
    "                \"month_cos\": np.cos(2 * np.pi * target_month / 12),\n",
    "                \"year_norm\": target_year - min_year,  # min_year défini plus haut (2015)\n",
    "\n",
    "                # Client features\n",
    "                \"avg_client_mean_tx\": merchant_row.get(\"avg_client_mean_tx\", 0),\n",
    "                \"avg_client_std_tx\": merchant_row.get(\"avg_client_std_tx\", 0),\n",
    "                \"avg_client_tx_count\": merchant_row.get(\"avg_client_tx_count\", 0),\n",
    "\n",
    "                # Growth/diff features\n",
    "                \"amount_growth_1_2\": merchant_row.get(\"amount_growth_1_2\", 1.0),\n",
    "                \"amount_diff_1_2\": merchant_row.get(\"amount_diff_1_2\", 0.0),\n",
    "            }\n",
    "\n",
    "            prediction_rows.append(row_dict)\n",
    "\n",
    "# Prediction DataFrame creation\n",
    "prediction_df = pd.DataFrame(prediction_rows)\n",
    "print(f\"Created prediction dataset: {len(prediction_df):,} rows\")\n",
    "print(f\"   Covering {prediction_df['ym'].nunique()} months for {prediction_df['merchant_id'].nunique():,} merchants\")\n",
    "\n",
    "# -------------------------\n",
    "# 14.5) Generate prediction with LightGBM + ARIMA\n",
    "# -------------------------\n",
    "print(\"\\n[Step 5/5] Generating predictions (LightGBM + ARIMA)...\")\n",
    "\n",
    "# Prediction using LightGBM\n",
    "X_pred = prediction_df[existing_features].values.astype(np.float32)\n",
    "pred_lgbm = final_production_model.predict(X_pred)\n",
    "\n",
    "prediction_df[\"pred_lgbm_log\"] = pred_lgbm\n",
    "prediction_df[\"pred_lgbm\"] = np.exp(pred_lgbm) - 1  # Échelle originale\n",
    "\n",
    "# Correction using ARIMA\n",
    "prediction_df[\"ym_dt\"] = pd.to_datetime(\n",
    "    prediction_df[\"year\"].astype(int).astype(str) +\n",
    "    prediction_df[\"month\"].astype(int).astype(str).str.zfill(2) + \"01\",\n",
    "    format=\"%Y%m%d\"\n",
    ")\n",
    "\n",
    "prediction_df[\"arima_correction\"] = 0.0\n",
    "prediction_df[\"pred_hybrid_log\"] = prediction_df[\"pred_lgbm_log\"].copy()\n",
    "\n",
    "print(\"Applying ARIMA corrections...\")\n",
    "for mid, group in tqdm(prediction_df.groupby(\"merchant_id\", sort=False), desc=\"ARIMA corrections\"):\n",
    "    if mid not in merchant_arima_models:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        arima_model = merchant_arima_models[mid]\n",
    "        last_hist_date = merchant_hist_full[mid].index.max()\n",
    "\n",
    "        for idx in group.index:\n",
    "            row = prediction_df.loc[idx]\n",
    "            months_ahead = (row[\"ym_dt\"].year - last_hist_date.year) * 12 + \\\n",
    "                           (row[\"ym_dt\"].month - last_hist_date.month)\n",
    "\n",
    "            if months_ahead > 0:\n",
    "                fc = arima_model.forecast(steps=int(months_ahead))\n",
    "                corr = float(fc[-1]) if len(fc) > 0 else 0.0\n",
    "                prediction_df.loc[idx, \"arima_correction\"] = corr\n",
    "                prediction_df.loc[idx, \"pred_hybrid_log\"] = prediction_df.loc[idx, \"pred_lgbm_log\"] + corr\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "prediction_df[\"pred_hybrid\"] = np.exp(prediction_df[\"pred_hybrid_log\"]) - 1\n",
    "\n",
    "# -------------------------\n",
    "# 14.6) Export of results\n",
    "# -------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRODUCTION PREDICTIONS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Total predictions: {len(prediction_df):,}\")\n",
    "print(f\"Period: {prediction_df['ym'].min()} to {prediction_df['ym'].max()}\")\n",
    "print(f\"Merchants covered: {prediction_df['merchant_id'].nunique():,}\")\n",
    "print(f\"ARIMA corrections applied: {(prediction_df['arima_correction'] != 0).sum():,} rows\")\n",
    "\n",
    "print(\"\\nMonthly aggregated predictions:\")\n",
    "monthly_summary = prediction_df.groupby(\"ym\").agg({\n",
    "    \"pred_lgbm\": \"sum\",\n",
    "    \"pred_hybrid\": \"sum\",\n",
    "    \"merchant_id\": \"nunique\"\n",
    "}).rename(columns={\"merchant_id\": \"n_merchants\"})\n",
    "print(monthly_summary)\n",
    "\n",
    "print(\"\\nTotal predicted amounts:\")\n",
    "print(f\"  LightGBM only: ${prediction_df['pred_lgbm'].sum():,.2f}\")\n",
    "print(f\"  Hybrid (ARIMA): ${prediction_df['pred_hybrid'].sum():,.2f}\")\n",
    "print(f\"  Average per merchant per month: ${prediction_df['pred_hybrid'].mean():,.2f}\")\n",
    "\n",
    "# Select columns for the export\n",
    "export_cols = [\n",
    "    \"merchant_id\", \"year\", \"month\", \"ym\", \"ym_dt\",\n",
    "    \"pred_lgbm\", \"pred_hybrid\", \"pred_lgbm_log\", \"pred_hybrid_log\",\n",
    "    \"arima_correction\"\n",
    "] + existing_features\n",
    "\n",
    "prediction_export = prediction_df[export_cols]\n",
    "\n",
    "# Export\n",
    "output_file = \"production_predictions_nov2019_oct2020.csv\"\n",
    "prediction_export.to_csv(output_file, index=False)\n",
    "print(f\"\\n Exported production predictions -> {output_file}\")\n",
    "\n",
    "# Export of the monthly result per merchant\n",
    "monthly_merchant_summary = prediction_df.groupby([\"merchant_id\", \"year\", \"month\"]).agg({\n",
    "    \"pred_lgbm\": \"sum\",\n",
    "    \"pred_hybrid\": \"sum\",\n",
    "    \"ym\": \"first\"\n",
    "}).reset_index()\n",
    "\n",
    "monthly_merchant_summary.to_csv(\"production_predictions_monthly_summary.csv\", index=False)\n",
    "print(f\" Exported monthly summary -> production_predictions_monthly_summary.csv\")\n",
    "\n",
    "# Stats per merchant\n",
    "merchant_stats = prediction_df.groupby(\"merchant_id\").agg({\n",
    "    \"pred_hybrid\": [\"sum\", \"mean\", \"std\"],\n",
    "    \"month\": \"count\"\n",
    "}).reset_index()\n",
    "merchant_stats.columns = [\"merchant_id\", \"total_predicted\", \"avg_monthly\", \"std_monthly\", \"n_months\"]\n",
    "merchant_stats = merchant_stats.sort_values(\"total_predicted\", ascending=False)\n",
    "\n",
    "merchant_stats.to_csv(\"production_merchant_statistics.csv\", index=False)\n",
    "print(f\" Exported merchant statistics -> production_merchant_statistics.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRODUCTION PREDICTIONS COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cleanup\n",
    "train_full_agg.unpersist()\n",
    "train_full_transactions.unpersist()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7d98806a-e3f7-4007-a440-42144d4be937",
    "6236c9e4-2f80-4ecd-a106-daf1b0c99b85",
    "hpHIbdddqQ7x",
    "wqWbNdpckbHb",
    "2FPBvicqLr3v"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
